<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-20 03:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250920_0334</div>
    <div class="row"><div class="card">
<div class="title">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2025-09-15T05:10:43+00:00 · Latest: 2025-09-15T05:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.11587v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.11587v1">PDF</a> · <a href="https://github.com/haonanshi0125/HIL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外行人重识别的分层身份学习</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在通过减少模态差异并最小化昂贵的手动注释依赖性，从跨模态的无标签行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来处理USVI-ReID，通过单个聚类中心表示一个人。然而，它们主要关注每个聚类内的图像共性，而忽视了它们之间的细微差异。为了解决这一局限性，我们提出了一种分层身份学习（HIL）框架。由于每个聚类可能包含多个较小的子聚类，反映图像之间的细微变化，我们通过二级聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习（MCCL）以细化表示，增强同模态聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输（BRST）机制，通过双向匹配伪标签建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法优于现有方法。源代码可在：https://github.com/haonanshi0125/HIL 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of existing cluster-based methods. The proposed Hierarchical Identity Learning (HIL) framework introduces multiple memories for each cluster and a Multi-Center Contrastive Learning (MCCL) method to refine intra-modal clustering and reduce cross-modal discrepancies. The Bidirectional Reverse Selection Transmission (BRST) mechanism further enhances cross-modal matching. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing approaches.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有基于聚类的方法的局限性，提高无监督的可见光-红外人像重识别性能。提出的层次身份学习（HIL）框架为每个聚类生成多个记忆，并使用多中心对比学习（MCCL）来细化表示。此外，设计了双向反向选择传输（BRST）机制，以通过双向匹配伪标签来增强跨模态匹配质量。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法在跨模态人像重识别方面优于现有方法，且无需依赖人工标注。源代码已公开。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Duality Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu</div>
<div class="meta-line">First: 2025-05-05T10:36:52+00:00 · Latest: 2025-05-06T07:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.02549v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.02549v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) aims to
retrieve pedestrian images across different modalities without costly
annotations, but faces challenges due to the modality gap and lack of
supervision. Existing methods often adopt self-training with
clustering-generated pseudo-labels but implicitly assume these labels are
always correct. In practice, however, this assumption fails due to inevitable
pseudo-label noise, which hinders model learning. To address this, we introduce
a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),
characterized by three key challenges: noise overfitting, error accumulation,
and noisy cluster correspondence. To this end, we propose a novel Robust
Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy
pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning
mechanism (RAL) is proposed to dynamically emphasize clean samples while
down-weighting noisy ones. Second, to alleviate error accumulation-where the
model reinforces its own mistakes-RoDE employs dual distinct models that are
alternately trained using pseudo-labels from each other, encouraging diversity
and preventing collapse. However, this dual-model strategy introduces
misalignment between clusters across models and modalities, creating noisy
cluster correspondence. To resolve this, we introduce Cluster Consistency
Matching (CCM), which aligns clusters across models and modalities by measuring
cross-cluster similarity. Extensive experiments on three benchmarks demonstrate
the effectiveness of RoDE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒对偶学习在无监督可见-红外行人再识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人再识别（UVI-ReID）旨在无需昂贵标注的情况下，在不同模态中检索行人图像，但由于模态差距和缺乏监督，面临挑战。现有方法通常采用自训练结合聚类生成的伪标签，但隐含地假设这些标签总是正确的。然而，在实践中，由于不可避免的伪标签噪声，这一假设会失败，阻碍模型学习。为解决这一问题，我们提出了一种新的学习范式，明确考虑伪标签噪声（PLN），其包含三个关键挑战：噪声过拟合、错误累积和嘈杂的聚类对应。为此，我们提出了一种新颖的鲁棒对偶学习框架（RoDE）以减轻噪声伪标签的影响。首先，为对抗噪声过拟合，我们提出了一种鲁棒自适应学习机制（RAL），动态强调干净样本并降低噪声样本的权重。其次，为缓解错误累积，RoDE 使用相互的伪标签交替训练两个不同的模型，鼓励多样性并防止模型崩溃。然而，这种双模型策略引入了模型和模态之间的聚类对齐问题，导致嘈杂的聚类对应。为解决这一问题，我们引入了聚类一致性匹配（CCM），通过测量跨聚类相似性来对齐模型和模态之间的聚类。在三个基准上的广泛实验表明了RoDE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a Robust Duality Learning framework (RoDE) that explicitly handles pseudo-label noise. RoDE includes a Robust Adaptive Learning mechanism to dynamically weight samples and a dual-model training approach to prevent error accumulation. Additionally, it introduces Cluster Consistency Matching to align clusters across models and modalities. Experiments on three benchmarks show the effectiveness of RoDE in mitigating the effects of noisy pseudo-labels.</div>
<div class="mono" style="margin-top:8px">本文通过引入一个明确考虑伪标签噪声的新学习范式，解决了无监督可见红外行人重识别（UVI-ReID）的挑战。提出的Robust Duality Learning框架（RoDE）包括Robust Adaptive Learning机制以应对噪声过拟合，并使用双模型来防止错误累积。此外，引入了Cluster Consistency Matching来跨模型和模态对齐集群。在三个基准上的实验表明，RoDE在缓解噪声伪标签影响方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised   VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于协作精炼的语义对齐学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需人类注释的情况下，匹配不同模态下同一个体的行人图像。先前的方法通过标签关联算法统一跨模态图像的伪标签，然后设计对比学习框架进行全局特征学习。然而，这些方法忽略了由细粒度模式带来的特征表示和伪标签分布的跨模态变化。这种洞察导致仅优化全局特征时，模态共享学习不足。为解决这一问题，我们提出了一种语义对齐学习与协作精炼（SALCR）框架，该框架为每个模态强调的特定细粒度模式建立优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一种双向全局学习关联（DAGI）模块，以双向方式统一跨模态实例的伪标签。随后，执行了一种细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。基于语义对齐特征及其相应的标签空间，构建了优化目标。为缓解来自噪声伪标签的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集，并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有方法。我们的代码可在https://github.com/FranklinLingfeng/code-for-SALCR 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Extended Cross-Modality United Learning for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-26T09:30:26+00:00 · Latest: 2024-12-26T09:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.19134v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.19134v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展跨模态联合学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征并减少模态间差距。然而，现有方法缺乏跨模态聚类或过度追求聚类级别的关联，这使得难以进行可靠的模态不变特征学习。为解决这一问题，我们提出了一种扩展跨模态联合学习（ECUL）框架，结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。具体而言，我们设计ECUL以自然地整合模态内聚类、模态间聚类和模态间实例选择，建立紧凑且准确的跨模态关联，同时减少引入噪声标签。此外，EMCC通过扩展编码向量捕获和过滤邻域关系，进一步促进聚类算法中模态不变和相机不变知识的学习。最后，TSMem通过分阶段更新记忆为对比学习提供准确且通用的代理点。在SYSU-MM01和RegDB数据集上的广泛实验结果表明，提出的ECUL表现出有希望的性能，并且甚至优于某些监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Relieving Universal Label Noise for Unsupervised Visible-Infrared Person   Re-Identification by Inferring from Neighbors</div>
<div class="meta-line">Authors: Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin</div>
<div class="meta-line">First: 2024-12-16T04:04:41+00:00 · Latest: 2024-12-16T04:04:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.12220v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.12220v1">PDF</a> · <a href="https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of
great research and practical significance yet remains challenging due to the
absence of annotations. Existing approaches aim to learn modality-invariant
representations in an unsupervised setting. However, these methods often
encounter label noise within and across modalities due to suboptimal clustering
results and considerable modality discrepancies, which impedes effective
training. To address these challenges, we propose a straightforward yet
effective solution for USL-VI-ReID by mitigating universal label noise using
neighbor information. Specifically, we introduce the Neighbor-guided Universal
Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in
both homogeneous and heterogeneous spaces with soft labels derived from
neighboring samples to reduce label noise. Additionally, we present the
Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability
by minimizing the influence of unreliable samples. Extensive experiments on the
RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing
USL-VI-ReID approaches, despite its simplicity. The source code is available
at: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过从邻居推断缓解通用标签噪声以实现无监督可见-红外行人再识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人再识别（USL-VI-ReID）具有重要的研究和实际意义，但由于缺乏标注而面临挑战。现有方法旨在在无监督设置中学习跨模态不变的表示。然而，这些方法常常由于聚类结果不佳和模态差异较大而遇到跨模态的标签噪声问题，这阻碍了有效的训练。为了解决这些挑战，我们提出了一种简单而有效的USL-VI-ReID解决方案，通过邻居信息减轻通用标签噪声。具体而言，我们引入了邻居引导的通用标签校准（N-ULC）模块，该模块用来自邻居样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们提出了邻居引导的动态加权（N-DW）模块，通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，但我们的方法仍优于现有USL-VI-ReID方法。源代码可在：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of great research and practical significance yet remains challenging due to the absence of annotations.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决标签噪声问题来提升无监督的可见光-红外行人重识别。提出了邻域引导的全局标签校准（N-ULC）模块，利用邻近样本的软标签来减少标签噪声，并提出了邻域引导的动态加权（N-DW）模块以增强训练稳定性。实验表明，该方法在RegDB和SYSU-MM01数据集上优于现有方法，尽管其结构简单。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modality-Camera Invariant Clustering for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yiming Yang, Weipeng Hu, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-11T09:31:03+00:00 · Latest: 2024-12-11T09:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.08231v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.08231v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
offers a more flexible and cost-effective alternative compared to supervised
methods. This field has gained increasing attention due to its promising
potential. Existing methods simply cluster modality-specific samples and employ
strong association techniques to achieve instance-to-cluster or
cluster-to-cluster cross-modality associations. However, they ignore
cross-camera differences, leading to noticeable issues with excessive splitting
of identities. Consequently, this undermines the accuracy and reliability of
cross-modal associations. To address these issues, we propose a novel Dynamic
Modality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID.
Specifically, our DMIC naturally integrates Modality-Camera Invariant Expansion
(MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive
Learning (HMCL) into a unified framework, which eliminates both the
cross-modality and cross-camera discrepancies in clustering. MIE fuses
inter-modal and inter-camera distance coding to bridge the gaps between
modalities and cameras at the clustering level. DNC employs two dynamic search
strategies to refine the network&#x27;s optimization objective, transitioning from
improving discriminability to enhancing cross-modal and cross-camera
generalizability. Moreover, HMCL is designed to optimize instance-level and
cluster-level distributions. Memories for intra-modality and inter-modality
training are updated using randomly selected samples, facilitating real-time
exploration of modality-invariant representations. Extensive experiments have
demonstrated that our DMIC addresses the limitations present in current
clustering approaches and achieve competitive performance, which significantly
reduces the performance gap with supervised methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of existing methods, which often ignore cross-camera differences leading to identity splitting. The proposed DMIC framework integrates MIE, DNC, and HMCL to eliminate both cross-modality and cross-camera discrepancies. MIE fuses inter-modal and inter-camera distance coding, DNC uses dynamic search strategies to refine the network&#x27;s optimization, and HMCL optimizes instance and cluster distributions. Experiments show that DMIC achieves competitive performance, reducing the performance gap with supervised methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决因跨摄像头差异导致的过多身份分裂问题，提高无监督可见-红外行人再识别的性能。提出的DMIC框架整合了MIE、DNC和HMCL，以消除跨模态和跨摄像头的差异。MIE融合了跨模态和跨摄像头的距离编码，DNC使用动态搜索策略提高可区分性和跨模态、跨摄像头的一般性，而HMCL优化了实例和聚类的分布。实验表明，DMIC超越了现有方法，并显著缩小了与监督方法的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Homogeneous and Heterogeneous Consistent Label Associations   for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2024-02-01T15:33:17+00:00 · Latest: 2024-12-04T03:55:35+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.00672v4">Abs</a> · <a href="http://arxiv.org/pdf/2402.00672v4">PDF</a> · <a href="https://github.com/FranklinLingfeng/code_for_MULT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID)
endeavors to retrieve pedestrian images of the same identity from different
modalities without annotations. While prior work focuses on establishing
cross-modality pseudo-label associations to bridge the modality-gap, they
ignore maintaining the instance-level homogeneous and heterogeneous consistency
between the feature space and the pseudo-label space, resulting in coarse
associations. In response, we introduce a Modality-Unified Label Transfer
(MULT) module that simultaneously accounts for both homogeneous and
heterogeneous fine-grained instance-level structures, yielding high-quality
cross-modality label associations. It models both homogeneous and heterogeneous
affinities, leveraging them to quantify the inconsistency between the
pseudo-label space and the feature space, subsequently minimizing it. The
proposed MULT ensures that the generated pseudo-labels maintain alignment
across modalities while upholding structural consistency within intra-modality.
Additionally, a straightforward plug-and-play Online Cross-memory Label
Refinement (OCLR) module is proposed to further mitigate the side effects of
noisy pseudo-labels while simultaneously aligning different modalities, coupled
with an Alternative Modality-Invariant Representation Learning (AMIRL)
framework. Experiments demonstrate that our proposed method outperforms
existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of
our MULT in comparison to other cross-modality association methods. Code is
available at https://github.com/FranklinLingfeng/code_for_MULT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索一致标签关联的同质性和异质性在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，从不同模态中检索同一身份的行人图像。先前的工作集中在建立跨模态的伪标签关联以弥补模态差异，但忽略了在特征空间和伪标签空间之间保持实例级的同质性和异质一致性，导致关联粗糙。为此，我们引入了一个模态统一标签转移（MULT）模块，该模块同时考虑了同质性和异质性的细粒度实例结构，从而产生高质量的跨模态标签关联。该模块利用同质性和异质性亲和力来量化伪标签空间与特征空间之间的不一致性，并对其进行最小化。所提出的MULT确保生成的伪标签在不同模态之间保持对齐，同时在同模态内部保持结构一致性。此外，我们还提出了一种简单的即插即用在线跨记忆标签精炼（OCLR）模块，以进一步减轻噪声伪标签的副作用，同时对齐不同模态，结合了一种替代模态不变表示学习（AMIRL）框架。实验表明，我们提出的方法在现有的USL-VI-ReID方法中表现更优，突显了MULT在与其他跨模态关联方法相比的优势。代码可在https://github.com/FranklinLingfeng/code_for_MULT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by introducing a Modality-Unified Label Transfer (MULT) module that simultaneously considers both homogeneous and heterogeneous fine-grained instance-level structures. The MULT module models affinities to quantify inconsistencies between the pseudo-label space and the feature space, thereby improving cross-modality label associations. Additionally, an Online Cross-memory Label Refinement (OCLR) module and an Alternative Modality-Invariant Representation Learning (AMIRL) framework are proposed to further refine and align pseudo-labels. Experimental results show that the proposed method outperforms existing state-of-the-art USL-VI-ReID methods.</div>
<div class="mono" style="margin-top:8px">本文通过引入同时考虑同质性和异质性细粒度实例结构的Modality-Unified Label Transfer (MULT)模块，解决了无监督可见-红外行人重识别（USL-VI-ReID）的挑战。MULT模块通过建模亲和力来量化伪标签空间与特征空间之间的不一致性，从而改善跨模态标签关联。此外，还提出了Online Cross-memory Label Refinement (OCLR)模块和Alternative Modality-Invariant Representation Learning (AMIRL)框架，进一步细化和对齐伪标签。实验结果表明，所提出的方法在现有最先进的USL-VI-ReID方法中表现出更优的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with   Neighbor-Guided Label Refinement</div>
<div class="meta-line">Authors: De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T04:40:30+00:00 · Latest: 2024-11-03T13:50:53+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12711v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12711v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作学习与邻域引导标签精炼的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征，这对于视频监控系统中的实际应用至关重要。解决USL-VI-ReID任务的关键在于解决跨模态数据关联问题，以便进行进一步的异质联合学习。为了解决这一问题，我们提出了一种双最优传输标签分配（DOTLA）框架，以同时将一种模态生成的标签分配给其对应的模态。提出的DOTLA机制将跨模态数据关联问题转化为一种相互强化和高效的解决方案，从而有效减少了某些不足和噪声标签关联的副作用。此外，我们还提出了一种跨模态邻域一致性引导的标签精炼和正则化模块，在假设每个示例的预测或标签分布应与其最近邻相似的前提下，消除不准确监督信号带来的负面影响。在公开的SYSU-MM01和RegDB数据集上的广泛实验结果表明，所提出的方法具有很高的有效性，平均mAP值比现有最先进的方法高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the unsupervised visible-infrared person re-identification (USL-VI-ReID) task by proposing a Dual Optimal Transport Label Assignment (DOTLA) framework to solve the cross-modality data association problem. This framework assigns generated labels from one modality to its counterpart, enhancing the joint learning process. Additionally, a neighbor consistency guided label refinement module is introduced to improve the accuracy of the labels. Experimental results on public datasets SYSU-MM01 and RegDB show that the proposed method outperforms existing state-of-the-art approaches by 7.76% in mean average precision (mAP).</div>
<div class="mono" style="margin-top:8px">论文提出了一种双最优传输标签分配（DOTLA）框架，用于解决可见光-红外人再识别（USL-VI-ReID）的无监督学习问题。该框架通过将一种模态生成的标签分配给另一种模态，增强跨模态数据关联。此外，还提出了一种跨模态邻居一致性引导的标签精炼和正则化模块，以提高标签准确性。实验结果表明，该方法在SYSU-MM01和RegDB数据集上的平均mAP性能优于现有最先进的方法，高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised   Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T03:27:46+00:00 · Latest: 2024-11-03T13:48:21+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12673v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12673v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to
match pedestrian images of the same identity from different modalities without
annotations. Existing works mainly focus on alleviating the modality gap by
aligning instance-level features of the unlabeled samples. However, the
relationships between cross-modality clusters are not well explored. To this
end, we propose a novel bilateral cluster matching-based learning framework to
reduce the modality gap by matching cross-modality clusters. Specifically, we
design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)
algorithm through optimizing the maximum matching problem in a bipartite graph.
Then, the matched pairwise clusters utilize shared visible and infrared
pseudo-labels during the model training. Under such a supervisory signal, a
Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework
is proposed to align features jointly at a cluster-level. Meanwhile, the
cross-modality Consistency Constraint (CC) is proposed to explicitly reduce the
large modality discrepancy. Extensive experiments on the public SYSU-MM01 and
RegDB datasets demonstrate the effectiveness of the proposed method, surpassing
state-of-the-art approaches by a large margin of 8.76% mAP on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效双边跨模态簇匹配的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，匹配不同模态下同一身份的行人图像。现有工作主要集中在通过对齐未标注样本的实例级特征来缓解模态差异。然而，跨模态簇之间的关系尚未得到充分探索。为此，我们提出了一种新颖的双边簇匹配学习框架，通过匹配跨模态簇来减少模态差异。具体地，我们通过在二分图中优化最大匹配问题设计了一种多对多双边跨模态簇匹配（MBCCM）算法。然后，在模型训练过程中，匹配的成对簇利用共享的可见和红外伪标签。在这样的监督信号下，我们提出了一种模态特定和模态无关的对比学习框架，以在簇级联合对齐特征。同时，我们提出了跨模态一致性约束（CC）以显式地减少模态差异。在公开的SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法具有很高的有效性，平均mAP比现有最佳方法高出8.76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a novel bilateral cluster matching framework. The method focuses on matching cross-modality clusters to reduce the modality gap, using a Many-to-many Bilateral Cross-Modality Cluster Matching algorithm and a Modality-Specific and Modality-Agnostic contrastive learning framework. Experiments show the proposed method outperforms existing approaches by 8.76% in mean average precision.</div>
<div class="mono" style="margin-top:8px">论文提出了一种新颖的双边集群匹配框架，以解决无监督的可见红外行人重识别问题。该方法通过匹配跨模态集群来减少模态差距。方法名为Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)，通过在二分图中优化最大匹配问题来实现。匹配的集群用于训练Modality-Specific and Modality-Agnostic (MSMA)对比学习框架，并引入了跨模态一致性约束。在SYSU-MM01和RegDB数据集上的实验表明，该方法显著提高了性能，平均mAP提高了8.76%，超过了现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Commonality, Divergence and Variety for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-02-29T10:37:49+00:00 · Latest: 2024-10-24T09:00:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.19026v3">Abs</a> · <a href="http://arxiv.org/pdf/2402.19026v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
match specified people in infrared images to visible images without
annotations, and vice versa. USVI-ReID is a challenging yet under-explored
task. Most existing methods address the USVI-ReID using cluster-based
contrastive learning, which simply employs the cluster center as a
representation of a person. However, the cluster center primarily focuses on
commonality, overlooking divergence and variety. To address the problem, we
propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes
method for USVI-ReID. In brief, we generate the hard prototype by selecting the
sample with the maximum distance from the cluster center. We theoretically show
that the hard prototype is used in the contrastive loss to emphasize
divergence. Additionally, instead of rigidly aligning query images to a
specific prototype, we generate the dynamic prototype by randomly picking
samples within a cluster. The dynamic prototype is used to encourage the
variety. Finally, we introduce a progressive learning strategy to gradually
shift the model&#x27;s attention towards divergence and variety, avoiding cluster
deterioration. Extensive experiments conducted on the publicly available
SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习共性、差异性和多样性以进行无监督的可见光-红外人体重识别</div>
<div class="mono" style="margin-top:8px">无监督的可见光-红外人体重识别（USVI-ReID）旨在无需标注的情况下，在红外图像中匹配指定的人并在可见图像中反向匹配，反之亦然。USVI-ReID 是一个具有挑战性但尚未充分探索的任务。大多数现有方法使用基于聚类的对比学习来解决 USVI-ReID，简单地将聚类中心作为人的表示。然而，聚类中心主要关注共性，忽视了差异性和多样性。为了解决这个问题，我们提出了一种用于 USVI-ReID 的渐进对比学习与硬动态原型方法。简而言之，我们通过选择与聚类中心最大距离的样本生成硬原型。我们从理论上证明，硬原型用于对比损失中以强调差异。此外，我们不是将查询图像严格对齐到特定的原型，而是通过在聚类内随机选择样本生成动态原型。动态原型用于鼓励多样性。最后，我们引入了一种渐进学习策略，逐步将模型的注意力转向差异性和多样性，避免聚类退化。在公开的 SYSU-MM01 和 RegDB 数据集上进行的广泛实验验证了所提出方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Progressive Contrastive Learning with Hard and Dynamic Prototypes method. This method generates hard prototypes to emphasize divergence and dynamic prototypes to encourage variety, while using a progressive learning strategy to shift focus towards these aspects. Experiments on SYSU-MM01 and RegDB datasets demonstrate the method&#x27;s effectiveness in improving USVI-ReID performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种渐进对比学习与硬动态原型方法，用于解决无监督可见红外行人重识别（USVI-ReID）问题。该方法通过生成硬原型来强调差异性，生成动态原型来鼓励多样性，并采用渐进学习策略逐步将模型的关注点转向这些方面。在SYSU-MM01和RegDB数据集上的实验验证了所提出方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Memory Matching for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-01-12T01:24:04+00:00 · Latest: 2024-07-29T09:40:11+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2401.06825v2">Abs</a> · <a href="http://arxiv.org/pdf/2401.06825v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a
promising yet challenging retrieval task. The key challenges in USL-VI-ReID are
to effectively generate pseudo-labels and establish pseudo-label
correspondences across modalities without relying on any prior annotations.
Recently, clustered pseudo-label methods have gained more attention in
USL-VI-ReID. However, previous methods fell short of fully exploiting the
individual nuances, as they simply utilized a single memory that represented an
identity to establish cross-modality correspondences, resulting in ambiguous
cross-modality correspondences. To address the problem, we propose a
Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a
Cross-Modality Clustering (CMC) module to generate the pseudo-labels through
clustering together both two modality samples. To associate cross-modality
clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM)
module, ensuring that optimization explicitly focuses on the nuances of
individual perspectives and establishes reliable cross-modality
correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module
to narrow the modality gap while mitigating the effect of noise pseudo-labels
through a soft many-to-many alignment strategy. Extensive experiments on the
public SYSU-MM01 and RegDB datasets demonstrate the reliability of the
established cross-modality correspondences and the effectiveness of our MMM.
The source codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多记忆匹配在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）是一项有前景但极具挑战性的检索任务。USL-VI-ReID 的关键挑战在于有效生成伪标签并跨模态建立伪标签对应关系，而不依赖任何先验注释。最近，聚类伪标签方法在 USL-VI-ReID 中引起了更多关注。然而，先前的方法未能充分利用个体差异，因为它们仅利用单一记忆来表示身份以建立跨模态对应关系，导致跨模态对应关系模糊。为解决这一问题，我们提出了一种 USL-VI-ReID 的多记忆匹配（MMM）框架。我们首先设计了一个跨模态聚类（CMC）模块，通过聚类两个模态样本来生成伪标签。为了关联跨模态聚类伪标签，我们设计了一个多记忆学习和匹配（MMLM）模块，确保优化明确关注个体视角的差异并建立可靠的跨模态对应关系。最后，我们设计了一个软聚类级对齐（SCA）模块，通过软多对多对齐策略缩小模态差距并减轻噪声伪标签的影响。在公开的 SYSU-MM01 和 RegDB 数据集上的广泛实验表明，建立的跨模态对应关系的可靠性和我们 MMM 的有效性。源代码将发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Multi-Memory Matching (MMM) framework. It introduces a Cross-Modality Clustering (CMC) module to generate pseudo-labels and a Multi-Memory Learning and Matching (MMLM) module to establish reliable cross-modality correspondences. Additionally, a Soft Cluster-level Alignment (SCA) module is designed to reduce the modality gap and mitigate noise. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the proposed method.</div>
<div class="mono" style="margin-top:8px">论文提出了一个多记忆匹配（MMM）框架来解决无监督可见红外行人重识别的挑战。它引入了跨模态聚类（CMC）模块生成伪标签，并设计了多记忆学习和匹配（MMLM）模块以建立可靠的跨模态对应关系。研究还包含了一个软簇级对齐（SCA）模块，以减少模态差距并减轻噪声伪标签的影响。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法在建立准确的跨模态对应关系方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="http://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a
challenging retrieval task that aims to retrieve cross-modality pedestrian
images without using any label information. In this task, the large
cross-modality variance makes it difficult to generate reliable cross-modality
labels, and the lack of annotations also provides additional difficulties for
learning modality-invariant features. In this paper, we first deduce an
optimization objective for unsupervised VI-ReID based on the mutual information
between the model&#x27;s cross-modality input and output. With equivalent
derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy
minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable
cross-modality matching) are obtained. Under their guidance, we design a loop
iterative training strategy alternating between model training and
cross-modality matching. In the matching stage, a uniform prior guided optimal
transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched
visible and infrared prototypes. In the training stage, we utilize this
matching information to introduce prototype-based contrastive learning for
minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive
experimental results on benchmarks demonstrate the effectiveness of our method,
e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any
annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人重识别（USVI-ReID）是一个具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出一个无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一个交替进行模型训练和跨模态匹配的循环训练策略。在匹配阶段，我们提出了一种由均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化类内和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如在SYSU-MM01和RegDB上无任何注释的情况下，Rank-1精度分别为60.6%和90.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: &#x27;Sharpness&#x27; for minimizing entropy, &#x27;Fairness&#x27; for uniform label distribution, and &#x27;Fitness&#x27; for reliable cross-modality matching. The method uses a loop iterative training strategy and proposes a uniform prior guided optimal transport assignment to enhance cross-modality matching. Experimental results show significant improvements, achieving 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB benchmarks without any annotations.</div>
<div class="mono" style="margin-top:8px">本文通过基于互信息的方法解决了无监督的可见光-红外行人再识别挑战，提出了三个学习原则：清晰度、公平性和适应性，并提出了一种交替进行模型训练和跨模态匹配的循环迭代训练策略。该方法使用均匀先验引导的最优运输分配来选择匹配的原型，并引入基于原型的对比学习来最小化熵。实验结果表明，该方法在SYSU-MM01和RegDB上取得了显著效果，分别达到了60.6%和90.3%的Rank-1准确率，且无需任何标注。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised   Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Xiangbo Yin, Jiangming Shi, Yachao Zhang, Yang Lu, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-05-09T08:17:06+00:00 · Latest: 2024-05-09T08:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2405.05613v1">Abs</a> · <a href="http://arxiv.org/pdf/2405.05613v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a
formidable challenge, which aims to match pedestrian images across visible and
infrared modalities without any annotations. Recently, clustered pseudo-label
methods have become predominant in USVI-ReID, although the inherent noise in
pseudo-labels presents a significant obstacle. Most existing works primarily
focus on shielding the model from the harmful effects of noise, neglecting to
calibrate noisy pseudo-labels usually associated with hard samples, which will
compromise the robustness of the model. To address this issue, we design a
Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for
USVI-ReID. To be specific, we first introduce a straightforward yet potent
Noisy Pseudo-label Calibration module to correct noisy pseudo-labels. Due to
the high intra-class variations, noisy pseudo-labels are difficult to calibrate
completely. Therefore, we introduce a Neighbor Relation Learning module to
reduce high intra-class variations by modeling potential interactions between
all samples. Subsequently, we devise an Optimal Transport Prototype Matching
module to establish reliable cross-modality correspondences. On that basis, we
design a Memory Hybrid Learning module to jointly learn modality-specific and
modality-invariant information. Comprehensive experiments conducted on two
widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR
outperforms the current state-of-the-art GUR with an average Rank-1 improvement
of 10.3%. The source codes will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒的邻域关系伪标签学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）面临巨大挑战，旨在无需任何标注的情况下，在可见光和红外模态之间匹配行人图像。最近，聚类伪标签方法在USVI-ReID中占据主导地位，尽管伪标签中的固有噪声构成了重大障碍。现有大多数工作主要关注于保护模型免受噪声的负面影响，而忽视了对通常与硬样本相关的噪声伪标签进行校准，这会损害模型的鲁棒性。为解决这一问题，我们为USVI-ReID设计了一种鲁棒的邻域关系伪标签学习框架（RPNR）。具体而言，我们首先引入了一个简单而有效的噪声伪标签校准模块来修正噪声伪标签。由于类内变异性高，噪声伪标签难以完全校准。因此，我们引入了一个邻域关系学习模块，通过建模所有样本之间的潜在交互来降低类内变异性。随后，我们设计了一种最优传输原型匹配模块以建立可靠的跨模态对应关系。在此基础上，我们设计了一种记忆混合学习模块以联合学习模态特定和模态不变信息。在SYSU-MM01和RegDB两个广泛认可的基准上进行的全面实验表明，RPNR在平均Rank-1上优于当前最先进的GUR，提高了10.3%。源代码将很快发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework. This framework includes a Noisy Pseudo-label Calibration module to correct noisy pseudo-labels, a Neighbor Relation Learning module to reduce intra-class variations, and an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences. Experimental results on SYSU-MM01 and RegDB benchmarks show that RPNR outperforms the current state-of-the-art method GUR by 10.3% in terms of Rank-1 accuracy.</div>
<div class="mono" style="margin-top:8px">本文提出了一种鲁棒伪标签学习与邻域关系框架（RPNR），以解决无监督可见-红外行人重识别的挑战。该方法包括一个伪标签校准模块来纠正噪声标签、一个邻域关系学习模块来减少类内差异，以及一个最优传输原型匹配模块来建立跨模态对应关系。在SYSU-MM01和RegDB基准上的实验表明，RPNR在Rank-1准确率上比当前最先进的GUR提高了10.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared ReID via Pseudo-label Correction and   Modality-level Alignment</div>
<div class="meta-line">Authors: Yexin Liu, Weiming Zhang, Athanasios V. Vasilakos, Lin Wang</div>
<div class="meta-line">First: 2024-04-10T02:03:14+00:00 · Latest: 2024-04-10T02:03:14+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2404.06683v1">Abs</a> · <a href="http://arxiv.org/pdf/2404.06683v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) has
recently gained great attention due to its potential for enhancing human
detection in diverse environments without labeling. Previous methods utilize
intra-modality clustering and cross-modality feature matching to achieve
UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be
generated in the clustering process, and 2) the cross-modality feature
alignment via matching the marginal distribution of visible and infrared
modalities may misalign the different identities from two modalities. In this
paper, we first conduct a theoretic analysis where an interpretable
generalization upper bound is introduced. Based on the analysis, we then
propose a novel unsupervised cross-modality person re-identification framework
(PRAISE). Specifically, to address the first challenge, we propose a
pseudo-label correction strategy that utilizes a Beta Mixture Model to predict
the probability of mis-clustering based network&#x27;s memory effect and rectifies
the correspondence by adding a perceptual term to contrastive learning. Next,
we introduce a modality-level alignment strategy that generates paired
visible-infrared latent features and reduces the modality gap by aligning the
labeling function of visible and infrared features to learn identity
discriminative and modality-invariant features. Experimental results on two
benchmark datasets demonstrate that our method achieves state-of-the-art
performance than the unsupervised visible-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于伪标签校正和模态级对齐的无监督可见-红外ReID</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别(UVI-ReID)由于其在不同环境中增强人类检测的潜力而受到广泛关注。先前的方法利用同模态聚类和跨模态特征匹配来实现UVI-ReID。然而，存在两个挑战：1) 聚类过程中可能会生成噪声伪标签；2) 通过匹配可见光和红外模态的边缘分布来进行跨模态特征对齐可能会导致两种模态不同身份的对齐错误。在本文中，我们首先进行理论分析，引入了一个可解释的泛化上界。基于分析，我们提出了一种新颖的无监督跨模态行人重识别框架(PRAISE)。具体而言，为了解决第一个挑战，我们提出了一种伪标签校正策略，利用Beta混合模型预测网络记忆效应导致的误聚类概率，并通过在对比学习中添加感知项来纠正对应关系。其次，我们引入了一种模态级对齐策略，生成可见-红外配对的潜在特征，并通过使可见光和红外特征的标签函数对齐来减少模态差距，从而学习身份判别性和模态不变性的特征。在两个基准数据集上的实验结果表明，我们的方法在无监督可见-ReID方法中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a novel framework called PRAISE. It introduces a pseudo-label correction strategy using a Beta Mixture Model to rectify mis-clustering and a modality-level alignment strategy to align the labeling functions of visible and infrared features. The method outperforms existing unsupervised visible-ReID methods on two benchmark datasets.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为PRAISE的新框架，以解决无监督可见-红外人再识别（UVI-ReID）中的挑战。该方法引入了一种使用Beta混合模型进行伪标签修正的策略，以纠正误聚类样本，并引入了一种模态级对齐策略，通过对齐可见光和红外特征的标签函数来减少模态差异。实验结果表明，该方法在两个基准数据集上达到了最先进的性能，证明了其在处理噪声伪标签和模态错位问题方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Shape-consistent Generative Adversarial Networks for multi-modal Medical   segmentation maps</div>
<div class="meta-line">Authors: Leo Segre, Or Hirschorn, Dvir Ginzburg, Dan Raviv</div>
<div class="meta-line">First: 2022-01-24T13:57:31+00:00 · Latest: 2022-02-04T07:24:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2201.09693v2">Abs</a> · <a href="http://arxiv.org/pdf/2201.09693v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image translation across domains for unpaired datasets has gained interest
and great improvement lately. In medical imaging, there are multiple imaging
modalities, with very different characteristics. Our goal is to use
cross-modality adaptation between CT and MRI whole cardiac scans for semantic
segmentation. We present a segmentation network using synthesised cardiac
volumes for extremely limited datasets. Our solution is based on a 3D
cross-modality generative adversarial network to share information between
modalities and generate synthesized data using unpaired datasets. Our network
utilizes semantic segmentation to improve generator shape consistency, thus
creating more realistic synthesised volumes to be used when re-training the
segmentation network. We show that improved segmentation can be achieved on
small datasets when using spatial augmentations to improve a generative
adversarial network. These augmentations improve the generator capabilities,
thus enhancing the performance of the Segmentor. Using only 16 CT and 16 MRI
cardiovascular volumes, improved results are shown over other segmentation
methods while using the suggested architecture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>形状一致的生成对抗网络用于多模态医学分割图</div>
<div class="mono" style="margin-top:8px">跨领域图像翻译在无配对数据集上引起了关注并取得了显著进步。在医学成像中，存在多种具有非常不同特性的成像模态。我们的目标是利用CT和MRI全心脏扫描之间的跨模态适应进行语义分割。我们提出了一种使用合成心脏体积的分割网络，以应对极度有限的数据集。我们的解决方案基于一个3D跨模态生成对抗网络，用于在模态之间共享信息并使用无配对数据集生成合成数据。我们的网络利用语义分割来提高生成器的形状一致性，从而创建更真实的合成体积，用于重新训练分割网络。我们展示了在使用空间增强改进生成对抗网络时，可以在小数据集上实现更好的分割效果。这些增强提高了生成器的能力，从而增强了分割器的性能。仅使用16个CT和16个MRI心血管体积，与建议的架构相比，所展示的结果优于其他分割方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve semantic segmentation in medical imaging by adapting between CT and MRI scans using a 3D generative adversarial network. The method involves generating synthetic cardiac volumes to enhance the performance of the segmentation network, especially for small datasets. Key findings show that using spatial augmentations in the generative adversarial network leads to better segmentation results, even with only 16 CT and 16 MRI volumes.</div>
<div class="mono" style="margin-top:8px">研究旨在通过在CT和MRI扫描之间进行适配，使用3D生成对抗网络改进医学影像的语义分割。方法包括生成合成的心脏体积以增强分割网络的性能，特别是对于小数据集。关键发现表明，使用生成对抗网络中的空间增强可以更好地实现分割结果，即使只有16个CT和16个MRI心血管体积。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchy of GANs for learning embodied self-awareness model</div>
<div class="meta-line">Authors: Mahdyar Ravanbakhsh, Mohamad Baydoun, Damian Campo, Pablo Marin, David Martin, Lucio Marcenaro, Carlo S. Regazzoni</div>
<div class="meta-line">First: 2018-06-08T13:24:57+00:00 · Latest: 2018-06-08T13:24:57+00:00</div>
<div class="meta-line">Comments: 2018 IEEE International Conference on Image Processing - ICIP&#x27;18.
  arXiv admin note: text overlap with arXiv:1806.02609</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/1806.04012v1">Abs</a> · <a href="http://arxiv.org/pdf/1806.04012v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years several architectures have been proposed to learn embodied
agents complex self-awareness models. In this paper, dynamic incremental
self-awareness (SA) models are proposed that allow experiences done by an agent
to be modeled in a hierarchical fashion, starting from more simple situations
to more structured ones. Each situation is learned from subsets of private
agent perception data as a model capable to predict normal behaviors and detect
abnormalities. Hierarchical SA models have been already proposed using low
dimensional sensorial inputs. In this work, a hierarchical model is introduced
by means of a cross-modal Generative Adversarial Networks (GANs) processing
high dimensional visual data. Different levels of the GANs are detected in a
self-supervised manner using GANs discriminators decision boundaries. Real
experiments on semi-autonomous ground vehicles are presented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于GAN的体态自我意识模型层次结构学习</div>
<div class="mono" style="margin-top:8px">近年来，提出了多种架构来学习复杂体态代理的自我意识模型。本文提出了动态增量自我意识(SA)模型，允许将代理的经验以层次结构的方式建模，从更简单的场景开始，逐步到更结构化的场景。每个场景都是从代理私有感知数据的子集中学习的，作为能够预测正常行为并检测异常情况的模型。已经使用低维感官输入提出了层次SA模型。在本文中，通过处理高维视觉数据的跨模态生成对抗网络(GANs)引入了层次模型。GANs判别器决策边界以自监督方式检测GANs的不同层次。展示了半自主地面车辆的实际实验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a dynamic incremental self-awareness (SA) model for embodied agents, which learns SA in a hierarchical manner from simple to structured situations. The model uses cross-modal GANs to process high-dimensional visual data, with different GAN levels detected using discriminators&#x27; decision boundaries. Experiments on semi-autonomous ground vehicles demonstrate the model&#x27;s effectiveness in predicting normal behaviors and detecting abnormalities in complex scenarios.</div>
<div class="mono" style="margin-top:8px">本文提出了一种动态增量自我意识（SA）模型，该模型能够从简单到复杂的情况层次化地学习SA。该模型使用跨模态GAN处理高维视觉数据，并通过判别器的决策边界检测不同GAN层次。半自主地面车辆的实际实验表明，该模型在复杂场景中能够有效预测正常行为并检测异常情况。</div>
</details>
</div>
<div class="card">
<div class="title">CM-GANs: Cross-modal Generative Adversarial Networks for Common   Representation Learning</div>
<div class="meta-line">Authors: Yuxin Peng, Jinwei Qi, Yuxin Yuan</div>
<div class="meta-line">First: 2017-10-14T00:15:56+00:00 · Latest: 2018-04-26T16:38:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/1710.05106v2">Abs</a> · <a href="http://arxiv.org/pdf/1710.05106v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It is known that the inconsistent distribution and representation of
different modalities, such as image and text, cause the heterogeneity gap that
makes it challenging to correlate such heterogeneous data. Generative
adversarial networks (GANs) have shown its strong ability of modeling data
distribution and learning discriminative representation, existing GANs-based
works mainly focus on generative problem to generate new data. We have
different goal, aim to correlate heterogeneous data, by utilizing the power of
GANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs
to learn discriminative common representation for bridging heterogeneity gap.
The main contributions are: (1) Cross-modal GANs architecture is proposed to
model joint distribution over data of different modalities. The inter-modality
and intra-modality correlation can be explored simultaneously in generative and
discriminative models. Both of them beat each other to promote cross-modal
correlation learning. (2) Cross-modal convolutional autoencoders with
weight-sharing constraint are proposed to form generative model. They can not
only exploit cross-modal correlation for learning common representation, but
also preserve reconstruction information for capturing semantic consistency
within each modality. (3) Cross-modal adversarial mechanism is proposed, which
utilizes two kinds of discriminative models to simultaneously conduct
intra-modality and inter-modality discrimination. They can mutually boost to
make common representation more discriminative by adversarial training process.
To the best of our knowledge, our proposed CM-GANs approach is the first to
utilize GANs to perform cross-modal common representation learning. Experiments
are conducted to verify the performance of our proposed approach on cross-modal
retrieval paradigm, compared with 10 methods on 3 cross-modal datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CM-GANs：跨模态生成对抗网络在共同表示学习中的应用</div>
<div class="mono" style="margin-top:8px">已知不同模态（如图像和文本）的分布和表示不一致，导致异质性差距，使得关联此类异质数据变得具有挑战性。生成对抗网络（GANs）展示了其强大的数据分布建模能力和学习判别表示的能力，现有的基于GANs的工作主要集中在生成问题上，生成新的数据。我们的目标不同，旨在关联异质数据，通过利用GANs建模跨模态联合分布的能力。因此，我们提出了跨模态GANs，以学习跨模态判别共同表示，以弥合异质性差距。主要贡献包括：(1) 提出了跨模态GANs架构，以建模不同模态数据的联合分布。生成模型和判别模型可以同时探索跨模态和同模态的相关性，两者相互促进跨模态相关性学习。(2) 提出了具有权重共享约束的跨模态卷积自编码器，以形成生成模型。它们不仅可以利用跨模态相关性学习共同表示，还可以保留重建信息以捕捉每个模态内的语义一致性。(3) 提出了跨模态对抗机制，利用两种类型的判别模型同时进行同模态和跨模态的判别。通过对抗训练过程，它们可以相互增强，使共同表示更具判别性。据我们所知，我们提出的CM-GANs方法是首次利用GANs进行跨模态共同表示学习。实验验证了我们提出的方法在跨模态检索范式中的性能，与3个跨模态数据集上的10种方法进行了比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the heterogeneity gap between different modalities like images and text by leveraging GANs to learn a common representation. The proposed CM-GANs model introduces a cross-modal GAN architecture to model joint distributions and uses cross-modal convolutional autoencoders with weight-sharing constraints to generate common representations. The model also employs a cross-modal adversarial mechanism to enhance discriminative learning. Experiments show that CM-GANs outperforms 10 other methods on three cross-modal datasets in cross-modal retrieval tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过生成对抗网络（GANs）解决不同模态（如图像和文本）之间的异质性差距问题。提出的CM-GANs模型通过建模跨模态数据的联合分布来学习共同表示。关键发现包括通过对抗训练提高跨模态相关性，并在三个跨模态数据集上与10种其他方法相比，在跨模态检索方面表现出更好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SyncGAN: Synchronize the Latent Space of Cross-modal Generative   Adversarial Networks</div>
<div class="meta-line">Authors: Wen-Cheng Chen, Chien-Wen Chen, Min-Chun Hu</div>
<div class="meta-line">First: 2018-04-02T06:27:50+00:00 · Latest: 2018-04-02T06:27:50+00:00</div>
<div class="meta-line">Comments: 9 pages, Part of this work is accepted by IEEE International
  Conference on Multimedia Expo 2018</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/1804.00410v1">Abs</a> · <a href="http://arxiv.org/pdf/1804.00410v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative adversarial network (GAN) has achieved impressive success on
cross-domain generation, but it faces difficulty in cross-modal generation due
to the lack of a common distribution between heterogeneous data. Most existing
methods of conditional based cross-modal GANs adopt the strategy of
one-directional transfer and have achieved preliminary success on text-to-image
transfer. Instead of learning the transfer between different modalities, we aim
to learn a synchronous latent space representing the cross-modal common
concept. A novel network component named synchronizer is proposed in this work
to judge whether the paired data is synchronous/corresponding or not, which can
constrain the latent space of generators in the GANs. Our GAN model, named as
SyncGAN, can successfully generate synchronous data (e.g., a pair of image and
sound) from identical random noise. For transforming data from one modality to
another, we recover the latent code by inverting the mappings of a generator
and use it to generate data of different modality. In addition, the proposed
model can achieve semi-supervised learning, which makes our model more flexible
for practical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncGAN: 同步跨模态生成对抗网络的潜在空间</div>
<div class="mono" style="margin-top:8px">生成对抗网络（GAN）在跨域生成方面取得了显著成果，但在跨模态生成方面由于异构数据缺乏共同分布而面临困难。现有的大多数基于条件的跨模态GAN方法采用单向转移策略，并在文本到图像转换方面取得了初步成功。我们不是学习不同模态之间的转移，而是旨在学习一个同步的潜在空间来表示跨模态的共同概念。本文提出了一种名为同步器的新网络组件，用于判断配对数据是否同步/对应，这可以约束GAN生成器的潜在空间。我们的GAN模型，名为SyncGAN，可以从相同的随机噪声生成同步数据（例如，一对图像和声音）。对于从一种模态转换到另一种模态的数据，我们通过反转生成器的映射恢复潜在代码，并使用它生成不同模态的数据。此外，所提出的模型可以实现半监督学习，使我们的模型在实际应用中更具灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of cross-modal generation in GANs by synchronizing the latent spaces of different modalities. The authors introduce a synchronizer component to ensure that paired data is consistent, leading to the development of SyncGAN. This model can generate synchronized data (e.g., image and sound) from the same random noise and can also transform data from one modality to another using the latent code. Additionally, the model supports semi-supervised learning, enhancing its practical applicability.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出SyncGAN解决GAN在跨模态生成中的挑战，该模型引入了同步器来学习不同模态的共同潜在空间。方法包括从相同随机噪声生成同步数据，并使用同步器确保潜在空间在不同模态之间一致。主要发现包括成功生成配对的同步数据（如图像和声音）以及实现半监督学习，增强其实用性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250919_1904.html">20250919_1904</a>
<a href="archive/20250919_1023.html">20250919_1023</a>
<a href="archive/20250919_0332.html">20250919_0332</a>
<a href="archive/20250918_2047.html">20250918_2047</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250918_0326.html">20250918_0326</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250917_0326.html">20250917_0326</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250916_0328.html">20250916_0328</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
