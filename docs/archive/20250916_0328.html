<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-16 03:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250916_0328</div>
    <div class="row"><div class="card">
<div class="title">VSRM: A Robust Mamba-Based Framework for Video Super-Resolution</div>
<div class="meta-line">Authors: Dinh Phu Tran, Dao Duy Hung, Daeyoung Kim</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-06-28T05:51:42+00:00 · Latest: 2025-09-02T07:31:44+00:00</div>
<div class="meta-line">Comments: Arxiv version of ICCV 2025 paper</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.22762v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.22762v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VSRM：一种基于Mamba的视频超分辨率稳健框架</div>
<div class="mono" style="margin-top:8px">视频超分辨率仍然是低级视觉任务中的主要挑战。
到目前为止，基于CNN和Transformer的方法已经取得了令人印象深刻的成果。
然而，CNN受限于局部感受野，而Transformer则面临二次复杂度问题，这给处理VSR中的长序列带来了挑战。最近，Mamba因其长序列建模、线性复杂度和大感受野而引起了关注。在本文中，我们提出了一种新颖的视频超分辨率框架VSRM，该框架利用了Mamba的强大功能。VSRM引入了空间到时间Mamba和时间到空间Mamba块，以提取长距离时空特征并高效增强感受野。为了更好地对齐相邻帧，我们提出了可变形跨Mamba对齐模块。该模块利用可变形跨Mamba机制使补偿阶段更具动态性和灵活性，防止特征失真。最后，我们通过提出一种简单而有效的频域Charbonnier似损失来最小化重建帧和真实帧之间的频域差距，从而更好地保留高频内容并提高视觉质量。通过广泛的实验，VSRM在多种基准上取得了最先进的结果，为未来的研究奠定了坚实的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges in video super-resolution by leveraging Mamba&#x27;s long-sequence modeling and linear complexity. VSRM introduces Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks to enhance long-range spatio-temporal feature extraction and receptive fields. The Deformable Cross-Mamba Alignment module ensures better alignment between adjacent frames, and a Frequency Charbonnier-like loss is proposed to preserve high-frequency content. Extensive experiments show that VSRM achieves state-of-the-art results on various benchmarks, making it a robust foundation for future research.</div>
<div class="mono" style="margin-top:8px">研究旨在利用Mamba的长序列建模能力和线性复杂度解决视频超分辨率的挑战。VSRM引入了空间到时间Mamba和时间到空间Mamba块以增强特征提取和感受野，并提出了可变形交叉Mamba对齐模块以改善帧对齐。还提出了频域Charbonnier-like损失以最小化重建帧与真实帧之间的频域差距。实验表明，VSRM在多种基准上达到了最先进的效果，证明了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CMFDNet: Cross-Mamba and Feature Discovery Network for Polyp   Segmentation</div>
<div class="meta-line">Authors: Feng Jiang, Zongfei Zhang, Xin Xu</div>
<div class="meta-line">First: 2025-08-25T07:12:00+00:00 · Latest: 2025-08-25T07:12:00+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures, 2 tables. This paper has been accepted by ICONIP
  2025 but not published</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.17729v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.17729v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated colonic polyp segmentation is crucial for assisting doctors in
screening of precancerous polyps and diagnosis of colorectal neoplasms.
Although existing methods have achieved promising results, polyp segmentation
remains hindered by the following limitations,including: (1) significant
variation in polyp shapes and sizes, (2) indistinct boundaries between polyps
and adjacent tissues, and (3) small-sized polyps are easily overlooked during
the segmentation process. Driven by these practical difficulties, an innovative
architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD
module. The CMD module, serving as an innovative decoder, introduces a
cross-scanning method to reduce blurry boundaries. The MSA module adopts a
multi-branch parallel structure to enhance the recognition ability for polyps
with diverse geometries and scale distributions. The FD module establishes
dependencies among all decoder features to alleviate the under-detection of
polyps with small-scale features. Experimental results show that CMFDNet
outperforms six SOTA methods used for comparison, especially on ETIS and
ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and
1.55%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMFDNet：跨Mamba和特征发现网络用于息肉分割</div>
<div class="mono" style="margin-top:8px">自动结肠息肉分割对于帮助医生筛查癌前息肉和诊断结直肠肿瘤至关重要。尽管现有方法取得了令人鼓舞的结果，但息肉分割仍受到以下限制的阻碍，包括：(1) 息肉形状和大小的显著变化，(2) 息肉与相邻组织边界模糊不清，(3) 小型息肉在分割过程中容易被忽视。受这些实际困难的驱动，提出了一种创新架构CMFDNet，包括CMD模块、MSA模块和FD模块。CMD模块作为创新的解码器，引入了交叉扫描方法以减少模糊边界。MSA模块采用多分支并行结构以增强对具有不同几何形状和尺度分布的息肉的识别能力。FD模块建立了所有解码特征之间的依赖关系，以缓解小尺度特征息肉的漏检问题。实验结果表明，CMFDNet在与六个SOTA方法的比较中表现出色，特别是在ETIS和ColonDB数据集中，mDice分数分别比最佳SOTA方法高出1.83%和1.55%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automated colonic polyp segmentation is crucial for assisting doctors in screening of precancerous polyps and diagnosis of colorectal neoplasms.</div>
<div class="mono" style="margin-top:8px">CMFDNet 旨在解决结肠息肉分割中的挑战，包括形状和大小的差异、边界不清晰以及小息肉的检测。它引入了 CMD 模块以实现更清晰的边界划分、MSA 模块以增强对不同几何形状和尺度分布的息肉的识别能力，以及 FD 模块以改善小息肉的检测。实验结果显示，CMFDNet 在 ETIS 和 ColonDB 数据集上的 mDice 分数分别比最佳方法高出 1.83% 和 1.55%。</div>
</details>
</div>
<div class="card">
<div class="title">MambaFlow: A Mamba-Centric Architecture for End-to-End Optical Flow   Estimation</div>
<div class="meta-line">Authors: Juntian Du, Zhihu Zhou, Runzhe Zhang, Yuan Sun, Pinyi Chen, Keji Mao</div>
<div class="meta-line">First: 2025-03-10T08:33:54+00:00 · Latest: 2025-08-18T03:53:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.07046v4">Abs</a> · <a href="http://arxiv.org/pdf/2503.07046v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the Mamba architecture has demonstrated significant successes in
various computer vision tasks, such as classification and segmentation.
However, its application to optical flow estimation remains unexplored. In this
paper, we introduce MambaFlow, a novel framework designed to leverage the high
accuracy and efficiency of the Mamba architecture for capturing locally
correlated features while preserving global information in end-to-end optical
flow estimation. To our knowledge, MambaFlow is the first architecture centered
around the Mamba design tailored specifically for optical flow estimation. It
comprises two key components: (1) PolyMamba, which enhances feature
representation through a dual-Mamba architecture, incorporating a Self-Mamba
module for intra-token modeling and a Cross-Mamba module for inter-modality
interaction, enabling both deep contextualization and effective feature fusion;
and (2) PulseMamba, which leverages an Attention Guidance Aggregator (AGA) to
adaptively integrate features with dynamically learned weights in contrast to
naive concatenation, and then employs the intrinsic recurrent mechanism of
Mamba to perform autoregressive flow decoding, facilitating efficient flow
information dissemination. Extensive experiments demonstrate that MambaFlow
achieves remarkable results comparable to mainstream methods on benchmark
datasets. Compared to SEA-RAFT, MambaFlow attains higher accuracy on the Sintel
benchmark, demonstrating stronger potential for real-world deployment on
resource-constrained devices. The source code will be made publicly available
upon acceptance of the paper.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MambaFlow：一种基于Mamba的端到端光学流估计架构</div>
<div class="mono" style="margin-top:8px">最近，Mamba架构在各种计算机视觉任务，如分类和分割中取得了显著的成功。然而，其在光学流估计中的应用尚未被探索。在本文中，我们介绍了MambaFlow，这是一种新颖的框架，旨在利用Mamba架构的高准确性和效率来捕捉局部相关特征，同时保留全局信息进行端到端的光学流估计。据我们所知，MambaFlow是第一个围绕Mamba设计中心化构建的架构，专门用于光学流估计。它包括两个关键组件：(1) PolyMamba，通过双重Mamba架构增强特征表示，结合Self-Mamba模块进行内部标记建模和Cross-Mamba模块进行跨模态交互，实现深层次的上下文化和有效的特征融合；(2) PulseMamba，利用注意力引导聚合器(AGA)自适应地集成具有动态学习权重的特征，而不是简单的连接，然后利用Mamba的固有递归机制进行自回归流解码，促进流信息的高效传播。广泛的实验表明，MambaFlow在基准数据集上的结果与主流方法相当。与SEA-RAFT相比，MambaFlow在Sintel基准上获得了更高的准确性，显示出在资源受限设备上进行实际部署的更强潜力。论文被接受后，源代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MambaFlow is a novel framework that leverages the Mamba architecture for optical flow estimation, enhancing feature representation and integrating global information. It consists of PolyMamba, which uses a dual-Mamba architecture with Self-Mamba and Cross-Mamba modules, and PulseMamba, which employs an Attention Guidance Aggregator and Mamba&#x27;s intrinsic mechanism for efficient flow decoding. Experiments show that MambaFlow outperforms mainstream methods and achieves higher accuracy on the Sintel benchmark compared to SEA-RAFT, making it suitable for resource-constrained devices.</div>
<div class="mono" style="margin-top:8px">MambaFlow 是一种利用 Mamba 架构进行光学流估计的新框架，增强特征表示并适应性地集成特征。它包括 PolyMamba 用于深度上下文化和特征融合，以及 PulseMamba 用于高效的流解码。实验表明，MambaFlow 在基准数据集上优于主流方法，特别是在 Sintel 基准上表现更优，显示出其在资源受限设备上的实际部署潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning   Method with State Space Model for PolSAR Image Classification</div>
<div class="meta-line">Authors: Zuzheng Kuang, Haixia Bi, Chen Xu, Jian Sun</div>
<div class="meta-line">First: 2025-06-01T14:52:54+00:00 · Latest: 2025-06-01T14:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.01040v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.01040v1">PDF</a> · <a href="https://github.com/HaixiaBi1982/ECP_Mamba">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, polarimetric synthetic aperture radar (PolSAR) image classification
has been greatly promoted by deep neural networks. However,current deep
learning-based PolSAR classification methods encounter difficulties due to its
dependence on extensive labeled data and the computational inefficiency of
architectures like Transformers. This paper presents ECP-Mamba, an efficient
framework integrating multi-scale self-supervised contrastive learning with a
state space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation
scarcity through a multi-scale predictive pretext task based on local-to-global
feature correspondences, which uses a simplified self-distillation paradigm
without negative sample pairs. To enhance computational efficiency,the Mamba
architecture (a selective SSM) is first tailored for pixel-wise PolSAR
classification task by designing a spiral scan strategy. This strategy
prioritizes causally relevant features near the central pixel, leveraging the
localized nature of pixel-wise classification tasks. Additionally, the
lightweight Cross Mamba module is proposed to facilitates complementary
multi-scale feature interaction with minimal overhead. Extensive experiments
across four benchmark datasets demonstrate ECP-Mamba&#x27;s effectiveness in
balancing high accuracy with resource efficiency. On the Flevoland 1989
dataset, ECP-Mamba achieves state-of-the-art performance with an overall
accuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of
99.62e-2. Our code will be available at
https://github.com/HaixiaBi1982/ECP_Mamba.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECP-Mamba：一种基于状态空间模型的高效多尺度自监督对比学习方法用于极化SAR图像分类</div>
<div class="mono" style="margin-top:8px">近年来，深度神经网络极大地推动了极化合成孔径雷达（PolSAR）图像分类。然而，当前基于深度学习的PolSAR分类方法由于依赖大量标注数据和Transformer等架构的计算效率低下而遇到困难。本文提出了一种高效框架ECP-Mamba，该框架结合了多尺度自监督对比学习和状态空间模型（SSM）骨干网络。具体而言，ECP-Mamba通过基于局部到全局特征对应关系的多尺度预测预训练任务解决了标注稀缺问题，该任务采用简化自蒸馏范式，无需负样本对。为了提高计算效率，首先通过设计螺旋扫描策略为像素级PolSAR分类任务定制了Mamba架构（一种选择性SSM），该策略优先处理与中心像素因果相关的特征，利用像素级分类任务的局部性质。此外，还提出了轻量级的Cross Mamba模块，以实现最少开销的多尺度特征互补交互。在四个基准数据集上的广泛实验表明，ECP-Mamba在保持高准确率的同时具有资源效率。在Flevoland 1989数据集上，ECP-Mamba实现了99.70%的整体准确率、99.64%的平均准确率和99.62e-2的Kappa系数。我们的代码将在https://github.com/HaixiaBi1982/ECP_Mamba上提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ECP-Mamba is a framework that integrates multi-scale self-supervised contrastive learning with a state space model (SSM) for PolSAR image classification. It addresses the scarcity of labeled data through a multi-scale predictive pretext task and enhances computational efficiency by using a spiral scan strategy and a lightweight Cross Mamba module. Experiments show that ECP-Mamba achieves high accuracy and resource efficiency, with state-of-the-art performance on the Flevoland 1989 dataset.</div>
<div class="mono" style="margin-top:8px">ECP-Mamba 是一种结合了多尺度自监督对比学习和状态空间模型（SSM）的高效框架，用于 PolSAR 图像分类。它通过多尺度预测预训练任务解决标注稀缺问题，并采用简化的自我蒸馏范式。Mamba 架构通过螺旋扫描策略进行定制，优先处理与中心像素相关的因果相关特征，适用于像素级分类任务。轻量级的 Cross Mamba 模块促进了多尺度特征的交互。实验表明，ECP-Mamba 在资源效率和高精度方面表现出色，在 Flevoland 1989 数据集上达到最佳性能，总体准确率为 99.70%，Kappa 系数为 99.62e-2。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Duality Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu</div>
<div class="meta-line">First: 2025-05-05T10:36:52+00:00 · Latest: 2025-05-06T07:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.02549v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.02549v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) aims to
retrieve pedestrian images across different modalities without costly
annotations, but faces challenges due to the modality gap and lack of
supervision. Existing methods often adopt self-training with
clustering-generated pseudo-labels but implicitly assume these labels are
always correct. In practice, however, this assumption fails due to inevitable
pseudo-label noise, which hinders model learning. To address this, we introduce
a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),
characterized by three key challenges: noise overfitting, error accumulation,
and noisy cluster correspondence. To this end, we propose a novel Robust
Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy
pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning
mechanism (RAL) is proposed to dynamically emphasize clean samples while
down-weighting noisy ones. Second, to alleviate error accumulation-where the
model reinforces its own mistakes-RoDE employs dual distinct models that are
alternately trained using pseudo-labels from each other, encouraging diversity
and preventing collapse. However, this dual-model strategy introduces
misalignment between clusters across models and modalities, creating noisy
cluster correspondence. To resolve this, we introduce Cluster Consistency
Matching (CCM), which aligns clusters across models and modalities by measuring
cross-cluster similarity. Extensive experiments on three benchmarks demonstrate
the effectiveness of RoDE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒对偶学习在无监督可见-红外行人再识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人再识别（UVI-ReID）旨在无需昂贵标注的情况下，在不同模态中检索行人图像，但由于模态差距和缺乏监督，面临挑战。现有方法通常采用自训练结合聚类生成的伪标签，但隐式假设这些标签总是正确的。然而，在实践中，由于不可避免的伪标签噪声，这一假设会失败，阻碍模型学习。为解决这一问题，我们引入了一种新的学习范式，明确考虑伪标签噪声（PLN），其包含三个关键挑战：噪声过拟合、错误累积和嘈杂的簇对应。为此，我们提出了一种新颖的鲁棒对偶学习框架（RoDE）以减轻噪声伪标签的影响。首先，为对抗噪声过拟合，我们提出了一种鲁棒自适应学习机制（RAL），动态强调干净样本并降低噪声样本的权重。其次，为缓解错误累积，RoDE 使用来自彼此的伪标签交替训练两个不同的模型，鼓励多样性并防止模型崩溃。然而，这种双模型策略引入了模型和模态之间的簇对齐问题，导致嘈杂的簇对应。为解决这一问题，我们引入了簇一致性匹配（CCM），通过测量跨簇相似性来对齐模型和模态之间的簇。在三个基准上的广泛实验表明了RoDE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (UVI-ReID) by introducing a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN). It proposes a Robust Duality Learning framework (RoDE) with a Robust Adaptive Learning mechanism (RAL) to emphasize clean samples and down-weight noisy ones, and Cluster Consistency Matching (CCM) to align clusters across models and modalities. Experiments on three benchmarks show the effectiveness of RoDE in mitigating the effects of noisy pseudo-labels.</div>
<div class="mono" style="margin-top:8px">论文通过提出一种鲁棒二元学习框架（RoDE）来解决无监督可见红外行人再识别（UVI-ReID）中的伪标签噪声问题。RoDE 包括鲁棒自适应学习机制以减轻噪声过拟合，并使用双模型来防止错误累积。为了在模型和模态之间对齐聚类，引入了聚类一致性匹配（CCM）。在三个基准上的实验表明，RoDE 能够在存在噪声伪标签的情况下提高 UVI-ReID 的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised   VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于协作精炼的语义对齐学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需人类注释的情况下，匹配不同模态下同一个体的行人图像。先前的方法通过标签关联算法统一跨模态图像的伪标签，然后设计对比学习框架进行全局特征学习。然而，这些方法忽略了由细粒度模式带来的特征表示和伪标签分布的跨模态变化。这种洞察导致仅优化全局特征时，模态共享学习不足。为解决这一问题，我们提出了一种语义对齐学习与协作精炼（SALCR）框架，该框架为每个模态强调的特定细粒度模式建立优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一种双向全局学习关联（DAGI）模块，以双向方式统一跨模态实例的伪标签。随后，执行了一种细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。然后基于语义对齐特征及其相应的标签空间制定了优化目标。为缓解由噪声伪标签引起的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有最佳方法。我们的代码可在https://github.com/FranklinLingfeng/code-for-SALCR 获取。</div>
</details>
</div>
<div class="card">
<div class="title">TransMamba: Fast Universal Architecture Adaption from Transformers to   Mamba</div>
<div class="meta-line">Authors: Xiuwei Chen, Sihao Lin, Xiao Dong, Zisheng Chen, Meng Cao, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2025-02-21T01:22:01+00:00 · Latest: 2025-02-21T01:22:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.15130v1">Abs</a> · <a href="http://arxiv.org/pdf/2502.15130v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers have been favored in both uni-modal and multi-modal foundation
models for their flexible scalability in attention modules. Consequently, a
number of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, are
publicly available. Recent research has introduced subquadratic architectures
like Mamba, which enables global awareness with linear complexity.
Nevertheless, training specialized subquadratic architectures from scratch for
certain tasks is both resource-intensive and time-consuming. As a motivator, we
explore cross-architecture training to transfer the ready knowledge in existing
Transformer models to alternative architecture Mamba, termed TransMamba. Our
approach employs a two-stage strategy to expedite training new Mamba models,
ensuring effectiveness in across uni-modal and cross-modal tasks. Concerning
architecture disparities, we project the intermediate features into an aligned
latent space before transferring knowledge. On top of that, a Weight Subcloning
and Adaptive Bidirectional distillation method (WSAB) is introduced for
knowledge transfer without limitations on varying layer counts. For cross-modal
learning, we propose a cross-Mamba module that integrates language awareness
into Mamba&#x27;s visual features, enhancing the cross-modal interaction
capabilities of Mamba architecture. Despite using less than 75% of the training
data typically required for training from scratch, TransMamba boasts
substantially stronger performance across various network architectures and
downstream tasks, including image classification, visual question answering,
and text-video retrieval. The code will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TransMamba：从Transformer到Mamba的快速通用架构适配</div>
<div class="mono" style="margin-top:8px">Transformer在单模态和多模态基础模型中因其在注意力模块上的灵活扩展性而受到青睐，因此，诸如LLaVA、CLIP和DEIT等预训练Transformer模型已公开可用。最近的研究引入了次二次架构，如Mamba，它能够在保持线性复杂度的同时实现全局意识。然而，从头开始为特定任务训练专门的次二次架构既耗时又耗资源。为此，我们探索了跨架构训练，将现有Transformer模型中的知识转移到替代架构Mamba，称为TransMamba。我们的方法采用两阶段策略来加速新Mamba模型的训练，确保在单模态和跨模态任务中均有效。针对架构差异，我们将在转移知识之前将中间特征投影到对齐的潜在空间中。此外，我们引入了一种名为WSAB的权重子克隆和自适应双向蒸馏方法，以在不同层数的情况下实现知识转移。对于跨模态学习，我们提出了一种跨Mamba模块，将语言意识整合到Mamba的视觉特征中，增强了Mamba架构的跨模态交互能力。尽管使用少于75%的通常用于从头开始训练的数据，TransMamba在各种网络架构和下游任务（包括图像分类、视觉问答和文本视频检索）中表现出显著更强的性能。代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">TransMamba aims to transfer knowledge from existing Transformer models to the subquadratic Mamba architecture, reducing the need for retraining from scratch. It uses a two-stage strategy and a Weight Subcloning and Adaptive Bidirectional distillation method to align features and transfer knowledge efficiently. TransMamba shows significantly improved performance across various tasks, such as image classification, visual question answering, and text-video retrieval, using less than 75% of the training data typically required for scratch training. The code is publicly available.</div>
<div class="mono" style="margin-top:8px">TransMamba旨在将现有Transformer模型的知识转移到子线性复杂度的Mamba架构中，减少训练时间和资源消耗。它采用两阶段策略和Weight Subcloning和Adaptive Bidirectional distillation方法对齐特征并高效转移知识。TransMamba在图像分类、视觉问答和文本视频检索等多种任务中表现出显著更好的性能，使用不到75%的通常用于从头开始训练的数据量。</div>
</details>
</div>
<div class="card">
<div class="title">Extended Cross-Modality United Learning for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-26T09:30:26+00:00 · Latest: 2024-12-26T09:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.19134v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.19134v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展跨模态联合学习在无监督可见-红外行人再识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人再识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征并减少模态间差距。然而，现有方法缺乏跨模态聚类或过度追求聚类级别的关联，这使得可靠地学习模态不变特征变得困难。为解决这一问题，我们提出了一种扩展跨模态联合学习（ECUL）框架，结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。具体而言，我们设计ECUL以自然地整合模态内聚类、模态间聚类和模态间实例选择，建立紧凑且准确的跨模态关联，同时减少引入噪声标签。此外，EMCC通过扩展编码向量捕获和过滤邻域关系，进一步促进聚类算法中模态不变和相机不变知识的学习。最后，TSMem通过分阶段更新记忆为对比学习提供准确且通用的代理点。在SYSU-MM01和RegDB数据集上的广泛实验结果表明，提出的ECUL表现出色，甚至优于某些监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised learning for visible-infrared person re-identification by proposing an Extended Cross-Modality United Learning (ECUL) framework. This framework includes Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem) to enhance cross-modality clustering and reduce the inter-modality gap. The experiments on SYSU-MM01 and RegDB datasets show that ECUL outperforms certain supervised methods and demonstrates promising performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种扩展跨模态联合学习（ECUL）框架，以解决可见红外行人再识别的无监督学习问题。该框架结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem），以增强跨模态聚类并减少模态间差异。在SYSU-MM01和RegDB数据集上的实验表明，ECUL的性能优于某些监督方法，并表现出良好的效果。</div>
</details>
</div>
<div class="card">
<div class="title">COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal   Object Detection</div>
<div class="meta-line">Authors: Chang Liu, Xin Ma, Xiaochen Yang, Yuxiang Zhang, Yanni Dong</div>
<div class="meta-line">First: 2024-12-24T01:14:48+00:00 · Latest: 2024-12-24T01:14:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.18076v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.18076v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-modal object detection tasks often experience performance degradation
when encountering diverse scenarios. In contrast, multimodal object detection
tasks can offer more comprehensive information about object features by
integrating data from various modalities. Current multimodal object detection
methods generally use various fusion techniques, including conventional neural
networks and transformer-based models, to implement feature fusion strategies
and achieve complementary information. However, since multimodal images are
captured by different sensors, there are often misalignments between them,
making direct matching challenging. This misalignment hinders the ability to
establish strong correlations for the same object across different modalities.
In this paper, we propose a novel approach called the CrOss-Mamba interaction
and Offset-guided fusion (COMO) framework for multimodal object detection
tasks. The COMO framework employs the cross-mamba technique to formulate
feature interaction equations, enabling multimodal serialized state
computation. This results in interactive fusion outputs while reducing
computational overhead and improving efficiency. Additionally, COMO leverages
high-level features, which are less affected by misalignment, to facilitate
interaction and transfer complementary information between modalities,
addressing the positional offset challenges caused by variations in camera
angles and capture times. Furthermore, COMO incorporates a global and local
scanning mechanism in the cross-mamba module to capture features with local
correlation, particularly in remote sensing images. To preserve low-level
features, the offset-guided fusion mechanism ensures effective multiscale
feature utilization, allowing the construction of a multiscale fusion data cube
that enhances detection performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>COMO: 跨马达巴交互和偏移导向融合在多模态目标检测中的应用</div>
<div class="mono" style="margin-top:8px">单模态目标检测任务在面对多样化场景时常常会表现出性能下降。相比之下，多模态目标检测任务可以通过整合多种模态的数据，提供更全面的对象特征信息。当前的多模态目标检测方法通常使用各种融合技术，包括传统的神经网络和基于变换器的模型，来实现特征融合策略并获得互补信息。然而，由于多模态图像是由不同的传感器捕获的，它们之间常常存在对齐问题，这使得直接匹配变得困难。这种对齐问题阻碍了在不同模态之间建立强关联的能力。在本文中，我们提出了一种名为跨马达巴交互和偏移导向融合（COMO）框架的新方法，用于多模态目标检测任务。COMO框架采用跨马达巴技术来制定特征交互方程，实现多模态序列化状态计算，从而实现交互式融合输出，同时减少计算开销并提高效率。此外，COMO利用受对齐影响较小的高层特征，促进模态之间的交互和互补信息的传递，解决由不同相机角度和捕获时间变化引起的位移问题。此外，COMO在跨马达巴模块中引入了全局和局部扫描机制，以捕捉具有局部相关性的特征，特别是在遥感图像中。为了保留低级特征，偏移导向融合机制确保有效利用多尺度特征，从而构建多尺度融合数据立方体，提高检测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the performance of multimodal object detection in diverse scenarios by addressing the challenge of misalignment between different modalities. The COMO framework uses cross-mamba interaction to enable feature interaction and offset-guided fusion to handle positional offsets. Key findings include improved efficiency and detection performance through the use of high-level and low-level features, and the ability to capture local correlations in remote sensing images.</div>
<div class="mono" style="margin-top:8px">该论文提出了COMO框架用于多模态目标检测，解决了不同传感器数据之间的对齐问题。COMO利用交叉马姆巴交互实现高效特征融合，并采用偏移导向融合机制处理位置偏移。主要发现包括通过使用高、低层次特征提高检测性能，并通过减少计算开销提高效率。</div>
</details>
</div>
<div class="card">
<div class="title">Relieving Universal Label Noise for Unsupervised Visible-Infrared Person   Re-Identification by Inferring from Neighbors</div>
<div class="meta-line">Authors: Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin</div>
<div class="meta-line">First: 2024-12-16T04:04:41+00:00 · Latest: 2024-12-16T04:04:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.12220v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.12220v1">PDF</a> · <a href="https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of
great research and practical significance yet remains challenging due to the
absence of annotations. Existing approaches aim to learn modality-invariant
representations in an unsupervised setting. However, these methods often
encounter label noise within and across modalities due to suboptimal clustering
results and considerable modality discrepancies, which impedes effective
training. To address these challenges, we propose a straightforward yet
effective solution for USL-VI-ReID by mitigating universal label noise using
neighbor information. Specifically, we introduce the Neighbor-guided Universal
Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in
both homogeneous and heterogeneous spaces with soft labels derived from
neighboring samples to reduce label noise. Additionally, we present the
Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability
by minimizing the influence of unreliable samples. Extensive experiments on the
RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing
USL-VI-ReID approaches, despite its simplicity. The source code is available
at: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过从邻居推断缓解通用标签噪声以实现无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）具有重要的研究和实际意义，但由于缺乏标注而仍然具有挑战性。现有方法旨在在无监督设置中学习跨模态不变的表示。然而，这些方法常常由于聚类结果欠佳和显著的模态差异而遇到跨模态的标签噪声问题，这阻碍了有效的训练。为了解决这些挑战，我们提出了一种简单而有效的USL-VI-ReID解决方案，通过邻居信息减轻通用标签噪声。具体而言，我们引入了邻居引导的通用标签校准（N-ULC）模块，该模块用来自邻居样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们提出了邻居引导的动态加权（N-DW）模块，通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，但我们的方法仍优于现有USL-VI-ReID方法。源代码可在：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by proposing a method to mitigate universal label noise using neighbor information. The method introduces the Neighbor-guided Universal Label Calibration (N-ULC) module, which uses soft labels from neighboring samples instead of hard pseudo labels to reduce label noise. Additionally, the Neighbor-guided Dynamic Weighting (N-DW) module is used to enhance training stability by minimizing the impact of unreliable samples. Experiments on RegDB and SYSU-MM01 datasets show that this approach outperforms existing methods despite its simplicity.</div>
<div class="mono" style="margin-top:8px">研究通过使用邻居信息来缓解无监督可见-红外行人重识别（USL-VI-ReID）中的普遍标签噪声问题，提出了一种方法。该方法引入了邻居引导的全局标签校准（N-ULC）模块，使用来自邻居样本的软标签代替硬伪标签，并引入了邻居引导的动态加权（N-DW）模块以减少不可靠样本的影响。实验结果表明，该方法在RegDB和SYSU-MM01数据集上优于现有方法，尽管其结构简单。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modality-Camera Invariant Clustering for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yiming Yang, Weipeng Hu, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-11T09:31:03+00:00 · Latest: 2024-12-11T09:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.08231v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.08231v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
offers a more flexible and cost-effective alternative compared to supervised
methods. This field has gained increasing attention due to its promising
potential. Existing methods simply cluster modality-specific samples and employ
strong association techniques to achieve instance-to-cluster or
cluster-to-cluster cross-modality associations. However, they ignore
cross-camera differences, leading to noticeable issues with excessive splitting
of identities. Consequently, this undermines the accuracy and reliability of
cross-modal associations. To address these issues, we propose a novel Dynamic
Modality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID.
Specifically, our DMIC naturally integrates Modality-Camera Invariant Expansion
(MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive
Learning (HMCL) into a unified framework, which eliminates both the
cross-modality and cross-camera discrepancies in clustering. MIE fuses
inter-modal and inter-camera distance coding to bridge the gaps between
modalities and cameras at the clustering level. DNC employs two dynamic search
strategies to refine the network&#x27;s optimization objective, transitioning from
improving discriminability to enhancing cross-modal and cross-camera
generalizability. Moreover, HMCL is designed to optimize instance-level and
cluster-level distributions. Memories for intra-modality and inter-modality
training are updated using randomly selected samples, facilitating real-time
exploration of modality-invariant representations. Extensive experiments have
demonstrated that our DMIC addresses the limitations present in current
clustering approaches and achieve competitive performance, which significantly
reduces the performance gap with supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态模态-摄像机不变聚类在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）相比监督方法提供了更灵活和成本效益更高的替代方案。该领域因其有希望的潜力而越来越受到关注。现有方法简单地聚类模态特定样本，并采用强关联技术实现实例到聚类或聚类到聚类的跨模态关联。然而，它们忽略了跨摄像机差异，导致身份分割过多的问题，从而削弱了跨模态关联的准确性和可靠性。为了解决这些问题，我们提出了一种新的动态模态-摄像机不变聚类（DMIC）框架用于USL-VI-ReID。具体而言，我们的DMIC自然地将模态-摄像机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）统一到一个框架中，消除了聚类中的跨模态和跨摄像机差异。MIE融合了跨模态和跨摄像机的距离编码，在聚类层面弥合了模态和摄像机之间的差距。DNC采用两种动态搜索策略来细化网络的优化目标，从提高可区分性过渡到增强跨模态和跨摄像机的一般性。此外，HMCL旨在优化实例级和聚类级分布。使用随机选择的样本更新模态内和跨模态训练的记忆，促进实时探索模态不变表示。大量实验表明，我们的DMIC解决了当前聚类方法的局限性，实现了竞争力的性能，显著减少了与监督方法的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a Dynamic Modality-Camera Invariant Clustering (DMIC) framework for unsupervised visible-infrared person re-identification (USL-VI-ReID) to address the issue of excessive identity splitting due to cross-camera differences. DMIC integrates Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC), and Hybrid Modality Contrastive Learning (HMCL) to eliminate discrepancies in clustering. MIE bridges gaps between modalities and cameras, DNC refines the network&#x27;s optimization, and HMCL optimizes instance and cluster distributions. Experiments show that DMIC improves cross-modal and cross-camera generalizability, achieving competitive performance with supervised methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有方法忽略跨摄像头差异导致的身份分裂问题，提高无监督的可见光-红外行人再识别。提出的DMIC框架整合了MIE、DNC和HMCL，以消除跨模态和跨摄像头的差异。MIE融合了跨模态和跨摄像头的距离编码，DNC使用动态搜索策略进行优化，而HMCL优化实例和聚类分布。实验表明，DMIC能够实现与监督方法相当的性能，显著缩小了性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Homogeneous and Heterogeneous Consistent Label Associations   for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2024-02-01T15:33:17+00:00 · Latest: 2024-12-04T03:55:35+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.00672v4">Abs</a> · <a href="http://arxiv.org/pdf/2402.00672v4">PDF</a> · <a href="https://github.com/FranklinLingfeng/code_for_MULT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID)
endeavors to retrieve pedestrian images of the same identity from different
modalities without annotations. While prior work focuses on establishing
cross-modality pseudo-label associations to bridge the modality-gap, they
ignore maintaining the instance-level homogeneous and heterogeneous consistency
between the feature space and the pseudo-label space, resulting in coarse
associations. In response, we introduce a Modality-Unified Label Transfer
(MULT) module that simultaneously accounts for both homogeneous and
heterogeneous fine-grained instance-level structures, yielding high-quality
cross-modality label associations. It models both homogeneous and heterogeneous
affinities, leveraging them to quantify the inconsistency between the
pseudo-label space and the feature space, subsequently minimizing it. The
proposed MULT ensures that the generated pseudo-labels maintain alignment
across modalities while upholding structural consistency within intra-modality.
Additionally, a straightforward plug-and-play Online Cross-memory Label
Refinement (OCLR) module is proposed to further mitigate the side effects of
noisy pseudo-labels while simultaneously aligning different modalities, coupled
with an Alternative Modality-Invariant Representation Learning (AMIRL)
framework. Experiments demonstrate that our proposed method outperforms
existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of
our MULT in comparison to other cross-modality association methods. Code is
available at https://github.com/FranklinLingfeng/code_for_MULT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索一致标签关联的同质性和异质性在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，从不同模态中检索同一身份的行人图像。先前的工作主要集中在建立跨模态的伪标签关联以弥补模态差异，但忽略了在特征空间和伪标签空间之间保持实例级的同质性和异质一致性，导致关联粗糙。为此，我们引入了一个统一模态标签转移（MULT）模块，该模块同时考虑了同质性和异质性的细粒度实例级结构，从而产生高质量的跨模态标签关联。该模块利用同质性和异质性亲和力来量化伪标签空间与特征空间之间的不一致性，并随后最小化这种不一致性。所提出的MULT确保生成的伪标签在不同模态之间保持对齐，同时在同模态内保持结构一致性。此外，我们还提出了一种简单的即插即用在线跨记忆标签精炼（OCLR）模块，以进一步减轻噪声伪标签的副作用，同时对齐不同模态，结合了一种交替模态不变表示学习（AMIRL）框架。实验表明，我们提出的方法在现有的USL-VI-ReID方法中表现更优，突显了我们MULT在与其他跨模态关联方法相比的优势。代码可在https://github.com/FranklinLingfeng/code_for_MULT获取。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with   Neighbor-Guided Label Refinement</div>
<div class="meta-line">Authors: De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T04:40:30+00:00 · Latest: 2024-11-03T13:50:53+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12711v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12711v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作学习与邻域引导标签精炼的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征，这对于视频监控系统中的实际应用至关重要。解决USL-VI-ReID任务的关键在于解决跨模态数据关联问题，以便进一步进行异质联合学习。为了解决这一问题，我们提出了一种双最优传输标签分配（DOTLA）框架，以同时将一种模态生成的标签分配给其对应的模态。提出的DOTLA机制将跨模态数据关联问题转化为一种相互强化和高效的解决方案，从而有效减少了某些不足和噪声标签关联的副作用。此外，我们还提出了一种跨模态邻域一致性引导的标签精炼和正则化模块，在假设每个示例的预测或标签分布应与其最近邻相似的前提下，消除不准确监督信号带来的负面影响。在公开的SYSU-MM01和RegDB数据集上的广泛实验结果表明，所提出的方法具有很高的有效性，平均mAP比现有最先进的方法高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the unsupervised learning of visible-infrared person re-identification (USL-VI-ReID) by proposing a Dual Optimal Transport Label Assignment (DOTLA) framework to solve the cross-modality data association problem. This framework assigns generated labels from one modality to its counterpart, enhancing the joint learning process. Additionally, a cross-modality neighbor consistency guided label refinement module is introduced to improve label accuracy. Experimental results on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing state-of-the-art approaches by 7.76% in mean average precision (mAP).</div>
<div class="mono" style="margin-top:8px">论文提出了一种双最优传输标签分配(DOTLA)框架，用于解决无监督可见-红外行人重识别（USL-VI-ReID）中的跨模态数据关联问题。该方法通过基于邻居一致性的标签精炼和正则化来提高标签关联的准确性。该方法在公共数据集SYSU-MM01和RegDB上的平均mAP性能比现有最先进的方法高出7.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised   Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T03:27:46+00:00 · Latest: 2024-11-03T13:48:21+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12673v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12673v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to
match pedestrian images of the same identity from different modalities without
annotations. Existing works mainly focus on alleviating the modality gap by
aligning instance-level features of the unlabeled samples. However, the
relationships between cross-modality clusters are not well explored. To this
end, we propose a novel bilateral cluster matching-based learning framework to
reduce the modality gap by matching cross-modality clusters. Specifically, we
design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)
algorithm through optimizing the maximum matching problem in a bipartite graph.
Then, the matched pairwise clusters utilize shared visible and infrared
pseudo-labels during the model training. Under such a supervisory signal, a
Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework
is proposed to align features jointly at a cluster-level. Meanwhile, the
cross-modality Consistency Constraint (CC) is proposed to explicitly reduce the
large modality discrepancy. Extensive experiments on the public SYSU-MM01 and
RegDB datasets demonstrate the effectiveness of the proposed method, surpassing
state-of-the-art approaches by a large margin of 8.76% mAP on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效双边跨模态簇匹配的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，匹配不同模态下同一身份的行人图像。现有工作主要集中在通过对齐未标注样本的实例级特征来缓解模态差异。然而，跨模态簇之间的关系尚未得到充分探索。为此，我们提出了一种新颖的双边簇匹配学习框架，通过匹配跨模态簇来减少模态差异。具体地，我们通过在二分图中优化最大匹配问题设计了一种多对多双边跨模态簇匹配（MBCCM）算法。然后，匹配的成对簇在模型训练过程中利用共享的可见光和红外伪标签。在这样的监督信号下，我们提出了一种模态特定和模态无关的对比学习框架，以在簇级联合对齐特征。同时，我们提出了跨模态一致性约束（CC）以显式地减少模态差异。在公开的SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法具有很高的有效性，在平均mAP上比最先进的方法高出8.76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by proposing a novel bilateral cluster matching framework. It focuses on reducing the modality gap by matching cross-modality clusters and aligning features at a cluster-level. The method outperforms existing approaches, achieving an average improvement of 8.76% in mAP on public datasets.</div>
<div class="mono" style="margin-top:8px">论文提出了一种新的双边簇匹配框架来解决无监督可见红外行人重识别（USL-VI-ReID）的问题。该框架通过匹配跨模态簇来减少模态差距，并利用共享的伪标签进行训练。方法还引入了簇级对比学习框架和跨模态一致性约束。实验结果表明，该方法在公共数据集上的表现显著优于现有方法，平均mAP提高了8.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Commonality, Divergence and Variety for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-02-29T10:37:49+00:00 · Latest: 2024-10-24T09:00:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.19026v3">Abs</a> · <a href="http://arxiv.org/pdf/2402.19026v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
match specified people in infrared images to visible images without
annotations, and vice versa. USVI-ReID is a challenging yet under-explored
task. Most existing methods address the USVI-ReID using cluster-based
contrastive learning, which simply employs the cluster center as a
representation of a person. However, the cluster center primarily focuses on
commonality, overlooking divergence and variety. To address the problem, we
propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes
method for USVI-ReID. In brief, we generate the hard prototype by selecting the
sample with the maximum distance from the cluster center. We theoretically show
that the hard prototype is used in the contrastive loss to emphasize
divergence. Additionally, instead of rigidly aligning query images to a
specific prototype, we generate the dynamic prototype by randomly picking
samples within a cluster. The dynamic prototype is used to encourage the
variety. Finally, we introduce a progressive learning strategy to gradually
shift the model&#x27;s attention towards divergence and variety, avoiding cluster
deterioration. Extensive experiments conducted on the publicly available
SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习共性、差异性和多样性以进行无监督的可见光-红外人体再识别</div>
<div class="mono" style="margin-top:8px">无监督的可见光-红外人体再识别（USVI-ReID）旨在无需标注的情况下，在红外图像和可见图像之间匹配指定的人，反之亦然。USVI-ReID 是一个具有挑战性但尚未充分探索的任务。现有大多数方法使用基于聚类的对比学习来解决 USVI-ReID，简单地将聚类中心作为人的表示。然而，聚类中心主要关注共性，忽视了差异性和多样性。为了解决这一问题，我们提出了一种渐进对比学习与硬动态原型的方法用于 USVI-ReID。简而言之，我们通过选择与聚类中心距离最大的样本生成硬原型。我们从理论上证明，硬原型用于对比损失中以强调差异。此外，我们不是将查询图像严格对齐到特定的原型，而是通过在聚类内随机选择样本生成动态原型。动态原型用于鼓励多样性。最后，我们引入了一种渐进学习策略，逐步将模型的注意力转向差异性和多样性，避免聚类退化。在公开的 SYSU-MM01 和 RegDB 数据集上进行的大量实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Progressive Contrastive Learning with Hard and Dynamic Prototypes method. This method generates hard prototypes to emphasize divergence and dynamic prototypes to encourage variety, while a progressive learning strategy ensures the model focuses on these aspects. Experiments on SYSU-MM01 and RegDB datasets demonstrate the method&#x27;s effectiveness in USVI-ReID.</div>
<div class="mono" style="margin-top:8px">论文提出了一种渐进对比学习与硬动态原型方法来解决无监督可见-红外行人重识别（USVI-ReID）的问题。该方法通过生成硬原型来强调差异性，生成动态原型来鼓励多样性，并引入渐进学习策略逐步调整模型的关注点。在SYSU-MM01和RegDB数据集上的实验验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Memory Matching for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-01-12T01:24:04+00:00 · Latest: 2024-07-29T09:40:11+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2401.06825v2">Abs</a> · <a href="http://arxiv.org/pdf/2401.06825v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a
promising yet challenging retrieval task. The key challenges in USL-VI-ReID are
to effectively generate pseudo-labels and establish pseudo-label
correspondences across modalities without relying on any prior annotations.
Recently, clustered pseudo-label methods have gained more attention in
USL-VI-ReID. However, previous methods fell short of fully exploiting the
individual nuances, as they simply utilized a single memory that represented an
identity to establish cross-modality correspondences, resulting in ambiguous
cross-modality correspondences. To address the problem, we propose a
Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a
Cross-Modality Clustering (CMC) module to generate the pseudo-labels through
clustering together both two modality samples. To associate cross-modality
clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM)
module, ensuring that optimization explicitly focuses on the nuances of
individual perspectives and establishes reliable cross-modality
correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module
to narrow the modality gap while mitigating the effect of noise pseudo-labels
through a soft many-to-many alignment strategy. Extensive experiments on the
public SYSU-MM01 and RegDB datasets demonstrate the reliability of the
established cross-modality correspondences and the effectiveness of our MMM.
The source codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多记忆匹配在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）是一项有前景但极具挑战性的检索任务。USL-VI-ReID 的关键挑战在于有效生成伪标签，并在不依赖任何先验注释的情况下建立跨模态的伪标签对应关系。最近，聚类伪标签方法在 USL-VI-ReID 中获得了更多关注。然而，之前的方法未能充分利用个体差异，因为它们仅利用单一记忆来表示身份以建立跨模态对应关系，导致跨模态对应关系模糊。为了解决这一问题，我们提出了一种 USL-VI-ReID 的多记忆匹配（MMM）框架。我们首先设计了一个跨模态聚类（CMC）模块，通过聚类两种模态样本来生成伪标签。为了关联跨模态聚类伪标签，我们设计了一个多记忆学习和匹配（MMLM）模块，确保优化明确关注个体视角的差异，并建立可靠的跨模态对应关系。最后，我们设计了一个软聚类级对齐（SCA）模块，在通过软多对多对齐策略减轻噪声伪标签影响的同时，缩小模态差距。在公开的 SYSU-MM01 和 RegDB 数据集上的广泛实验表明，建立的跨模态对应关系的可靠性以及我们 MMM 的有效性。源代码将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Multi-Memory Matching (MMM) framework. This framework includes a Cross-Modality Clustering (CMC) module for generating pseudo-labels and a Multi-Memory Learning and Matching (MMLM) module for establishing reliable cross-modality correspondences. The Soft Cluster-level Alignment (SCA) module further refines these correspondences. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the MMM framework in generating reliable cross-modality correspondences and improving re-identification accuracy.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多记忆匹配（MMM）框架，以解决无监督可见红外行人重识别的挑战。该框架通过跨模态聚类（CMC）模块生成伪标签，并通过多记忆学习和匹配（MMLM）模块建立可靠的跨模态对应关系。软聚类级对齐（SCA）模块进一步通过软多对多对齐策略细化对应关系，减轻噪声影响。在SYSU-MM01和RegDB数据集上的实验表明，该方法能够有效建立准确的跨模态对应关系。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="http://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a
challenging retrieval task that aims to retrieve cross-modality pedestrian
images without using any label information. In this task, the large
cross-modality variance makes it difficult to generate reliable cross-modality
labels, and the lack of annotations also provides additional difficulties for
learning modality-invariant features. In this paper, we first deduce an
optimization objective for unsupervised VI-ReID based on the mutual information
between the model&#x27;s cross-modality input and output. With equivalent
derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy
minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable
cross-modality matching) are obtained. Under their guidance, we design a loop
iterative training strategy alternating between model training and
cross-modality matching. In the matching stage, a uniform prior guided optimal
transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched
visible and infrared prototypes. In the training stage, we utilize this
matching information to introduce prototype-based contrastive learning for
minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive
experimental results on benchmarks demonstrate the effectiveness of our method,
e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any
annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人重识别（USVI-ReID）是一个具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出一个无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一个交替进行模型训练和跨模态匹配的循环训练策略。在匹配阶段，我们提出了一种由均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化类内和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如在SYSU-MM01和RegDB上无任何注释的情况下，Rank-1精度分别为60.6%和90.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: Sharpness (entropy minimization), Fairness (uniform label distribution), and Fitness (reliable cross-modality matching). The method employs a loop iterative training strategy that alternates between model training and cross-modality matching. Specifically, it uses a uniform prior guided optimal transport assignment to select matched visible and infrared prototypes during the matching stage, and introduces prototype-based contrastive learning to minimize intra- and cross-modality entropy during the training stage. Experimental results show that the proposed method achieves 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div>
<div class="mono" style="margin-top:8px">论文通过基于互信息的方法解决了无监督的可见光-红外行人再识别挑战，并提出了三个学习原则：清晰度、公平性和适应性。提出了交替进行模型训练和跨模态匹配的循环迭代训练策略。方法使用均匀先验引导的最优运输分配来选择匹配的原型，并引入基于原型的对比学习来最小化熵。实验结果表明该方法的有效性，分别在SYSU-MM01和RegDB上达到了60.6%和90.3%的Rank-1准确率，且无需标注。</div>
</details>
</div>
<div class="card">
<div class="title">MMR-Mamba: Multi-Modal MRI Reconstruction with Mamba and   Spatial-Frequency Information Fusion</div>
<div class="meta-line">Authors: Jing Zou, Lanqing Liu, Qi Chen, Shujun Wang, Zhanli Hu, Xiaohan Xing, Jing Qin</div>
<div class="meta-line">First: 2024-06-27T07:30:54+00:00 · Latest: 2024-07-07T18:19:38+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figure</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.18950v2">Abs</a> · <a href="http://arxiv.org/pdf/2406.18950v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal MRI offers valuable complementary information for diagnosis and
treatment; however, its utility is limited by prolonged scanning times. To
accelerate the acquisition process, a practical approach is to reconstruct
images of the target modality, which requires longer scanning times, from
under-sampled k-space data using the fully-sampled reference modality with
shorter scanning times as guidance. The primary challenge of this task is
comprehensively and efficiently integrating complementary information from
different modalities to achieve high-quality reconstruction. Existing methods
struggle with this: 1) convolution-based models fail to capture long-range
dependencies; 2) transformer-based models, while excelling in global feature
modeling, struggle with quadratic computational complexity. To address this, we
propose MMR-Mamba, a novel framework that thoroughly and efficiently integrates
multi-modal features for MRI reconstruction, leveraging Mamba&#x27;s capability to
capture long-range dependencies with linear computational complexity while
exploiting global properties of the Fourier domain. Specifically, we first
design a Target modality-guided Cross Mamba (TCM) module in the spatial domain,
which maximally restores the target modality information by selectively
incorporating relevant information from the reference modality. Then, we
introduce a Selective Frequency Fusion (SFF) module to efficiently integrate
global information in the Fourier domain and recover high-frequency signals for
the reconstruction of structural details. Furthermore, we devise an Adaptive
Spatial-Frequency Fusion (ASFF) module, which mutually enhances the spatial and
frequency domains by supplementing less informative channels from one domain
with corresponding channels from the other.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMR-Mamba：基于Mamba和空间-频率信息融合的多模态MRI重建</div>
<div class="mono" style="margin-top:8px">多模态MRI提供了诊断和治疗中宝贵的补充信息；然而，其应用受到长时间扫描的限制。为了加速采集过程，一种实用的方法是从使用较短扫描时间的完全采样参考模态的欠采样k空间数据中，重建目标模态的图像，这需要更长的扫描时间作为指导。这一任务的主要挑战是全面而高效地整合不同模态的补充信息，以实现高质量的重建。现有方法难以应对这一挑战：1）基于卷积的模型无法捕捉长距离依赖性；2）基于变换的模型虽然在全局特征建模方面表现出色，但计算复杂度呈平方增长。为了解决这一问题，我们提出了一种名为MMR-Mamba的新框架，该框架充分利用Mamba的能力，以线性计算复杂度捕捉长距离依赖性，同时利用傅里叶域的全局性质，全面高效地整合多模态特征进行MRI重建。具体而言，我们首先在空间域中设计了一种目标模态引导的交叉Mamba（TCM）模块，该模块通过选择性地从参考模态中整合相关信息，最大限度地恢复目标模态的信息。然后，我们引入了一种选择性频率融合（SFF）模块，以高效地在傅里叶域中整合全局信息并恢复高频信号，以重建结构细节。此外，我们设计了一种自适应空间-频率融合（ASFF）模块，该模块通过在两个域之间补充信息，相互增强空间和频率域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MMR-Mamba is a novel framework designed to accelerate multi-modal MRI reconstruction by integrating spatial-frequency information. It addresses the challenge of efficiently combining information from different modalities using Mamba for capturing long-range dependencies and a Fourier domain-based Selective Frequency Fusion module for high-frequency signal recovery. Key findings include improved reconstruction quality with reduced computational complexity compared to existing methods.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为MMR-Mamba的新框架，通过整合空间和频域信息来加速多模态MRI重建。该框架利用Mamba高效捕捉长距离依赖关系，并引入了Target模态引导的交叉Mamba (TCM) 模块、选择性频域融合 (SFF) 模块和自适应空间-频域融合 (ASFF) 模块，以提高重建质量。主要发现包括相比现有方法，提高了重建精度并降低了计算复杂度。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised   Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Xiangbo Yin, Jiangming Shi, Yachao Zhang, Yang Lu, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-05-09T08:17:06+00:00 · Latest: 2024-05-09T08:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2405.05613v1">Abs</a> · <a href="http://arxiv.org/pdf/2405.05613v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a
formidable challenge, which aims to match pedestrian images across visible and
infrared modalities without any annotations. Recently, clustered pseudo-label
methods have become predominant in USVI-ReID, although the inherent noise in
pseudo-labels presents a significant obstacle. Most existing works primarily
focus on shielding the model from the harmful effects of noise, neglecting to
calibrate noisy pseudo-labels usually associated with hard samples, which will
compromise the robustness of the model. To address this issue, we design a
Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for
USVI-ReID. To be specific, we first introduce a straightforward yet potent
Noisy Pseudo-label Calibration module to correct noisy pseudo-labels. Due to
the high intra-class variations, noisy pseudo-labels are difficult to calibrate
completely. Therefore, we introduce a Neighbor Relation Learning module to
reduce high intra-class variations by modeling potential interactions between
all samples. Subsequently, we devise an Optimal Transport Prototype Matching
module to establish reliable cross-modality correspondences. On that basis, we
design a Memory Hybrid Learning module to jointly learn modality-specific and
modality-invariant information. Comprehensive experiments conducted on two
widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR
outperforms the current state-of-the-art GUR with an average Rank-1 improvement
of 10.3%. The source codes will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒的邻域关系伪标签学习在无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）面临巨大挑战，旨在跨可见光和红外模态匹配行人图像，而无需任何注释。最近，聚类伪标签方法在USVI-ReID中占据主导地位，尽管伪标签中的固有噪声构成了重大障碍。现有大多数工作主要集中在保护模型免受噪声的有害影响，而忽视了通常与硬样本相关的噪声伪标签的校准，这会损害模型的鲁棒性。为解决这一问题，我们为USVI-ReID设计了一种鲁棒的邻域关系伪标签学习（RPNR）框架。具体而言，我们首先引入了一个简单而有效的噪声伪标签校准模块，以纠正噪声伪标签。由于类内变异性高，噪声伪标签难以完全校准。因此，我们引入了邻域关系学习模块，通过建模所有样本之间的潜在交互来降低高类内变异性。随后，我们设计了最优传输原型匹配模块，以建立可靠的跨模态对应关系。在此基础上，我们设计了记忆混合学习模块，以联合学习模态特定和模态不变信息。在两个广泛认可的基准数据集SYSU-MM01和RegDB上进行的全面实验表明，RPNR在平均Rank-1上优于当前最先进的GUR，提高了10.3%。源代码将很快发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework. This framework includes a Noisy Pseudo-label Calibration module to correct noisy pseudo-labels, a Neighbor Relation Learning module to reduce intra-class variations, and an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences. Experimental results on SYSU-MM01 and RegDB benchmarks show that RPNR outperforms the current state-of-the-art method GUR by 10.3% in terms of Rank-1 accuracy.</div>
<div class="mono" style="margin-top:8px">论文提出了一种鲁棒伪标签学习与邻域关系框架（RPNR），以解决无监督可见-红外行人重识别（USVI-ReID）的挑战。该框架包括一个伪标签校准模块以纠正噪声伪标签，一个邻域关系学习模块以减少类内差异，以及一个最优传输原型匹配模块以建立可靠的跨模态对应关系。在SYSU-MM01和RegDB基准上的实验结果表明，RPNR在Rank-1准确率上比当前最先进的方法GUR提高了10.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared ReID via Pseudo-label Correction and   Modality-level Alignment</div>
<div class="meta-line">Authors: Yexin Liu, Weiming Zhang, Athanasios V. Vasilakos, Lin Wang</div>
<div class="meta-line">First: 2024-04-10T02:03:14+00:00 · Latest: 2024-04-10T02:03:14+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2404.06683v1">Abs</a> · <a href="http://arxiv.org/pdf/2404.06683v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) has
recently gained great attention due to its potential for enhancing human
detection in diverse environments without labeling. Previous methods utilize
intra-modality clustering and cross-modality feature matching to achieve
UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be
generated in the clustering process, and 2) the cross-modality feature
alignment via matching the marginal distribution of visible and infrared
modalities may misalign the different identities from two modalities. In this
paper, we first conduct a theoretic analysis where an interpretable
generalization upper bound is introduced. Based on the analysis, we then
propose a novel unsupervised cross-modality person re-identification framework
(PRAISE). Specifically, to address the first challenge, we propose a
pseudo-label correction strategy that utilizes a Beta Mixture Model to predict
the probability of mis-clustering based network&#x27;s memory effect and rectifies
the correspondence by adding a perceptual term to contrastive learning. Next,
we introduce a modality-level alignment strategy that generates paired
visible-infrared latent features and reduces the modality gap by aligning the
labeling function of visible and infrared features to learn identity
discriminative and modality-invariant features. Experimental results on two
benchmark datasets demonstrate that our method achieves state-of-the-art
performance than the unsupervised visible-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外ReID通过伪标签校正和模态级对齐</div>
<div class="mono" style="margin-top:8px">无监督可见-红外人员再识别(UVI-ReID)由于其在不同环境中增强人类检测的潜力而受到广泛关注，无需标注。先前的方法利用同模态聚类和跨模态特征匹配来实现UVI-ReID。然而，存在两个挑战：1) 聚类过程中可能会生成噪声伪标签；2) 通过匹配可见光和红外模态的边缘分布来进行跨模态特征对齐可能会导致两个模态中不同身份的对齐错误。在本文中，我们首先进行理论分析，引入了一个可解释的泛化上界。基于分析，我们提出了一种新颖的无监督跨模态人员再识别框架(PRAISE)。具体而言，为了解决第一个挑战，我们提出了一种伪标签校正策略，利用Beta混合模型预测网络记忆效应导致的误聚类概率，并通过在对比学习中添加感知项来纠正对应关系。接下来，我们引入了一种模态级对齐策略，生成可见光-红外配对的潜在特征，并通过使可见光和红外特征的标签函数对齐来减少模态差距，从而学习身份判别性和模态不变性的特征。在两个基准数据集上的实验结果表明，我们的方法在无监督可见光-ReID方法中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a novel framework called PRAISE. It introduces a pseudo-label correction strategy using a Beta Mixture Model to rectify mis-clustered samples and a modality-level alignment strategy to align the labeling functions of visible and infrared features. The method outperforms existing unsupervised visible-ReID methods on two benchmark datasets.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为PRAISE的新框架，以解决无监督可见红外行人再识别（UVI-ReID）中的挑战。该方法引入了一种使用Beta混合模型进行伪标签校正的策略，以纠正误聚类样本，并引入了一种模态级对齐策略，通过对齐可见光和红外特征的标注函数来减少模态差异。实验结果表明，该方法在两个基准数据集上达到了最先进的性能，证明了其在处理噪声伪标签和模态错位问题方面的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
