<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-05 03:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250905_0335</div>
    <div class="row"><div class="card">
<div class="title">Background Matters Too: A Language-Enhanced Adversarial Framework for   Person Re-Identification</div>
<div class="meta-line">Authors: Kaicong Huang, Talha Azfar, Jack M. Reilly, Thomas Guggisberg, Ruimin Ke</div>
<div class="meta-line">First: 2025-09-03T05:38:22+00:00 · Latest: 2025-09-03T05:38:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.03032v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.03032v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Person re-identification faces two core challenges: precisely locating the
foreground target while suppressing background noise and extracting
fine-grained features from the target region. Numerous visual-only approaches
address these issues by partitioning an image and applying attention modules,
yet they rely on costly manual annotations and struggle with complex
occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic
cues to guide visual understanding. However, they focus solely on foreground
information, but overlook the potential value of background cues. Inspired by
human perception, we argue that background semantics are as important as the
foreground semantics in ReID, as humans tend to eliminate background
distractions while focusing on target appearance. Therefore, this paper
proposes an end-to-end framework that jointly models foreground and background
information within a dual-branch cross-modal feature extraction pipeline. To
help the network distinguish between the two domains, we propose an
intra-semantic alignment and inter-semantic adversarial learning strategy.
Specifically, we align visual and textual features that share the same
semantics across domains, while simultaneously penalizing similarity between
foreground and background features to enhance the network&#x27;s discriminative
power. This strategy drives the model to actively suppress noisy background
regions and enhance attention toward identity-relevant foreground cues.
Comprehensive experiments on two holistic and two occluded ReID benchmarks
demonstrate the effectiveness and generality of the proposed method, with
results that match or surpass those of current state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>背景同样重要：一种增强语言的对抗框架用于行人再识别</div>
<div class="mono" style="margin-top:8px">行人再识别面临两个核心挑战：精确地定位前景目标并抑制背景噪声，以及从目标区域中提取细粒度特征。众多仅基于视觉的方法通过分割图像并应用注意力模块来解决这些问题，但它们依赖于昂贵的手动注释，并且难以处理复杂的遮挡。受CLIP的启发，最近的多模态方法引入了语义线索来引导视觉理解。然而，这些方法仅专注于前景信息，而忽视了背景线索的潜在价值。受人类感知的启发，我们认为在行人再识别中，背景语义与前景语义同样重要，因为人类倾向于在关注目标外观时排除背景干扰。因此，本文提出了一种端到端框架，在双分支跨模态特征提取管道中联合建模前景和背景信息。为了帮助网络区分两个领域，我们提出了一种内在语义对齐和跨语义对抗学习策略。具体来说，我们对跨领域具有相同语义的视觉和文本特征进行对齐，同时惩罚前景和背景特征之间的相似性，以增强网络的判别能力。该策略促使模型积极抑制噪声背景区域，并增强对与身份相关的前景线索的注意力。在两个整体和两个遮挡的行人再识别基准上的全面实验表明，所提出的方法具有有效性和普适性，其结果与当前最先进的方法相当或超越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in person re-identification by proposing an end-to-end framework that jointly models foreground and background information. The method uses a dual-branch cross-modal feature extraction pipeline and an intra-semantic alignment and inter-semantic adversarial learning strategy to enhance the network&#x27;s discriminative power. Experiments on four benchmarks show that the proposed method outperforms or matches current state-of-the-art approaches in both holistic and occluded scenarios.</div>
<div class="mono" style="margin-top:8px">该论文通过提出一个端到端框架，同时建模前景和背景信息来解决人员再识别的挑战。该方法使用一个双分支跨模态特征提取管道，并结合内部语义对齐和跨语义对抗学习来增强判别能力。在四个基准上的实验表明，所提出的方法在整体和遮挡场景中均能超越或匹配当前最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person   Search in Full Images</div>
<div class="meta-line">Authors: Zengli Luo, Canlong Zhang, Zhixin Li, Zhiwen Wang, Chunrong Wei</div>
<div class="meta-line">First: 2025-05-06T14:25:30+00:00 · Latest: 2025-09-01T03:06:50+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures. Accepted by the 18th International Conference on
  Knowledge Science, Engineering and Management (KSEM 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.03567v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.03567v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向全图的具有不确定性意识的原型语义解耦文本基于人员搜索</div>
<div class="mono" style="margin-top:8px">全图中的基于文本的人行道搜索（TBPS）旨在使用自然语言描述在未裁剪的图像中定位目标人行道。然而，在包含多个行人复杂场景中，现有方法受限于检测和匹配中的不确定性，导致性能下降。为了解决这一问题，我们提出了一种新颖的UPD-TBPS框架，包含三个模块：多粒度不确定性估计（MUE）、原型基础不确定性解耦（PUD）和跨模态再识别（ReID）。MUE执行多粒度查询以识别潜在目标并分配置信度分数以减少早期不确定性。PUD利用视觉上下文解耦和原型挖掘来提取查询中描述的目标行人的特征。它在粗粒度聚类级别和细粒度个体级别分离和学习行人的原型表示，从而减少匹配不确定性。ReID评估具有不同置信度级别的候选者，提高检测和检索准确性。在CUHK-SYSU-TBPS和PRW-TBPS数据集上的实验验证了我们框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing text-based pedestrian search methods in complex scenes by proposing UPD-TBPS, which includes Multi-granularity Uncertainty Estimation, Prototype-based Uncertainty Decoupling, and Cross-modal Re-identification modules. The framework reduces uncertainties through multi-granularity queries, visual context decoupling, and prototype mining, and improves detection and retrieval accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets demonstrate the effectiveness of the proposed method.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决检测和匹配中的不确定性，提高复杂场景下多行人中的基于文本的行人搜索。提出的UPD-TBPS框架包括三个模块：多粒度不确定性估计（MUE）、基于原型的不确定性解耦（PUD）和跨模态再识别（ReID）。MUE减少早期不确定性，PUD通过解耦视觉上下文和挖掘原型来提取目标行人的特征，ReID评估不同置信水平的候选者以提高准确性和检索精度。实验结果在CUHK-SYSU-TBPS和PRW-TBPS数据集上验证了该框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FeatureSORT: Essential Features for Effective Tracking</div>
<div class="meta-line">Authors: Hamidreza Hashempoor, Rosemary Koikara, Yu Dong Hwang</div>
<div class="meta-line">First: 2024-07-05T04:37:39+00:00 · Latest: 2025-09-01T02:44:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2407.04249v2">Abs</a> · <a href="http://arxiv.org/pdf/2407.04249v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce FeatureSORT, a simple yet effective online multiple object
tracker that reinforces the DeepSORT baseline with a redesigned detector and
additional feature cues. In contrast to conventional detectors that only
provide bounding boxes, our modified YOLOX architecture is extended to output
multiple appearance attributes, including clothing color, clothing style, and
motion direction, alongside the bounding boxes. These feature cues, together
with a ReID network, form complementary embeddings that substantially improve
association accuracy. Furthermore, we incorporate stronger post-processing
strategies, such as global linking and Gaussian Smoothing Process
interpolation, to handle missing associations and detections. During online
tracking, we define a measurement-to-track distance function that jointly
considers IoU, direction, color, style, and ReID similarity. This design
enables FeatureSORT to maintain consistent identities through longer occlusions
while reducing identity switches. Extensive experiments on standard MOT
benchmarks demonstrate that FeatureSORT achieves state-of-the-art online
performance, with MOTA scores of 79.7 on MOT16, 80.6 on MOT17, 77.9 on MOT20,
and 92.2 on DanceTrack, underscoring the effectiveness of feature-enriched
detection and modular post processing in advancing multi-object tracking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FeatureSORT：有效跟踪的基本特征</div>
<div class="mono" style="margin-top:8px">我们介绍了FeatureSORT，这是一种简单而有效的在线多目标跟踪器，它在DeepSORT基线上通过重新设计的检测器和额外的特征提示进行了增强。与传统的仅提供边界框的检测器不同，我们修改后的YOLOX架构被扩展以输出多个外观属性，包括服装颜色、服装风格和运动方向，以及边界框。这些特征提示，加上一个ReID网络，形成了互补的嵌入，显著提高了关联准确性。此外，我们还引入了更强的后处理策略，如全局链接和高斯平滑过程插值，以处理缺失的关联和检测。在线跟踪过程中，我们定义了一个度量到跟踪的距离函数，该函数同时考虑了IoU、方向、颜色、风格和ReID相似性。这种设计使FeatureSORT能够在更长时间的遮挡下保持一致的身份，同时减少身份切换。在标准MOT基准上的广泛实验表明，FeatureSORT在MOT16上的MOTA得分为79.7，在MOT17上的得分为80.6，在MOT20上的得分为77.9，在DanceTrack上的得分为92.2，这突显了特征丰富检测和模块化后处理在多目标跟踪中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FeatureSORT is an enhanced online multiple object tracker that builds upon the DeepSORT baseline by incorporating a modified YOLOX detector that outputs additional appearance attributes such as clothing color, style, and motion direction. It also uses a ReID network and advanced post-processing techniques like global linking and Gaussian Smoothing Process interpolation to improve association accuracy and handle missing detections. The tracker defines a measurement-to-track distance function that considers multiple factors including IoU, direction, color, style, and ReID similarity, leading to better identity maintenance during occlusions. Experimental results show that FeatureSORT outperforms existing methods on standard MOT benchmarks, achieving high MOTA scores.</div>
<div class="mono" style="margin-top:8px">FeatureSORT 是一种基于 DeepSORT 基线的增强型多目标跟踪器，通过改进的 YOLOX 检测器和服装颜色、风格和运动方向等附加特征来提升性能。它结合了 ReID 网络，并使用全局链接和高斯平滑等高级后处理技术来处理缺失数据。该跟踪器定义了一个考虑多个因素的距离函数，以更好地处理遮挡情况。实验表明，FeatureSORT 在标准基准测试中表现出色，获得了较高的 MOTA 分数。</div>
</details>
</div>
<div class="card">
<div class="title">PS-ReID: Advancing Person Re-Identification and Precise Segmentation   with Multimodal Retrieval</div>
<div class="meta-line">Authors: Jincheng Yan, Yun Wang, Xiaoyan Luo, Yu-Wing Tai</div>
<div class="meta-line">First: 2025-03-27T15:14:03+00:00 · Latest: 2025-08-31T12:50:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.21595v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.21595v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Person re-identification (ReID) plays a critical role in applications such as
security surveillance and criminal investigations. Most traditional image-based
ReID methods face challenges including occlusions and lighting changes, while
text provides complementary information to mitigate these issues. However, the
integration of both image and text modalities remains underexplored. To address
this gap, we propose {\bf PS-ReID}, a multimodal model that combines image and
text inputs to enhance ReID performance. In contrast to existing ReID methods
limited by cropped pedestrian images, our PS-ReID focuses on full-scene
settings and introduces a multimodal ReID task that incorporates segmentation,
enabling precise feature extraction of the queried individual, even under
challenging conditions such as occlusion. To this end, our model adopts a
dual-path asymmetric encoding scheme that explicitly separates query and target
roles: the query branch captures identity-discriminative cues, while the target
branch performs holistic scene reasoning. Additionally, a token-level ReID loss
supervises identity-aware tokens, coupling retrieval and segmentation to yield
masks that are both spatially precise and identity-consistent. To facilitate
systematic evaluation, we construct M2ReID, currently the largest full-scene
multimodal ReID dataset, with over 200K images and 4,894 identities, featuring
multimodal queries and high-quality segmentation masks. Experimental results
demonstrate that PS-ReID significantly outperforms unimodal query-based models
in both ReID and segmentation tasks. The model excels in challenging real-world
scenarios such as occlusion, low lighting, and background clutter, offering a
robust and flexible solution for person retrieval and segmentation. All code,
models, and datasets will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PS-ReID：多模态检索促进人员再识别和精确分割</div>
<div class="mono" style="margin-top:8px">人员再识别（ReID）在安全监控和刑事调查等应用中起着关键作用。大多数传统的基于图像的ReID方法面临遮挡和光照变化的挑战，而文本提供了互补信息以缓解这些问题。然而，图像和文本模态的结合仍然未被充分探索。为了解决这一差距，我们提出了**PS-ReID**，这是一种结合图像和文本输入的多模态模型，以提高ReID性能。与现有的受限于裁剪行人图像的ReID方法不同，我们的PS-ReID专注于全景设置，并引入了一个结合分割的多模态ReID任务，使在遮挡等挑战条件下也能精确提取查询个体的特征。为此，我们的模型采用了一种双路径不对称编码方案，明确地分离查询和目标角色：查询分支捕获身份鉴别线索，而目标分支执行整体场景推理。此外，一种基于token的ReID损失监督身份感知token，将检索和分割耦合起来，生成既空间精确又身份一致的掩码。为了便于系统评估，我们构建了M2ReID，目前是最大的全景多模态ReID数据集，包含超过20万张图像和4,894个身份，具有多模态查询和高质量分割掩码。实验结果表明，PS-ReID在ReID和分割任务中显著优于单模态查询模型。该模型在遮挡、低光照和背景杂乱等具有挑战性的现实场景中表现出色，提供了一种稳健且灵活的人员检索和分割解决方案。所有代码、模型和数据集将公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PS-ReID is a multimodal model that integrates image and text inputs to improve person re-identification (ReID) performance, especially in challenging conditions like occlusions and low lighting. It introduces a dual-path asymmetric encoding scheme and a token-level ReID loss that couples retrieval and segmentation. Experimental results show that PS-ReID outperforms unimodal models in both ReID and segmentation tasks, particularly in real-world scenarios with occlusions and background clutter. The model also includes a large multimodal dataset, M2ReID, for systematic evaluation.</div>
<div class="mono" style="margin-top:8px">PS-ReID 是一种结合图像和文本输入的多模态模型，旨在提高人员再识别（ReID）性能，特别是在遮挡、低光照等挑战性条件下。它引入了一种双路径不对称编码方案和基于标记的 ReID 损失，将检索与分割耦合起来。实验结果表明，PS-ReID 在 ReID 和分割任务中均优于单模态模型，特别是在具有遮挡和背景杂乱的现实世界场景中表现出色。该模型还包含一个大型多模态数据集 M2ReID，用于系统评估。</div>
</details>
</div>
<div class="card">
<div class="title">Generalizable Object Re-Identification via Visual In-Context Prompting</div>
<div class="meta-line">Authors: Zhizhong Huang, Xiaoming Liu</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-28T21:24:06+00:00 · Latest: 2025-08-28T21:24:06+00:00</div>
<div class="meta-line">Comments: ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.21222v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.21222v1">PDF</a> · <a href="https://github.com/Hzzone/VICP">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current object re-identification (ReID) methods train domain-specific models
(e.g., for persons or vehicles), which lack generalization and demand costly
labeled data for new categories. While self-supervised learning reduces
annotation needs by learning instance-wise invariance, it struggles to capture
\textit{identity-sensitive} features critical for ReID. This paper proposes
Visual In-Context Prompting~(VICP), a novel framework where models trained on
seen categories can directly generalize to unseen novel categories using only
\textit{in-context examples} as prompts, without requiring parameter
adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer
semantic identity rules from few-shot positive/negative pairs through
task-specific prompting, which then guides a VFM (\eg, DINO) to extract
ID-discriminative features via \textit{dynamic visual prompts}. By aligning
LLM-derived semantic concepts with the VFM&#x27;s pre-trained prior, VICP enables
generalization to novel categories, eliminating the need for dataset-specific
retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K
object instances from e-commerce platforms, featuring multi-view images and
cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks
demonstrate that VICP outperforms baselines by a clear margin on unseen
categories. Code is available at https://github.com/Hzzone/VICP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过视觉上下文提示实现通用对象重识别</div>
<div class="mono" style="margin-top:8px">当前的对象重识别（ReID）方法训练特定领域的模型（例如，针对人员或车辆），这些模型缺乏泛化能力，需要为新类别提供昂贵的标注数据。虽然自监督学习通过学习实例不变性来减少标注需求，但它难以捕捉到ReID至关重要的身份敏感特征。本文提出了一种新颖的框架——视觉上下文提示（VICP），该框架允许在已见类别上训练的模型仅通过上下文示例作为提示直接泛化到未见的新类别，而无需参数调整。VICP 结合了语言大模型（LLM）和视觉基础模型（VFM）：LLM 通过任务特定的提示从少量正负样本对中推断出语义身份规则，然后引导 VFM（例如，DINO）通过动态视觉提示提取身份区分特征。通过将LLM推断出的语义概念与VFM的预训练先验对齐，VICP 使模型能够泛化到新类别，从而消除针对特定数据集的重新训练需求。为了支持评估，我们引入了包含10K个电子商务平台对象实例的ShopID10K数据集，这些实例具有多视角图像和跨域测试。在ShopID10K和多种ReID基准测试上的实验表明，VICP 在未见类别上明显优于基线方法。代码可在 https://github.com/Hzzone/VICP 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current object re-identification methods by proposing Visual In-Context Prompting (VICP), which enables models trained on seen categories to generalize to unseen categories using in-context examples as prompts. By integrating language models and vision foundation models, VICP infers semantic identity rules and guides feature extraction, achieving better performance on unseen categories compared to baselines. Experiments on ShopID10K and other ReID benchmarks show significant improvements in generalization to novel categories.</div>
<div class="mono" style="margin-top:8px">本文提出了一种视觉上下文提示（VICP）方法，以解决当前对象重识别方法在训练特定领域模型时缺乏泛化能力的问题，无需参数调整即可使模型从已见类别推广到未见类别。VICP 结合语言模型和视觉基础模型来推断语义身份规则并指导特征提取，从而实现对新类别的泛化。实验结果表明，VICP 在 ShopID10K 和其他基准测试中在未见类别上的表现优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved   Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Yan Jiang, Hao Yu, Mengting Wei, Zhaodong Sun, Haoyu Chen, Xu Cheng, Guoying Zhao</div>
<div class="meta-line">First: 2025-03-15T18:56:29+00:00 · Latest: 2025-08-28T14:32:17+00:00</div>
<div class="meta-line">Comments: Extended Version of L2RW. We extend it from image to video data</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.12232v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.12232v2">PDF</a> · <a href="https://github.com/Joey623/L2RW">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) is a challenging task
that aims to match pedestrian images captured under varying lighting
conditions, which has drawn intensive research attention and achieved promising
results. However, existing methods adopt the centralized training, ignoring the
potential privacy concerns as the data is distributed across multiple devices
or entities in reality. In this paper, we propose L2RW+, a benchmark that
brings VI-ReID closer to real-world applications. The core rationale behind
L2RW+ is that incorporating decentralized training into VI-ReID can address
privacy concerns in scenarios with limited data-sharing constrains.
Specifically, we design protocols and corresponding algorithms for different
privacy sensitivity levels. In our new benchmark, we simulate the training
under real-world data conditions that: 1) data from each camera is completely
isolated, or 2) different data entities (e.g., data controllers of a certain
region) can selectively share the data. In this way, we simulate scenarios with
strict privacy restrictions, which is closer to real-world conditions.
Comprehensive experiments show the feasibility and potential of decentralized
VI-ReID training at both image and video levels. In particular, with increasing
data scales, the performance gap between decentralized and centralized training
decreases, especially in video-level VI-ReID. In unseen domains, decentralized
training even achieves performance comparable to SOTA centralized methods. This
work offers a novel research entry for deploying VI-ReID into real-world
scenarios and can benefit the community. Code is available at:
https://github.com/Joey623/L2RW.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>L2RW+: 针对隐私保护的可见光-红外行人重识别综合基准</div>
<div class="mono" style="margin-top:8px">可见光-红外行人重识别（VI-ReID）是一项具有挑战性的任务，旨在匹配在不同光照条件下拍摄的行人图像，已引起广泛关注并取得了显著成果。然而，现有方法采用集中式训练，忽略了实际中数据分布在多个设备或实体之间可能带来的隐私问题。本文提出L2RW+基准，旨在将VI-ReID推向实际应用。L2RW+的核心理念是，在数据共享受限的场景中，将去中心化训练纳入VI-ReID可以解决隐私问题。具体而言，我们设计了不同隐私敏感度级别的协议和相应算法。在我们的新基准中，我们模拟了在实际数据条件下进行训练：1）每个摄像头的数据完全隔离，或2）不同的数据实体（例如，某一地区的数据控制者）可以选择性地共享数据。这样，我们模拟了具有严格隐私限制的场景，更接近实际条件。全面的实验表明，去中心化VI-ReID训练在图像和视频级别上具有可行性和潜力。特别是在数据规模增加时，去中心化和集中式训练之间的性能差距减小，特别是在视频级别的VI-ReID中。在未见领域，去中心化训练甚至达到了与最新集中式方法相当的性能。这项工作为将VI-ReID部署到实际场景提供了新的研究切入点，并可惠及社区。代码可在：https://github.com/Joey623/L2RW 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces L2RW+, a benchmark for visible-infrared person re-identification (VI-ReID) that emphasizes privacy preservation through decentralized training. The method addresses privacy concerns by simulating scenarios where data is isolated or selectively shared among different entities. Experiments show that decentralized training is feasible and can achieve performance comparable to centralized methods, especially in video-level VI-ReID, even in unseen domains.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出L2RW+基准，解决可见-红外行人再识别(VI-ReID)中的隐私问题，该基准采用分散式训练方法，并设计了不同隐私敏感级别的协议，模拟了数据隔离或选择性共享的真实世界数据条件。关键发现表明，分散式训练是可行的，并且在视频级别的VI-ReID中可以达到与集中式方法相当的性能，即使在严格的隐私限制下也是如此。</div>
</details>
</div>
<div class="card">
<div class="title">HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for   Video-based Person ReID</div>
<div class="meta-line">Authors: Yiyang Su, Yunping Shi, Feng Liu, Xiaoming Liu</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-07T05:34:14+00:00 · Latest: 2025-08-27T04:31:10+00:00</div>
<div class="meta-line">Comments: Published at ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.05038v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.05038v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, research interest in person re-identification (ReID) has
increasingly focused on video-based scenarios, which are essential for robust
surveillance and security in varied and dynamic environments. However, existing
video-based ReID methods often overlook the necessity of identifying and
selecting the most discriminative features from both videos in a query-gallery
pair for effective matching. To address this issue, we propose a novel
Hierarchical and Adaptive Mixture of Biometric Experts (HAMoBE) framework,
which leverages multi-layer features from a pre-trained large model (e.g.,
CLIP) and is designed to mimic human perceptual mechanisms by independently
modeling key biometric features--appearance, static body shape, and dynamic
gait--and adaptively integrating them. Specifically, HAMoBE includes two
levels: the first level extracts low-level features from multi-layer
representations provided by the frozen large model, while the second level
consists of specialized experts focusing on long-term, short-term, and temporal
features. To ensure robust matching, we introduce a new dual-input decision
gating network that dynamically adjusts the contributions of each expert based
on their relevance to the input scenarios. Extensive evaluations on benchmarks
like MEVID demonstrate that our approach yields significant performance
improvements (e.g., +13.0% Rank-1 accuracy).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HAMoBE：层次化和自适应生物特征专家混合体用于基于视频的人再识别</div>
<div class="mono" style="margin-top:8px">近年来，关于人再识别（ReID）的研究兴趣越来越多地集中在视频场景上，这对于在多变和动态环境中实现稳健的监控和安全至关重要。然而，现有的基于视频的ReID方法往往忽视了从查询-画廊对中的视频中识别和选择最具区分性的特征以实现有效匹配的必要性。为了解决这一问题，我们提出了一种新颖的层次化和自适应生物特征专家混合体（HAMoBE）框架，该框架利用预训练大型模型（例如，CLIP）的多层特征，并通过独立建模关键生物特征——外观、静态身体形状和动态步态——并自适应地整合它们来模拟人类感知机制。具体而言，HAMoBE 包括两个层次：第一层次从冻结的大型模型提供的多层表示中提取低级特征，而第二层次则由专注于长期、短期和时间特征的专门专家组成。为了确保稳健的匹配，我们引入了一种新的双输入决策门控网络，该网络能够根据输入场景的相关性动态调整每个专家的贡献。在MEVID等基准上的广泛评估表明，我们的方法在性能上取得了显著的改进（例如，Rank-1精度提高13.0%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve video-based person re-identification by addressing the need to identify and select discriminative features from query-gallery pairs. The proposed HAMoBE framework uses a hierarchical structure with a pre-trained model to extract multi-layer features and specialized experts to model appearance, static body shape, and dynamic gait. It also includes a dual-input decision gating network to adaptively integrate these features. Experiments on benchmarks like MEVID show a significant improvement in Rank-1 accuracy by +13.0%.</div>
<div class="mono" style="margin-top:8px">研究旨在通过识别和选择查询-库对中的 discriminative 特征来提高基于视频的人再识别。提出的 HAMoBE 框架使用分层结构和预训练模型来提取多层特征，并使用专门的专家来建模外观、静态身体形状和动态步态。引入了双输入决策门控网络以适应性地整合这些特征。实验表明，HAMoBE 在 MEVID 基准上的性能显著提升，例如 Rank-1 准确率提高了 13.0%。</div>
</details>
</div>
<div class="card">
<div class="title">LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground   Person Re-Identification</div>
<div class="meta-line">Authors: Xiang Hu, Yuhao Wang, Pingping Zhang, Huchuan Lu</div>
<div class="meta-line">First: 2025-03-31T04:47:05+00:00 · Latest: 2025-08-26T02:37:10+00:00</div>
<div class="meta-line">Comments: Add more experiments and improve the writing</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.23722v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.23722v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As an important task in intelligent transportation systems, Aerial-Ground
person Re-IDentification (AG-ReID) aims to retrieve specific persons across
heterogeneous cameras in different viewpoints. Previous methods typically adopt
deep learning-based models, focusing on extracting view-invariant features.
However, they usually overlook the semantic information in person attributes.
In addition, existing training strategies often rely on full fine-tuning
large-scale models, which significantly increases training costs. To address
these issues, we propose a novel framework named LATex for AG-ReID, which
adopts prompt-tuning strategies to leverage attribute-based text knowledge.
More specifically, we first introduce the Contrastive Language-Image
Pre-training (CLIP) model as the backbone, and propose an Attribute-aware Image
Encoder (AIE) to extract both global semantic features and attribute-aware
features from input images. Then, with these features, we propose a Prompted
Attribute Classifier Group (PACG) to predict person attributes and obtain
attribute representations. Finally, we design a Coupled Prompt Template (CPT)
to transform attribute representations and view information into structured
sentences. These sentences are processed by the text encoder of CLIP to
generate more discriminative features. As a result, our framework can fully
leverage attribute-based text knowledge to improve AG-ReID performance.
Extensive experiments on three AG-ReID benchmarks demonstrate the effectiveness
of our proposed methods. The source code will be available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LATex：利用基于属性的文本知识进行空地行人重识别</div>
<div class="mono" style="margin-top:8px">作为智能交通系统中的一个重要任务，空地行人重识别（AG-ReID）旨在跨不同视角的异构摄像头检索特定行人。以往的方法通常采用基于深度学习的模型，专注于提取视角不变特征。然而，它们通常忽略了行人属性中的语义信息。此外，现有的训练策略往往依赖于大规模模型的全面微调，这显著增加了训练成本。为了解决这些问题，我们提出了一种名为LATex的新框架，采用提示调优策略利用基于属性的文本知识。具体而言，我们首先引入对比语言-图像预训练（CLIP）模型作为骨干，并提出一种属性感知图像编码器（AIE）从输入图像中提取全局语义特征和属性感知特征。然后，利用这些特征，我们提出了一种提示属性分类组（PACG）来预测行人属性并获得属性表示。最后，我们设计了一种耦合提示模板（CPT）将属性表示和视图信息转换为结构化句子。这些句子通过CLIP的文本编码器生成更具判别性的特征。因此，我们的框架可以充分利用基于属性的文本知识来提高AG-ReID性能。在三个AG-ReID基准上的广泛实验表明了我们提出方法的有效性。源代码将公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As an important task in intelligent transportation systems, Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific persons across heterogeneous cameras in different viewpoints.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入基于属性的文本知识来提升Aerial-Ground人再识别（AG-ReID）性能。方法使用CLIP模型作为骨干，并引入了属性感知图像编码器（AIE）来提取全局和属性感知特征。Prompted属性分类组（PACG）预测人物属性，耦合提示模板（CPT）将这些属性和视图信息转换为结构化句子，这些句子再由CLIP的文本编码器处理以生成更具区分性的特征。在三个AG-ReID基准上的实验表明了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with   Identification and Pose Estimation in 3x3 Basketball Full-court Videos</div>
<div class="meta-line">Authors: Kazuhiro Yamada, Li Yin, Qingrui Hu, Ning Ding, Shunsuke Iwashita, Jun Ichikawa, Kiwamu Kotani, Calvin Yeung, Keisuke Fujii</div>
<div class="meta-line">First: 2025-03-24T01:55:46+00:00 · Latest: 2025-08-21T04:41:40+00:00</div>
<div class="meta-line">Comments: Accepted in MMSports&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.18282v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.18282v2">PDF</a> · <a href="https://github.com/open-starlab/TrackID3x3">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-object tracking, player identification, and pose estimation are
fundamental components of sports analytics, essential for analyzing player
movements, performance, and tactical strategies. However, existing datasets and
methodologies primarily target mainstream team sports such as soccer and
conventional 5-on-5 basketball, often overlooking scenarios involving
fixed-camera setups commonly used at amateur levels, less mainstream sports, or
datasets that explicitly incorporate pose annotations. In this paper, we
propose the TrackID3x3 dataset, the first publicly available comprehensive
dataset specifically designed for multi-player tracking, player identification,
and pose estimation in 3x3 basketball scenarios. The dataset comprises three
distinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera
footage), capturing diverse full-court camera perspectives and environments. We
also introduce the Track-ID task, a simplified variant of the game state
reconstruction task that excludes field detection and focuses exclusively on
fixed-camera scenarios. To evaluate performance, we propose a baseline
algorithm called Track-ID algorithm, tailored to assess tracking and
identification quality. Furthermore, our benchmark experiments, utilizing
recent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose
estimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results
and highlight remaining challenges. Our dataset and evaluation benchmarks
provide a solid foundation for advancing automated analytics in 3x3 basketball.
Dataset and code will be available at
https://github.com/open-starlab/TrackID3x3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TrackID3x3：适用于3x3篮球全场视频中多人跟踪、身份识别和姿态估计的数据集和算法</div>
<div class="mono" style="margin-top:8px">多目标跟踪、球员识别和姿态估计是体育分析的基本组成部分，对于分析球员运动、表现和战术策略至关重要。然而，现有的数据集和方法主要针对主流团队运动如足球和传统5对5篮球，往往忽视了固定摄像头设置等常见场景，这些场景常用于业余水平、非主流运动或明确包含姿态注释的数据集。在本文中，我们提出了TrackID3x3数据集，这是第一个公开的专门设计用于3x3篮球多人跟踪、球员识别和姿态估计的综合数据集。数据集包含三个不同的子集（室内固定摄像头、室外固定摄像头和无人机镜头），捕捉了多样的全场摄像头视角和环境。我们还引入了Track-ID任务，这是一个简化版的游戏状态重建任务，排除了场地检测，仅专注于固定摄像头场景。为了评估性能，我们提出了一种名为Track-ID算法的基线算法，以评估跟踪和识别质量。此外，我们的基准实验使用了最近的多目标跟踪算法（例如BoT-SORT-ReID）和自上而下的姿态估计方法（HRNet、RTMPose和SwinPose），展示了稳健的结果并指出了剩余的挑战。我们的数据集和评估基准为3x3篮球的自动化分析提供了坚实的基础。数据集和代码将在https://github.com/open-starlab/TrackID3x3上提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the lack of datasets and methodologies for multi-player tracking, player identification, and pose estimation in 3x3 basketball, especially for fixed-camera setups. The authors introduce the TrackID3x3 dataset, which includes three subsets: indoor fixed-camera, outdoor fixed-camera, and drone footage. They also propose a Track-ID task and a baseline algorithm called Track-ID algorithm. Benchmark experiments using recent multi-object tracking and pose estimation methods show robust results but also highlight remaining challenges.</div>
<div class="mono" style="margin-top:8px">研究旨在解决3x3篮球中多球员跟踪、球员识别和姿态估计的缺口，重点关注固定摄像头设置。作者引入了TrackID3x3数据集，包括室内、室外和无人机拍摄的视频，并提出了一种Track-ID算法进行评估。使用先进的跟踪和姿态估计方法的基准实验显示了积极的结果，但也指出了该领域存在的持续挑战。数据集和代码已公开，供进一步研究使用。</div>
</details>
</div>
<div class="card">
<div class="title">MetaWild: A Multimodal Dataset for Animal Re-Identification with   Environmental Metadata</div>
<div class="meta-line">Authors: Yuzhuo Li, Di Zhao, Tingrui Qiao, Yihao Wu, Bo Pang, Yun Sing Koh</div>
<div class="meta-line">First: 2025-01-23T04:14:59+00:00 · Latest: 2025-08-20T10:02:32+00:00</div>
<div class="meta-line">Comments: 7 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.13368v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.13368v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying individual animals within large wildlife populations is essential
for effective wildlife monitoring and conservation efforts. Recent advancements
in computer vision have shown promise in animal re-identification (Animal ReID)
by leveraging data from camera traps. However, existing Animal ReID datasets
rely exclusively on visual data, overlooking environmental metadata that
ecologists have identified as highly correlated with animal behavior and
identity, such as temperature and circadian rhythms. Moreover, the emergence of
multimodal models capable of jointly processing visual and textual data
presents new opportunities for Animal ReID, but existing datasets fail to
leverage these models&#x27; text-processing capabilities, limiting their full
potential. Additionally, to facilitate the use of metadata in existing ReID
methods, we propose the Meta-Feature Adapter (MFA), a lightweight module that
can be incorporated into existing vision-language model (VLM)-based Animal ReID
methods, allowing ReID models to leverage both environmental metadata and
visual information to improve ReID performance. Experiments on MetaWild show
that combining baseline ReID models with MFA to incorporate metadata
consistently improves performance compared to using visual information alone,
validating the effectiveness of incorporating metadata in re-identification. We
hope that our proposed dataset can inspire further exploration of multimodal
approaches for Animal ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetaWild：一种包含环境元数据的动物再识别多模态数据集</div>
<div class="mono" style="margin-top:8px">在大型野生动物种群中识别个体动物对于有效的野生动物监测和保护工作至关重要。计算机视觉的最新进展通过利用相机陷阱数据在动物再识别（Animal ReID）方面显示出潜力。然而，现有的Animal ReID数据集仅依赖于视觉数据，忽略了生态学家认为与动物行为和身份高度相关的环境元数据，如温度和昼夜节律。此外，能够同时处理视觉和文本数据的多模态模型的出现为Animal ReID带来了新的机会，但现有的数据集未能利用这些模型的文本处理能力，限制了它们的全部潜力。此外，为了便于在现有ReID方法中使用元数据，我们提出了环境元特征适配器（MFA），这是一种轻量级模块，可以集成到基于视觉-语言模型（VLM）的Animal ReID方法中，使ReID模型能够利用环境元数据和视觉信息来提高ReID性能。在MetaWild上的实验表明，将基础ReID模型与MFA结合以包含元数据的一致性提高了性能，验证了在再识别中结合元数据的有效性。我们希望我们提出的数据集能够激发对Animal ReID的多模态方法的进一步探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance animal re-identification in wildlife monitoring by incorporating environmental metadata into existing visual data. The study introduces MetaWild, a multimodal dataset that includes both visual and environmental metadata. Key findings show that integrating environmental metadata with baseline re-identification models through the Meta-Feature Adapter (MFA) improves performance, validating the effectiveness of multimodal approaches in animal re-identification.</div>
<div class="mono" style="margin-top:8px">研究旨在通过将环境元数据与现有视觉数据结合来提升野生动物监测中的动物识别。方法包括开发一个多模态数据集MetaWild和一个轻量级模块Meta-Feature Adapter (MFA)，以将环境元数据与视觉信息整合。关键实验结果表明，将基础ReID模型与MFA结合使用可以一致地提高性能，验证了在识别中使用元数据的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian   Re-Identification in Autonomous Driving</div>
<div class="meta-line">Authors: Jialin Li, Shuqi Wu, Ning Wang</div>
<div class="meta-line">First: 2025-08-15T04:50:27+00:00 · Latest: 2025-08-15T04:50:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.11218v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.11218v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP&#x27;s vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP的不确定性模态建模（UMM）框架在自主驾驶中的人行道再识别</div>
<div class="mono" style="margin-top:8px">再识别（ReID）是智能感知系统中的关键技术，特别是在自主驾驶中，车载摄像头必须在实时环境中跨视角和时间识别行人，以支持安全导航和轨迹预测。然而，输入模态的不确定性或缺失——如RGB、红外、素描或文本描述——给传统的ReID方法带来了重大挑战。尽管大规模预训练模型提供了强大的多模态语义建模能力，但其计算开销限制了在资源受限环境中的实际部署。为了解决这些挑战，我们提出了一种轻量级的不确定性模态建模（UMM）框架，该框架集成了多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器。这些组件共同实现了统一的特征表示，减轻了缺失模态的影响，并从不同数据类型中提取互补信息。此外，UMM 利用CLIP的视觉-语言对齐能力高效地融合多模态输入，而无需大量微调。实验结果表明，UMM 在不确定性模态条件下实现了强大的鲁棒性、泛化能力和计算效率，为自主驾驶场景中的人行道再识别提供了可扩展且实用的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve pedestrian re-identification in autonomous driving by addressing the challenges posed by uncertain or missing input modalities. The proposed UMM framework integrates a multimodal token mapper, synthetic modality augmentation, and cross-modal cue interactive learner to enable unified feature representation and mitigate the impact of missing data. Experimental results show that UMM achieves strong robustness, generalization, and computational efficiency under uncertain modality conditions, providing a practical solution for pedestrian re-identification in autonomous driving scenarios.</div>
<div class="mono" style="margin-top:8px">论文提出了一种轻量级的不确定性模态建模（UMM）框架，以解决在自主驾驶中存在不确定或缺失输入模态时的人再识别挑战。该框架结合了多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器，以统一特征表示并减轻缺失数据的影响。实验结果表明，UMM在不确定模态条件下提供了强大的鲁棒性、泛化能力和计算效率，使其成为自主驾驶场景中行人再识别的实用解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">NEXT: Multi-Grained Mixture of Experts via Text-Modulation for   Multi-Modal Object Re-Identification</div>
<div class="meta-line">Authors: Shihao Li, Aihua Zheng, Andong Lu, Jin Tang, Jixin Ma</div>
<div class="meta-line">First: 2025-05-26T13:52:28+00:00 · Latest: 2025-08-10T11:01:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.20001v4">Abs</a> · <a href="http://arxiv.org/pdf/2505.20001v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal object Re-Identification (ReID) aims to obtain accurate identity
features across heterogeneous modalities. However, most existing methods rely
on implicit feature fusion modules, making it difficult to model fine-grained
recognition patterns under various challenges in real world. Benefiting from
the powerful Multi-modal Large Language Models (MLLMs), the object appearances
are effectively translated into descriptive captions. In this paper, we propose
a reliable caption generation pipeline based on attribute confidence, which
significantly reduces the unknown recognition rate of MLLMs and improves the
quality of generated text. Additionally, to model diverse identity patterns, we
propose a novel ReID framework, named NEXT, the Multi-grained Mixture of
Experts via Text-Modulation for Multi-modal Object Re-Identification.
Specifically, we decouple the recognition problem into semantic and structural
branches to separately capture fine-grained appearance features and
coarse-grained structure features. For semantic recognition, we first propose a
Text-Modulated Semantic Experts (TMSE), which randomly samples high-quality
captions to modulate experts capturing semantic features and mining
inter-modality complementary cues. Second, to recognize structure features, we
propose a Context-Shared Structure Experts (CSSE), which focuses on the
holistic object structure and maintains identity structural consistency via a
soft routing mechanism. Finally, we propose a Multi-Grained Features
Aggregation (MGFA), which adopts a unified fusion strategy to effectively
integrate multi-grained experts into the final identity representations.
Extensive experiments on four public datasets demonstrate the effectiveness of
our method and show that it significantly outperforms existing state-of-the-art
methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEXT：基于文本调制的多粒度专家混合多模态对象重识别</div>
<div class="mono" style="margin-top:8px">多模态对象重识别（ReID）旨在跨异构模态获取准确的身份特征。然而，大多数现有方法依赖于隐式的特征融合模块，使得在各种现实挑战下难以建模细粒度的识别模式。得益于强大的多模态大型语言模型（MLLMs），对象外观被有效转化为描述性标题。在本文中，我们提出了一种基于属性置信度的可靠标题生成流水线，显著降低了MLLMs的未知识别率并提高了生成文本的质量。此外，为了建模多样的身份模式，我们提出了一种新的ReID框架，名为NEXT，即基于文本调制的多粒度专家混合多模态对象重识别。具体而言，我们将识别问题分解为语义和结构分支，分别捕获细粒度的外观特征和粗粒度的结构特征。对于语义识别，我们首先提出了一种文本调制语义专家（TMSE），它随机采样高质量的标题来调制专家以捕获语义特征并挖掘跨模态互补线索。其次，为了识别结构特征，我们提出了一种上下文共享结构专家（CSSE），它专注于整体对象结构并通过软路由机制保持身份结构一致性。最后，我们提出了一种多粒度特征聚合（MGFA），它采用统一的融合策略有效地将多粒度专家整合到最终的身份表示中。在四个公开数据集上的广泛实验表明了我们方法的有效性，并且显示它显著优于现有最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes NEXT, a framework for multi-modal object re-identification that uses text-modulation to improve feature fusion. It introduces a caption generation pipeline based on attribute confidence to enhance the quality of generated text and reduce unknown recognition rates. The framework consists of semantic and structural branches, with Text-Modulated Semantic Experts (TMSE) and Context-Shared Structure Experts (CSSE) capturing fine-grained and coarse-grained features, respectively. Multi-Grained Features Aggregation (MGFA) integrates these features into final identity representations. Experiments on four datasets show that NEXT outperforms existing methods.</div>
<div class="mono" style="margin-top:8px">论文针对多模态对象重识别中跨不同模态准确识别对象的挑战，提出了NEXT框架，通过文本调制改进特征融合和识别。具体来说，NEXT将识别过程拆分为语义和结构分支，分别使用语义专家（TMSE）和结构专家（CSSE）来捕捉细粒度和粗粒度特征。该方法还包括多粒度特征聚合（MGFA），以有效整合这些特征。实验结果显示，NEXT在四个数据集上的表现优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile   Person Re-identification</div>
<div class="meta-line">Authors: Jinhao Li, Zijian Chen, Lirong Deng, Changbo Wang, Guangtao Zhai</div>
<div class="meta-line">First: 2025-08-09T09:42:09+00:00 · Latest: 2025-08-09T09:42:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.06908v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.06908v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMReID-Bench：利用MLLMs的强大功能实现有效的多模态人员再识别</div>
<div class="mono" style="margin-top:8px">人员再识别（ReID）旨在从画廊图像中检索感兴趣人员的图像，广泛应用于医疗康复、异常行为检测和公共安全等领域。然而，传统的人员ReID模型仅具备单模态能力，导致在RGB、热成像、红外、素描图像、文本描述等多模态数据中泛化能力较差。最近，多模态大型语言模型（MLLMs）的出现为解决这一问题提供了前景。尽管如此，现有方法仅将MLLMs视为特征提取器或描述生成器，未能充分发挥其推理、指令跟随和跨模态理解能力。为弥补这一差距，我们引入了MMReID-Bench，这是首个专门针对人员ReID设计的多任务多模态基准。MMReID-Bench 包含20,710个跨模态查询和画廊图像，覆盖了10种不同的人员ReID任务。全面的实验表明，MLLMs在提供有效的多模态人员再识别方面具有显著能力。然而，它们在处理某些模态时仍存在局限性，特别是热成像和红外数据。我们希望MMReID-Bench能够促进社区开发更稳健和泛化的多模态基础模型用于人员ReID。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance person re-identification (ReID) by leveraging multi-modal large language models (MLLMs) to overcome the limitations of traditional uni-modal models. The study introduces MMReID-Bench, a benchmark for multi-task multi-modal person ReID, which includes 20,710 multi-modal queries and gallery images. Experiments show that MLLMs can effectively perform person ReID across various modalities, but they face challenges with thermal and infrared data. The benchmark is expected to promote the development of more robust and generalizable multimodal foundation models for person ReID.</div>
<div class="mono" style="margin-top:8px">研究旨在通过利用多模态大型语言模型（MLLM）来增强人员再识别（ReID），以克服传统单模态模型的局限性。研究引入了MMReID-Bench，这是一个针对多任务多模态人员ReID的基准，包含20,710个跨10个任务的查询和图像。实验表明，MLLMs可以在多种模态下有效执行人员ReID，尽管它们在处理热成像和红外数据时存在挑战。该基准旨在促进更稳健和通用的多模态基础模型的发展，用于人员ReID。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person   Re-Identification</div>
<div class="meta-line">Authors: Taha Mustapha Nehdi, Nairouz Mrabah, Atif Belal, Marco Pedersoli, Eric Granger</div>
<div class="meta-line">First: 2025-08-09T05:10:22+00:00 · Latest: 2025-08-09T05:10:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.06831v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.06831v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (&lt;= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低秩专家合并用于人员重识别多源领域适应</div>
<div class="mono" style="margin-top:8px">将人员重识别(reID)模型适应新的目标环境仍然是一个具有挑战性的问题，通常使用无监督领域适应(UDA)方法来解决。最近的研究表明，当标记数据源自多个不同的来源（例如，数据集和摄像头）时，分别考虑每个来源并进行多源领域适应(MSDA)通常比混合这些来源并执行传统的UDA方法能获得更高的准确性和鲁棒性。然而，最先进的MSDA方法学习特定领域的主干模型或在适应过程中需要访问源领域数据，导致训练参数和计算成本显著增加。在本文中，提出了一种名为Source-free Adaptive Gated Experts(SAGE-reID)的方法用于人员重识别。我们的SAGE-reID是一种成本效益高、无需源数据的MSDA方法，首先通过无源UDA训练每个来源特定的低秩适配器(LoRA)。然后引入并训练一个轻量级门控网络，动态分配LoRA专家融合的最佳合并权重，实现有效的跨域知识迁移。尽管主干参数在不同来源领域中保持不变，但LoRA专家线性扩展但大小可忽略不计（&lt;=2%的主干），从而减少内存消耗和过拟合风险。在三个具有挑战性的基准数据集：Market-1501、DukeMTMC-reID和MSMT17上进行的广泛实验表明，SAGE-reID在计算效率方面优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SAGE-reID, a source-free method for multi-source domain adaptation in person re-identification. It uses source-free unsupervised domain adaptation to train low-rank adapters (LoRA) for each source, and a lightweight gating network to dynamically merge these adapters. Experiments show SAGE-reID outperforms state-of-the-art methods while being computationally efficient, reducing both memory consumption and overfitting risk.</div>
<div class="mono" style="margin-top:8px">本文提出了一种源无监督自适应门控专家（SAGE-reID）方法来解决使用无监督域适应（UDA）方法将人再识别模型适应到新环境中的挑战。该方法通过源无监督UDA训练每个源的低秩适配器（LoRA），并引入一个轻量级门控网络动态融合这些适配器。该方法保持了骨干参数数量不变，同时允许LoRA专家线性扩展，减少内存消耗和过拟合风险。在Market-1501、DukeMTMC-reID和MSMT17三个挑战性基准上的实验表明，SAGE-reID在保持计算效率的同时优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Attribute Guidance With Inherent Pseudo-label For Occluded Person   Re-identification</div>
<div class="meta-line">Authors: Rui Zhi, Zhen Yang, Haiyang Zhang</div>
<div class="meta-line">First: 2025-08-07T03:13:24+00:00 · Latest: 2025-08-07T03:13:24+00:00</div>
<div class="meta-line">Comments: 8 pages, 2 supplement pages, 3 figures, ECAI2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.04998v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.04998v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Person re-identification (Re-ID) aims to match person images across different
camera views, with occluded Re-ID addressing scenarios where pedestrians are
partially visible. While pre-trained vision-language models have shown
effectiveness in Re-ID tasks, they face significant challenges in occluded
scenarios by focusing on holistic image semantics while neglecting fine-grained
attribute information. This limitation becomes particularly evident when
dealing with partially occluded pedestrians or when distinguishing between
individuals with subtle appearance differences. To address this limitation, we
propose Attribute-Guide ReID (AG-ReID), a novel framework that leverages
pre-trained models&#x27; inherent capabilities to extract fine-grained semantic
attributes without additional data or annotations. Our framework operates
through a two-stage process: first generating attribute pseudo-labels that
capture subtle visual characteristics, then introducing a dual-guidance
mechanism that combines holistic and fine-grained attribute information to
enhance image feature extraction. Extensive experiments demonstrate that
AG-ReID achieves state-of-the-art results on multiple widely-used Re-ID
datasets, showing significant improvements in handling occlusions and subtle
attribute differences while maintaining competitive performance on standard
Re-ID scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于固有伪标签的属性引导行人再识别</div>
<div class="mono" style="margin-top:8px">行人再识别（Re-ID）旨在跨不同摄像机视角匹配行人图像，遮挡再识别处理行人部分可见的情况。虽然预训练的视觉-语言模型在Re-ID任务中显示出有效性，但在遮挡场景中，它们因专注于整体图像语义而忽视了细粒度的属性信息，面临重大挑战。这一局限性在处理部分遮挡的行人或区分具有细微外观差异的个体时尤为明显。为解决这一局限性，我们提出了一种新颖的框架——属性引导再识别（AG-ReID），该框架利用预训练模型的固有能力提取细粒度语义属性，无需额外数据或注释。我们的框架通过两阶段过程运作：首先生成捕捉细微视觉特征的属性伪标签，然后引入一种双重引导机制，结合整体和细粒度属性信息以增强图像特征提取。大量实验表明，AG-ReID 在多个广泛使用的 Re-ID 数据集上取得了最先进的结果，展示了在处理遮挡和细微属性差异方面的显著改进，同时在标准 Re-ID 场景中保持了竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes Attribute-Guide ReID (AG-ReID), which addresses the challenge of occluded person re-identification by leveraging pre-trained vision-language models to extract fine-grained semantic attributes. The method involves generating attribute pseudo-labels and using a dual-guidance mechanism to enhance feature extraction. Experiments show that AG-ReID outperforms existing methods on multiple Re-ID datasets, particularly in handling occlusions and subtle attribute differences.</div>
<div class="mono" style="margin-top:8px">研究旨在通过利用预训练的视觉-语言模型提取细粒度语义属性，来改善遮挡场景下的行人重识别（Re-ID）。提出的Attribute-Guide ReID (AG-ReID)框架生成属性伪标签，并结合整体和细粒度信息以增强特征提取。实验表明，AG-ReID 在多个 Re-ID 数据集上优于现有方法，特别是在处理遮挡和细微属性差异方面表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">CORE-ReID V2: Advancing the Domain Adaptation for Object   Re-Identification with Optimized Training and Ensemble Fusion</div>
<div class="meta-line">Authors: Trinh Quoc Nguyen, Oky Dicky Ardiansyah Prima, Syahid Al Irfan, Hindriyanto Dwi Purnomo, Radius Tanone</div>
<div class="meta-line">Venue: AI Sens. 2025, 1(1), 4</div>
<div class="meta-line">First: 2025-08-06T02:57:09+00:00 · Latest: 2025-08-06T02:57:09+00:00</div>
<div class="meta-line">Comments: AI Sens. 2025, Submission received: 8 May 2025 / Revised: 4 June 2025
  / Accepted: 30 June 2025 / Published: 4 July 2025. 3042-5999/1/1/4</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.04036v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.04036v1">PDF</a> · <a href="https://github.com/TrinhQuocNguyen/CORE-ReID-V2">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents CORE-ReID V2, an enhanced framework building upon
CORE-ReID. The new framework extends its predecessor by addressing Unsupervised
Domain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with
further applicability to Object ReID. During pre-training, CycleGAN is employed
to synthesize diverse data, bridging image characteristic gaps across different
domains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting
of the Efficient Channel Attention Block (ECAB) and the Simplified Efficient
Channel Attention Block (SECAB), enhances both local and global feature
representations while reducing ambiguity in pseudo-labels for target samples.
Experimental results on widely used UDA Person ReID and Vehicle ReID datasets
demonstrate that the proposed framework outperforms state-of-the-art methods,
achieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy
(Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones
such as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our
work not only pushes the boundaries of UDA-based Object ReID but also provides
a solid foundation for further research and advancements in this domain. Our
codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID-V2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CORE-ReID V2：通过优化训练和集成融合推进对象重识别领域的领域适应</div>
<div class="mono" style="margin-top:8px">本研究提出了CORE-ReID V2，这是一种基于CORE-ReID的增强框架。新框架在其前身的基础上，通过CycleGAN合成多样化的数据，解决了人员重识别和车辆重识别中的无监督领域适应(UDA)挑战，并进一步适用于对象重识别。在预训练阶段，使用CycleGAN合成多样化的数据，缩小不同领域之间的图像特征差距。在微调阶段，采用高效的通道注意模块(ECAB)和简化版的高效通道注意模块(SECAB)组成的先进集成融合机制，增强局部和全局特征表示，同时减少目标样本伪标签的模糊性。在广泛使用的UDA人员重识别和车辆重识别数据集上的实验结果表明，所提出的框架在均值平均精度(mAP)和Rank-k准确率(Top-1, Top-5, Top-10)方面优于最先进的方法，表现出最佳性能。此外，该框架支持轻量级骨干网络如ResNet18和ResNet34，确保了可扩展性和高效性。我们的工作不仅推动了基于UDA的对象重识别边界，还为该领域的进一步研究和进展提供了坚实的基础。我们的代码和模型可在https://github.com/TrinhQuocNguyen/CORE-ReID-V2获取。</div>
</details>
</div>
<div class="card">
<div class="title">CORE-ReID: Comprehensive Optimization and Refinement through Ensemble   fusion in Domain Adaptation for person re-identification</div>
<div class="meta-line">Authors: Trinh Quoc Nguyen, Oky Dicky Ardiansyah Prima, Katsuyoshi Hotta</div>
<div class="meta-line">Venue: Software 2024, 3(2), 227-249</div>
<div class="meta-line">First: 2025-08-05T04:25:03+00:00 · Latest: 2025-08-05T04:25:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.03064v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.03064v1">PDF</a> · <a href="https://github.com/TrinhQuocNguyen/CORE-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study introduces a novel framework, &quot;Comprehensive Optimization and
Refinement through Ensemble Fusion in Domain Adaptation for Person
Re-identification (CORE-ReID)&quot;, to address an Unsupervised Domain Adaptation
(UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to
generate diverse data that harmonizes differences in image characteristics from
different camera sources in the pre-training stage. In the fine-tuning stage,
based on a pair of teacher-student networks, the framework integrates
multi-view features for multi-level clustering to derive diverse pseudo labels.
A learnable Ensemble Fusion component that focuses on fine-grained local
information within global features is introduced to enhance learning
comprehensiveness and avoid ambiguity associated with multiple pseudo-labels.
Experimental results on three common UDAs in Person ReID demonstrate
significant performance gains over state-of-the-art approaches. Additional
enhancements, such as Efficient Channel Attention Block and Bidirectional Mean
Feature Normalization mitigate deviation effects and adaptive fusion of global
and local features using the ResNet-based model, further strengthening the
framework. The proposed framework ensures clarity in fusion features, avoids
ambiguity, and achieves high ac-curacy in terms of Mean Average Precision,
Top-1, Top-5, and Top-10, positioning it as an advanced and effective solution
for the UDA in Person ReID. Our codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CORE-ReID：领域适应中的人再识别综合优化与细化通过集成融合</div>
<div class="mono" style="margin-top:8px">本研究提出了一种新的框架“领域适应中的人再识别综合优化与细化通过集成融合（CORE-ReID）”，以解决人再识别（ReID）的无监督领域适应（UDA）问题。该框架利用CycleGAN在预训练阶段生成多样化的数据，以协调不同摄像源图像特征之间的差异。在微调阶段，基于一对教师-学生网络，框架结合多视角特征进行多级聚类，以提取多样化的伪标签。引入了一个可学习的集成融合组件，专注于全局特征内的细粒度局部信息，以增强学习的全面性并避免与多个伪标签相关的歧义。在人再识别中三种常见UDA上的实验结果表明，该框架在性能上显著优于现有方法。此外，高效的信道注意力模块和双向均值特征规范化进一步减轻了偏差效应，并使用基于ResNet的模型实现了全局和局部特征的自适应融合，从而加强了框架。所提出的框架确保了融合特征的清晰性，避免了歧义，并在平均精度、Top-1、Top-5和Top-10方面实现了高精度，将其定位为UDAs中人再识别的先进有效解决方案。我们的代码和模型可在https://github.com/TrinhQuocNguyen/CORE-ReID/获取。</div>
</details>
</div>
<div class="card">
<div class="title">VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video   Question Answering</div>
<div class="meta-line">Authors: Yiran Meng, Junhong Ye, Wei Zhou, Guanghui Yue, Xudong Mao, Ruomei Wang, Baoquan Zhao</div>
<div class="meta-line">First: 2025-08-05T03:33:24+00:00 · Latest: 2025-08-05T03:33:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.03039v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.03039v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-video question answering presents significant challenges beyond
traditional single-video understanding, particularly in establishing meaningful
connections across video streams and managing the complexity of multi-source
information retrieval. We introduce VideoForest, a novel framework that
addresses these challenges through person-anchored hierarchical reasoning. Our
approach leverages person-level features as natural bridge points between
videos, enabling effective cross-video understanding without requiring
end-to-end training. VideoForest integrates three key innovations: 1) a
human-anchored feature extraction mechanism that employs ReID and tracking
algorithms to establish robust spatiotemporal relationships across multiple
video sources; 2) a multi-granularity spanning tree structure that
hierarchically organizes visual content around person-level trajectories; and
3) a multi-agent reasoning framework that efficiently traverses this
hierarchical structure to answer complex cross-video queries. To evaluate our
approach, we develop CrossVideoQA, a comprehensive benchmark dataset
specifically designed for person-centric cross-video analysis. Experimental
results demonstrate VideoForest&#x27;s superior performance in cross-video reasoning
tasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior
analysis, and 51.67% in summarization and reasoning, significantly
outperforming existing methods. Our work establishes a new paradigm for
cross-video understanding by unifying multiple video streams through
person-level features, enabling sophisticated reasoning across distributed
visual information while maintaining computational efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoForest：基于人的层次推理跨视频问答</div>
<div class="mono" style="margin-top:8px">跨视频问答超越了传统单视频理解，特别是在建立视频流之间有意义的连接和管理多源信息检索的复杂性方面面临重大挑战。我们提出了VideoForest，一种通过基于人的层次推理解决这些挑战的新框架。我们的方法利用基于人的特征作为视频之间的自然桥梁，无需端到端训练即可实现有效的跨视频理解。VideoForest 结合了三项关键创新：1）一种基于人的特征提取机制，利用 ReID 和跟踪算法在多个视频源之间建立稳健的时空关系；2）一种多粒度的树状结构，按人的轨迹层次组织视觉内容；3）一种多智能体推理框架，高效地遍历这种层次结构以回答复杂的跨视频查询。为了评估我们的方法，我们开发了CrossVideoQA，一个专门用于基于人的跨视频分析的综合基准数据集。实验结果表明，VideoForest 在跨视频推理任务中的性能优于现有方法，准确率达到人识别71.93%，行为分析83.75%，总结和推理51.67%，显著优于现有方法。我们的工作通过利用基于人的特征统一多个视频流，建立了一种新的跨视频理解范式，能够在保持计算效率的同时实现分布式视觉信息的复杂推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VideoForest is a framework designed to address the challenges of cross-video question answering by leveraging person-anchored hierarchical reasoning. It uses a human-anchored feature extraction mechanism, a multi-granularity spanning tree structure, and a multi-agent reasoning framework. The approach achieves 71.93% accuracy in person recognition, 83.75% in behavior analysis, and 51.67% in summarization and reasoning, outperforming existing methods in cross-video reasoning tasks.</div>
<div class="mono" style="margin-top:8px">VideoForest 是一个框架，通过使用基于人的层次推理来解决跨视频问答的挑战。它整合了基于人的特征提取机制、多粒度的树结构以及多代理推理框架。实验结果表明，VideoForest 在人物识别、行为分析和总结推理等任务上的表现优于现有方法，准确率分别为 71.93%、83.75% 和 51.67%。</div>
</details>
</div>
<div class="card">
<div class="title">Semantics versus Identity: A Divide-and-Conquer Approach towards   Adjustable Medical Image De-Identification</div>
<div class="meta-line">Authors: Yuan Tian, Shuo Wang, Rongzhao Zhang, Zijian Chen, Yankai Jiang, Chunyi Li, Xiangyang Zhu, Fang Yan, Qiang Hu, XiaoSong Wang, Guangtao Zhai</div>
<div class="meta-line">First: 2025-07-25T06:59:05+00:00 · Latest: 2025-07-25T06:59:05+00:00</div>
<div class="meta-line">Comments: Accepted to ICCV2025;</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.21703v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.21703v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical imaging has significantly advanced computer-aided diagnosis, yet its
re-identification (ReID) risks raise critical privacy concerns, calling for
de-identification (DeID) techniques. Unfortunately, existing DeID methods
neither particularly preserve medical semantics, nor are flexibly adjustable
towards different privacy levels. To address these issues, we propose a
divide-and-conquer framework comprising two steps: (1) Identity-Blocking, which
blocks varying proportions of identity-related regions, to achieve different
privacy levels; and (2) Medical-Semantics-Compensation, which leverages
pre-trained Medical Foundation Models (MFMs) to extract medical semantic
features to compensate the blocked regions. Moreover, recognizing that features
from MFMs may still contain residual identity information, we introduce a
Minimum Description Length principle-based feature decoupling strategy, to
effectively decouple and discard such identity components. Extensive
evaluations against existing approaches across seven datasets and three
downstream tasks, demonstrates our state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义学与身份识别：一种分而治之的可调医疗图像去标识化方法</div>
<div class="mono" style="margin-top:8px">医学成像显著推动了计算机辅助诊断的发展，但其再识别（ReID）风险引发了严重的隐私问题，需要去标识化（DeID）技术。不幸的是，现有的DeID方法既不能特别保留医学语义，也不能灵活调整以适应不同的隐私级别。为了解决这些问题，我们提出了一种分而治之的框架，包括两个步骤：（1）身份阻断，通过阻断不同比例的身份相关区域，实现不同的隐私级别；（2）医学语义补偿，利用预训练的医学基础模型（MFMs）提取医学语义特征来补偿被阻断的区域。此外，考虑到MFMs的特征可能仍包含残留的身份信息，我们引入了一种基于最小描述长度原则的特征解耦策略，以有效解耦并丢弃这些身份成分。在七个数据集和三个下游任务上与现有方法的广泛评估表明，我们的方法具有最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of de-identifying medical images while preserving their medical semantics. It proposes a divide-and-conquer framework with two steps: Identity-Blocking to block varying proportions of identity-related regions for adjustable privacy levels, and Medical-Semantics-Compensation to use pre-trained Medical Foundation Models to extract medical semantic features to compensate for the blocked regions. The authors also introduce a feature decoupling strategy based on the Minimum Description Length principle to remove residual identity information. Experiments on seven datasets show that their method outperforms existing approaches in three downstream tasks.</div>
<div class="mono" style="margin-top:8px">论文旨在解决医学影像去标识化过程中平衡隐私保护与诊断准确性的问题。提出了一种分而治之的框架，包含两个步骤：通过调整掩蔽不同比例的身份相关区域实现Identity-Blocking以设定不同的隐私级别；利用预训练的医学基础模型提取医学语义特征进行Medical-Semantics-Compensation以保持医学信息。此外，还采用最小描述长度原则的特征解耦策略去除残留的身份信息。实验结果表明，该方法在多个数据集和任务上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Positive Style Accumulation: A Style Screening and Continuous   Utilization Framework for Federated DG-ReID</div>
<div class="meta-line">Authors: Xin Xu, Chaoyue Ren, Wei Liu, Wenke Huang, Bin Yang, Zhixi Yu, Kui Jiang</div>
<div class="meta-line">Venue: ACM MM 2025</div>
<div class="meta-line">First: 2025-07-22T05:21:00+00:00 · Latest: 2025-07-22T05:21:00+00:00</div>
<div class="meta-line">Comments: 10 pages, 3 figures, accepted at ACM MM 2025, Submission ID: 4394</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.16238v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.16238v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Federated Domain Generalization for Person re-identification (FedDG-ReID)
aims to learn a global server model that can be effectively generalized to
source and target domains through distributed source domain data. Existing
methods mainly improve the diversity of samples through style transformation,
which to some extent enhances the generalization performance of the model.
However, we discover that not all styles contribute to the generalization
performance. Therefore, we define styles that are beneficial or harmful to the
model&#x27;s generalization performance as positive or negative styles. Based on
this, new issues arise: How to effectively screen and continuously utilize the
positive styles. To solve these problems, we propose a Style Screening and
Continuous Utilization (SSCU) framework. Firstly, we design a Generalization
Gain-guided Dynamic Style Memory (GGDSM) for each client model to screen and
accumulate generated positive styles. Meanwhile, we propose a style memory
recognition loss to fully leverage the positive styles memorized by Memory.
Furthermore, we propose a Collaborative Style Training (CST) strategy to make
full use of positive styles. Unlike traditional learning strategies, our
approach leverages both newly generated styles and the accumulated positive
styles stored in memory to train client models on two distinct branches. This
training strategy is designed to effectively promote the rapid acquisition of
new styles by the client models, and guarantees the continuous and thorough
utilization of positive styles, which is highly beneficial for the model&#x27;s
generalization performance. Extensive experimental results demonstrate that our
method outperforms existing methods in both the source domain and the target
domain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>积极风格积累：一种联邦DG-ReID的风格筛选与持续利用框架</div>
<div class="mono" style="margin-top:8px">联邦领域泛化用于行人再识别（FedDG-ReID）旨在通过分布式源域数据学习一个能够有效泛化到源域和目标域的全局服务器模型。现有方法主要通过风格变换提高样本多样性，从而在一定程度上增强模型的泛化性能。然而，我们发现并非所有风格都对模型的泛化性能有益。因此，我们将有助于或损害模型泛化性能的风格定义为积极或消极风格。基于此，新的问题出现了：如何有效筛选和持续利用积极风格。为了解决这些问题，我们提出了一种风格筛选与持续利用（SSCU）框架。首先，我们为每个客户端模型设计了一种泛化增益引导动态风格记忆（GGDSM），用于筛选和积累生成的积极风格。同时，我们提出了一种风格记忆识别损失，以充分利用记忆中存储的积极风格。此外，我们提出了协作风格训练（CST）策略，充分利用积极风格。与传统的学习策略不同，我们的方法利用新生成的风格和存储在记忆中的积极风格，在两个不同的分支上训练客户端模型。这种训练策略旨在有效促进客户端模型快速获取新风格，并确保积极风格的持续和彻底利用，这对模型的泛化性能非常有益。广泛的实验结果表明，我们的方法在源域和目标域中均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the generalization performance of federated domain generalization for person re-identification (FedDG-ReID) by effectively screening and continuously utilizing positive styles. The proposed Style Screening and Continuous Utilization (SSCU) framework includes a Generalization Gain-guided Dynamic Style Memory (GGDSM) for accumulating positive styles and a Collaborative Style Training (CST) strategy to leverage both newly generated and stored positive styles. Experimental results show that the method outperforms existing approaches in both source and target domains.</div>
<div class="mono" style="margin-top:8px">研究旨在通过有效筛选和持续利用正向风格来提高联邦域泛化中的人再识别（FedDG-ReID）的性能。提出的Style Screening and Continuous Utilization (SSCU)框架包括一个Generalization Gain-guided Dynamic Style Memory (GGDSM)来筛选和累积正向风格，一个风格记忆识别损失以充分利用这些风格，以及一个Collaborative Style Training (CST)策略，在两个不同的分支上训练客户端模型，利用新生成的和存储在记忆中的正向风格。实验表明，该方法在源域和目标域中均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for   Multi-Camera Vehicle Tracking</div>
<div class="meta-line">Authors: Yuqiang Lin, Sam Lockyer, Mingxuan Sui, Li Gan, Florian Stanek, Markus Zarbock, Wenbin Li, Adrian Evans, Nic Zhang</div>
<div class="meta-line">First: 2025-07-11T16:30:27+00:00 · Latest: 2025-07-21T11:26:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.08729v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.08729v2">PDF</a> · <a href="https://github.com/siri-rouser/RoundaboutHD.git">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The multi-camera vehicle tracking (MCVT) framework holds significant
potential for smart city applications, including anomaly detection, traffic
density estimation, and suspect vehicle tracking. However, current publicly
available datasets exhibit limitations, such as overly simplistic scenarios,
low-resolution footage, and insufficiently diverse conditions, creating a
considerable gap between academic research and real-world scenario. To fill
this gap, we introduce RoundaboutHD, a comprehensive, high-resolution
multi-camera vehicle tracking benchmark dataset specifically designed to
represent real-world roundabout scenarios. RoundaboutHD provides a total of 40
minutes of labelled video footage captured by four non-overlapping,
high-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle
identities are annotated across different camera views, offering rich
cross-camera association data. RoundaboutHD offers temporal consistency video
footage and enhanced challenges, including increased occlusions and nonlinear
movement inside the roundabout. In addition to the full MCVT dataset, several
subsets are also available for object detection, single camera tracking, and
image-based vehicle re-identification (ReID) tasks. Vehicle model information
and camera modelling/ geometry information are also included to support further
analysis. We provide baseline results for vehicle detection, single-camera
tracking, image-based vehicle re-identification, and multi-camera tracking. The
dataset and the evaluation code are publicly available at:
https://github.com/siri-rouser/RoundaboutHD.git</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoundaboutHD：高分辨率真实城市环境多摄像头车辆跟踪基准</div>
<div class="mono" style="margin-top:8px">多摄像头车辆跟踪（MCVT）框架在智能城市应用中具有巨大潜力，包括异常检测、交通密度估计和可疑车辆跟踪。然而，当前可用的公开数据集存在局限性，如场景过于简单、分辨率低和条件不够多样化，导致学术研究与实际场景之间存在较大差距。为填补这一差距，我们引入了RoundaboutHD，这是一个全面的、高分辨率的多摄像头车辆跟踪基准数据集，专门设计用于代表真实的环岛场景。RoundaboutHD提供了由四个不重叠的高分辨率（4K分辨率，15帧/秒）摄像头拍摄的40分钟标注视频片段。共有512个独特的车辆身份在不同摄像头视角下被标注，提供了丰富的跨摄像头关联数据。RoundaboutHD提供了时间一致的视频片段和增强的挑战，包括环岛内的遮挡增加和非线性运动。除了完整的MCVT数据集，还提供了多个子集用于物体检测、单摄像头跟踪和基于图像的车辆重识别（ReID）任务。还包含了车辆模型信息和摄像头建模/几何信息以支持进一步分析。我们提供了车辆检测、单摄像头跟踪、基于图像的车辆重识别和多摄像头跟踪的基准结果。数据集和评估代码可在以下链接获取：https://github.com/siri-rouser/RoundaboutHD.git</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoundaboutHD is a high-resolution multi-camera vehicle tracking benchmark dataset designed to address the limitations of existing datasets by providing realistic roundabout scenarios. It includes 40 minutes of 4K resolution video footage from four non-overlapping cameras, with 512 unique vehicle identities annotated across different views. The dataset offers enhanced challenges such as increased occlusions and nonlinear movements, and provides baseline results for various tasks including vehicle detection, single-camera tracking, and multi-camera tracking.</div>
<div class="mono" style="margin-top:8px">RoundaboutHD 是一个高分辨率的多摄像头车辆跟踪基准，旨在通过提供真实的环岛场景来弥补现有数据集的不足。它包含来自四个非重叠摄像头的40分钟4K分辨率视频片段，并且有512个独特的车辆身份在不同视角下进行了标注。实验结果表明，存在更多的遮挡和非线性运动等挑战，并提供了各种跟踪任务的基线结果。</div>
</details>
</div>
<div class="card">
<div class="title">PAT++: a cautionary tale about generative visual augmentation for Object   Re-identification</div>
<div class="meta-line">Authors: Leonardo Santiago Benitez Pereira, Arathy Jeevan</div>
<div class="meta-line">First: 2025-07-19T15:01:05+00:00 · Latest: 2025-07-19T15:01:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.15888v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.15888v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative data augmentation has demonstrated gains in several vision tasks,
but its impact on object re-identification - where preserving fine-grained
visual details is essential - remains largely unexplored. In this work, we
assess the effectiveness of identity-preserving image generation for object
re-identification. Our novel pipeline, named PAT++, incorporates Diffusion
Self-Distillation into the well-established Part-Aware Transformer. Using the
Urban Elements ReID Challenge dataset, we conduct extensive experiments with
generated images used for both model training and query expansion. Our results
show consistent performance degradation, driven by domain shifts and failure to
retain identity-defining features. These findings challenge assumptions about
the transferability of generative models to fine-grained recognition tasks and
expose key limitations in current approaches to visual augmentation for
identity-preserving applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PAT++：关于生成视觉增强在物体再识别中保留身份的警示故事</div>
<div class="mono" style="margin-top:8px">生成数据增强在多个视觉任务中已经显示出改进，但在物体再识别中——保持细粒度视觉细节至关重要——其影响仍基本未被探索。在本研究中，我们评估了身份保留图像生成在物体再识别中的有效性。我们提出了一种名为PAT++的新管道，将扩散自我蒸馏整合到现有的部分感知变换器中。使用Urban Elements ReID挑战数据集，我们进行了广泛的实验，使用生成的图像进行模型训练和查询扩展。我们的结果显示了一致的性能下降，由领域偏移和无法保留身份定义特征驱动。这些发现挑战了生成模型在细粒度识别任务中的可转移性假设，并揭示了当前视觉增强方法在身份保留应用中的关键局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study evaluates the use of identity-preserving image generation for object re-identification, a task that requires maintaining fine-grained visual details. The research introduces PAT++, which combines Diffusion Self-Distillation with Part-Aware Transformer. Experiments on the Urban Elements ReID Challenge dataset reveal consistent performance decline, attributed to domain shifts and failure to retain critical identity features, challenging the assumption of generative models&#x27; transferability to such tasks.</div>
<div class="mono" style="margin-top:8px">该研究探讨了生成数据增强在对象重识别中的应用，这是一个需要保留细粒度视觉细节的任务。研究人员开发了一个名为PAT++的新管道，将Diffusion Self-Distillation集成到Part-Aware Transformer中。在Urban Elements ReID挑战数据集上的实验表明，当使用生成图像进行训练和查询扩展时，性能持续下降，主要原因是领域偏移和未能保留身份定义特征。这些结果表明，生成模型可能不适合细粒度识别任务，并揭示了当前用于身份保留应用的视觉增强方法的关键限制。</div>
</details>
</div>
<div class="card">
<div class="title">When Person Re-Identification Meets Event Camera: A Benchmark Dataset   and An Attribute-guided Re-Identification Framework</div>
<div class="meta-line">Authors: Xiao Wang, Qian Zhu, Shujuan Wu, Bo Jiang, Shiliang Zhang, Yaowei Wang, Yonghong Tian, Bin Luo</div>
<div class="meta-line">First: 2025-07-18T05:04:59+00:00 · Latest: 2025-07-18T05:04:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.13659v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.13659v1">PDF</a> · <a href="https://github.com/Event-AHU/Neuromorphic_ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当人员重识别遇到事件相机：一个基准数据集和一种基于属性的重识别框架</div>
<div class="mono" style="margin-top:8px">近期研究人员提出使用事件相机进行人员重识别（ReID），因其出色的性能和更好的隐私保护平衡，基于事件相机的人员ReID引起了广泛关注。目前，主流的事件驱动人员ReID算法主要集中在融合可见光和事件流以及保护隐私方面。尽管取得了显著进展，但这些方法通常在小型或模拟事件相机数据集上进行训练和评估，难以评估其实际识别性能和泛化能力。为解决数据稀缺问题，本文引入了一个大规模的RGB-事件驱动人员ReID数据集，称为EvReID。该数据集包含118,988对图像，涵盖1200个行人类别身份，数据采集跨越多个季节、场景和光照条件。我们还评估了15种最先进的人员ReID算法，为未来的研究提供了坚实的数据和基准。基于我们新构建的数据集，本文进一步提出了一种行人属性引导的对比学习框架，以增强人员重识别的特征学习，称为TriPro-ReID。该框架不仅有效探索了来自RGB帧和事件流的视觉特征，还充分利用了行人的属性作为中间语义特征。在EvReID数据集和MARS数据集上的广泛实验充分验证了我们提出的RGB-事件驱动人员ReID框架的有效性。基准数据集和源代码将在https://github.com/Event-AHU/Neuromorphic_ReID上发布</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the scarcity of large-scale datasets for event camera-based person re-identification (ReID) by introducing a new dataset called EvReID, which includes 118,988 image pairs and covers 1200 pedestrian identities under various conditions. The authors also propose a new framework, TriPro-ReID, which uses pedestrian attributes to enhance feature learning and validate its effectiveness through experiments on EvReID and MARS datasets.</div>
<div class="mono" style="margin-top:8px">本文通过引入一个新的名为EvReID的数据集来解决事件相机基于的人像再识别（ReID）数据稀缺的问题，该数据集包含118,988对图像，涵盖了1200个行人身份在多种条件下的数据。作者还提出了一种新的框架TriPro-ReID，利用行人属性来增强特征学习，并通过在EvReID和MARS数据集上的实验验证其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Weakly Supervised Visible-Infrared Person Re-Identification via   Heterogeneous Expert Collaborative Consistency Learning</div>
<div class="meta-line">Authors: Yafei Zhang, Lingqi Kong, Huafeng Li, Jie Wen</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-07-17T09:31:34+00:00 · Latest: 2025-07-17T09:31:34+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.12942v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.12942v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To reduce the reliance of visible-infrared person re-identification (ReID)
models on labeled cross-modal samples, this paper explores a weakly supervised
cross-modal person ReID method that uses only single-modal sample identity
labels, addressing scenarios where cross-modal identity labels are unavailable.
To mitigate the impact of missing cross-modal labels on model performance, we
propose a heterogeneous expert collaborative consistency learning framework,
designed to establish robust cross-modal identity correspondences in a weakly
supervised manner. This framework leverages labeled data from each modality to
independently train dedicated classification experts. To associate cross-modal
samples, these classification experts act as heterogeneous predictors,
predicting the identities of samples from the other modality. To improve
prediction accuracy, we design a cross-modal relationship fusion mechanism that
effectively integrates predictions from different experts. Under the implicit
supervision provided by cross-modal identity correspondences, collaborative and
consistent learning among the experts is encouraged, significantly enhancing
the model&#x27;s ability to extract modality-invariant features and improve
cross-modal identity recognition. Experimental results on two challenging
datasets validate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于异质专家协作一致性学习的弱监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">为减少可见-红外行人重识别（ReID）模型对跨模态标注样本的依赖，本文探索了一种仅使用单模态样本身份标签的弱监督跨模态行人ReID方法，以应对跨模态身份标签不可用的场景。为减轻缺失跨模态标签对模型性能的影响，我们提出了一种异质专家协作一致性学习框架，旨在以弱监督方式建立稳健的跨模态身份对应关系。该框架利用每种模态的标注数据独立训练专门的分类专家。为了关联跨模态样本，这些分类专家作为异质预测器，预测另一模态样本的身份。为了提高预测准确性，我们设计了一种跨模态关系融合机制，有效整合不同专家的预测结果。在跨模态身份对应关系的隐式监督下，专家间的协作和一致学习被鼓励，显著增强了模型提取模态不变特征和跨模态身份识别的能力。在两个具有挑战性的数据集上的实验结果验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a weakly supervised cross-modal person re-identification method that uses only single-modal sample identity labels to reduce reliance on labeled cross-modal samples. It introduces a heterogeneous expert collaborative consistency learning framework to establish robust cross-modal identity correspondences. The framework trains dedicated classification experts for each modality and uses a cross-modal relationship fusion mechanism to integrate their predictions, leading to improved cross-modal identity recognition. Experiments on two challenging datasets demonstrate the method&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种仅使用单模态样本身份标签的弱监督跨模态人员重识别方法，以减少对跨模态标注样本的依赖。它引入了一种异质专家协作一致性学习框架，为每个模态训练专门的分类专家，并使用跨模态关系融合机制整合他们的预测。该框架促进了专家之间的协作和一致学习，提高了模型提取模态不变特征和增强跨模态身份识别的能力。实验结果在两个具有挑战性的数据集上验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel   Dataset and Method</div>
<div class="meta-line">Authors: Han Wang, Shengyang Li, Jian Yang, Yuxuan Liu, Yixuan Lv, Zhuang Zhou</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-06-27T09:13:22+00:00 · Latest: 2025-07-16T02:40:11+00:00</div>
<div class="meta-line">Comments: Accepted to ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.22027v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.22027v3">PDF</a> · <a href="https://github.com/Alioth2000/Hoss-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting and tracking ground objects using earth observation imagery remains
a significant challenge in the field of remote sensing. Continuous maritime
ship tracking is crucial for applications such as maritime search and rescue,
law enforcement, and shipping analysis. However, most current ship tracking
methods rely on geostationary satellites or video satellites. The former offer
low resolution and are susceptible to weather conditions, while the latter have
short filming durations and limited coverage areas, making them less suitable
for the real-world requirements of ship tracking. To address these limitations,
we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship
Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the
effectiveness of ship tracking using low-Earth orbit constellations of optical
and SAR sensors. This approach ensures shorter re-imaging cycles and enables
all-weather tracking. HOSS ReID dataset includes images of the same ship
captured over extended periods under diverse conditions, using different
satellites of different modalities at varying times and angles. Furthermore, we
propose a baseline method for cross-modal ship re-identification, TransOSS,
which is built on the Vision Transformer architecture. It refines the patch
embedding structure to better accommodate cross-modal tasks, incorporates
additional embeddings to introduce more reference information, and employs
contrastive learning to pre-train on large-scale optical-SAR image pairs,
ensuring the model&#x27;s ability to extract modality-invariant features. Our
dataset and baseline method are publicly available on
https://github.com/Alioth2000/Hoss-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于光学和SAR图像的跨模态船舶重识别：一个新型数据集和方法</div>
<div class="mono" style="margin-top:8px">利用地球观测图像检测和跟踪地面目标仍然是遥感领域的一个重大挑战。持续的海上船舶跟踪对于海上搜救、法律执行和航运分析等应用至关重要。然而，目前大多数船舶跟踪方法依赖于地球静止卫星或视频卫星。前者分辨率低且易受天气条件影响，后者拍摄时间短且覆盖区域有限，使其不太适合船舶跟踪的实际需求。为解决这些限制，我们提出了混合光学和合成孔径雷达（SAR）船舶重识别数据集（HOSS重识别数据集），旨在评估使用低地球轨道光学和SAR传感器星座进行船舶跟踪的有效性。这种方法确保了较短的重拍周期，并实现了全天候跟踪。HOSS重识别数据集包括在不同条件下、不同时段和不同角度由不同类型的卫星拍摄的同一船舶的图像。此外，我们提出了一种跨模态船舶重识别的基线方法TransOSS，该方法基于视觉变换器架构。它改进了块嵌入结构以更好地适应跨模态任务，引入了额外的嵌入以提供更多参考信息，并采用了对比学习在大规模光学-SAR图像对上进行预训练，确保模型能够提取模态不变特征。我们的数据集和基线方法已公开发布在https://github.com/Alioth2000/Hoss-ReID。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of continuous maritime ship tracking using earth observation imagery. It introduces the HOSS ReID dataset, which includes images of ships captured under various conditions using both optical and SAR sensors. The dataset is designed to evaluate the effectiveness of ship tracking with low-Earth orbit constellations. Additionally, the paper proposes a baseline method called TransOSS, which uses a Vision Transformer architecture to perform cross-modal ship re-identification, achieving better performance in extracting modality-invariant features compared to existing methods.</div>
<div class="mono" style="margin-top:8px">论文旨在解决使用地球观测图像进行连续海上船只跟踪的挑战。它引入了Hybrid Optical and Synthetic Aperture Radar (SAR) Ship Re-Identification Dataset (HOSS ReID数据集)，以评估使用低地球轨道光学和SAR传感器进行船只跟踪的有效性。该数据集包含在不同条件和时间拍摄的同一船只的图像。作者提出了一种基于Vision Transformer架构的基线方法TransOSS，该方法改进了跨模态任务中的patch嵌入和对比学习，确保了全天候跟踪和较短的重拍周期。数据集和方法已公开可用。</div>
</details>
</div>
<div class="card">
<div class="title">Try Harder: Hard Sample Generation and Learning for Clothes-Changing   Person Re-ID</div>
<div class="meta-line">Authors: Hankun Liu, Yujian Zhao, Guanglin Niu</div>
<div class="meta-line">First: 2025-07-15T09:14:01+00:00 · Latest: 2025-07-15T09:14:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.11119v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.11119v1">PDF</a> · <a href="https://github.com/undooo/TryHarder-ACMMM25">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model&#x27;s robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model&#x27;s discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更努力：服装变化的人再识别中困难样本生成与学习</div>
<div class="mono" style="margin-top:8px">困难样本在人再识别（ReID）任务中构成了重大挑战，尤其是在服装变化的人再识别（CC-ReID）中。它们固有的模糊性或相似性，以及缺乏明确定义，使得它们成为根本性的瓶颈。这些问题不仅限制了目标学习策略的设计，还降低了模型在服装或视角变化下的鲁棒性。在本文中，我们提出了一种新颖的多模态引导困难样本生成与学习（HSGL）框架，这是首次尝试将文本和视觉模态统一起来，明确定义、生成和优化困难样本。HSGL 包含两个核心组件：（1）双粒度困难样本生成（DGHSG），利用多模态线索合成语义一致的样本，包括粗粒度和细粒度的困难正样本和负样本，以有效增加训练数据的难度和多样性。（2）困难样本自适应学习（HSAL），引入了一种基于文本语义标签的难度感知优化策略，根据特征距离调整，鼓励分离困难正样本并将困难负样本拉近嵌入空间，以增强模型的判别能力和对困难样本的鲁棒性。在多个 CC-ReID 基准上的广泛实验表明，我们的方法有效，并突出了多模态引导困难样本生成与学习在鲁棒 CC-ReID 中的潜力。值得注意的是，HSAL 显著加速了目标学习过程的收敛，并在 PRCC 和 LTCC 数据集上达到了最先进的性能。代码可在 https://github.com/undooo/TryHarder-ACMMM25 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of hard samples in clothing-changing person re-identification tasks. It introduces a multimodal-guided Hard Sample Generation and Learning (HSGL) framework that combines textual and visual information to define, generate, and optimize hard samples. The framework includes Dual-Granularity Hard Sample Generation (DGHSG) for creating diverse hard positives and negatives, and Hard Sample Adaptive Learning (HSAL) for enhancing model robustness through hardness-aware optimization. Experiments show that HSGL improves performance and accelerates convergence on multiple benchmarks, achieving state-of-the-art results on PRCC and LTCC datasets.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的多模态引导的Hard Sample Generation and Learning (HSGL)框架，以解决服装变化的人再识别（CC-ReID）中的硬样本问题。该框架包括Dual-Granularity Hard Sample Generation (DGHSG) 用于生成语义一致的硬样本，以及Hard Sample Adaptive Learning (HSAL) 用于基于文本语义标签优化特征距离。实验表明，HSGL 提高了模型的鲁棒性和判别能力，并在PRCC和LTCC数据集上达到了最先进的性能。特别地，HSAL 加速了收敛并增强了模型在硬样本下的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Colors See Colors Ignore: Clothes Changing ReID with Color   Disentanglement</div>
<div class="meta-line">Authors: Priyank Pathak, Yogesh S. Rawat</div>
<div class="meta-line">Venue: ICCV</div>
<div class="meta-line">First: 2025-07-09T19:05:46+00:00 · Latest: 2025-07-09T19:05:46+00:00</div>
<div class="meta-line">Comments: ICCV&#x27;25 paper</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.07230v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.07230v1">PDF</a> · <a href="https://github.com/ppriyank/ICCV-CSCI-Person-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals
across different locations and times, irrespective of clothing. Existing
methods often rely on additional models or annotations to learn robust,
clothing-invariant features, making them resource-intensive. In contrast, we
explore the use of color - specifically foreground and background colors - as a
lightweight, annotation-free proxy for mitigating appearance bias in ReID
models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that
leverages color information directly from raw images or video frames. CSCI
efficiently captures color-related appearance bias (&#x27;Color See&#x27;) while
disentangling it from identity-relevant ReID features (&#x27;Color Ignore&#x27;). To
achieve this, we introduce S2A self-attention, a novel self-attention to
prevent information leak between color and identity cues within the feature
space. Our analysis shows a strong correspondence between learned color
embeddings and clothing attributes, validating color as an effective proxy when
explicit clothing labels are unavailable. We demonstrate the effectiveness of
CSCI on both image and video ReID with extensive experiments on four CC-ReID
datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for
image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID
without relying on additional supervision. Our results highlight the potential
of color as a cost-effective solution for addressing appearance bias in
CC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>颜色看颜色忽略：基于颜色解缠的换装重识别</div>
<div class="mono" style="margin-top:8px">换装重识别（CC-ReID）旨在识别不同地点和时间的个体，不依赖于服装。现有方法通常依赖额外的模型或注释来学习鲁棒的、服装不变的特征，使其资源密集。相比之下，我们探索使用颜色——特别是前景和背景颜色——作为无注释的轻量级代理，以减轻重识别模型中的外观偏差。我们提出了颜色看颜色忽略（CSCI）方法，这是一种仅基于RGB的方法，直接从原始图像或视频帧中利用颜色信息。CSCI有效地捕捉了与颜色相关的外观偏差（“颜色看”）并将其与与身份相关的重识别特征（“颜色忽略”）解缠。为此，我们引入了S2A自注意力，这是一种新颖的自注意力机制，以防止特征空间内颜色和身份线索之间的信息泄露。我们的分析表明，学习到的颜色嵌入与服装属性之间存在强烈的相关性，验证了在显式服装标签不可用时，颜色作为有效代理的有效性。我们在四个CC-ReID数据集上进行了广泛的实验，证明了CSCI在图像和视频重识别中的有效性。我们提高了LTCC上的Top-1基线2.9%，PRCC上的5.0%，CCVID上的1.0%，MeVID上的2.5%，而无需依赖额外的监督。我们的结果突显了颜色作为解决CC-ReID中外观偏差成本效益解决方案的潜力。Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of recognizing individuals across different locations and times without being influenced by their clothing. The proposed method, Colors See, Colors Ignore (CSCI), uses color information from raw images to mitigate appearance bias in ReID models. CSCI improves the baseline by 2.9% on LTCC and 5.0% on PRCC for image-based ReID, and by 1.0% on CCVID and 2.5% on MeVID for video-based ReID, without requiring additional supervision. The method introduces S2A self-attention to disentangle color-related appearance bias from identity-relevant features, validating color as an effective proxy for clothing attributes when explicit labels are unavailable.</div>
<div class="mono" style="margin-top:8px">研究旨在解决在不同时间和地点识别个体而不受服装变化影响的挑战。提出的Colors See, Colors Ignore (CSCI) 方法直接从原始图像中使用颜色信息来减轻ReID模型中的外观偏差。CSCI 在基于图像的ReID 上提高了LTCC 2.9% 和PRCC 5.0% 的Top-1 准确率，在基于视频的ReID 上提高了CCVID 1.0% 和MeVID 2.5% 的Top-1 准确率，无需额外监督。该方法引入了S2A 自注意力机制，以防止特征空间中颜色和身份线索之间的信息泄露，验证了当没有明确标签时，颜色作为服装属性的有效代理的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">UGG-ReID: Uncertainty-Guided Graph Model for Multi-Modal Object   Re-Identification</div>
<div class="meta-line">Authors: Xixi Wan, Aihua Zheng, Bo Jiang, Beibei Wang, Chenglong Li, Jin Tang</div>
<div class="meta-line">First: 2025-07-07T03:41:08+00:00 · Latest: 2025-07-08T02:49:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04638v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.04638v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal object Re-IDentification (ReID) has gained considerable attention
with the goal of retrieving specific targets across cameras using heterogeneous
visual data sources. Existing methods primarily aim to improve identification
performance, but often overlook the uncertainty arising from inherent defects,
such as intra-modal noise and inter-modal conflicts. This uncertainty is
particularly significant in the case of fine-grained local occlusion and frame
loss, which becomes a challenge in multi-modal learning. To address the above
challenge, we propose a robust approach named Uncertainty-Guided Graph model
for multi-modal object ReID (UGG-ReID). UGG-ReID is designed to mitigate noise
interference and facilitate effective multi-modal fusion by estimating both
local and sample-level aleatoric uncertainty and explicitly modeling their
dependencies. Specifically, we first propose the Gaussian patch-graph
representation model that leverages uncertainty to quantify fine-grained local
cues and capture their structural relationships. This process boosts the
expressiveness of modal-specific information, ensuring that the generated
embeddings are both more informative and robust. Subsequently, we design an
uncertainty-guided mixture of experts strategy that dynamically routes samples
to experts exhibiting low uncertainty. This strategy effectively suppresses
noise-induced instability, leading to enhanced robustness. Meanwhile, we design
an uncertainty-guided routing to strengthen the multi-modal interaction,
improving the performance. UGG-ReID is comprehensively evaluated on five
representative multi-modal object ReID datasets, encompassing diverse spectral
modalities. Experimental results show that the proposed method achieves
excellent performance on all datasets and is significantly better than current
methods in terms of noise immunity. Our code will be made public upon
acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UGG-ReID：基于不确定性指导的多模态对象重识别图模型</div>
<div class="mono" style="margin-top:8px">多模态对象重识别（Re-ID）因其目标是在不同摄像机之间使用异构视觉数据源检索特定目标而引起了广泛关注。现有方法主要致力于提高识别性能，但往往忽视了由内在缺陷（如同模态噪声和跨模态冲突）引起的不确定性。这种不确定性在细粒度局部遮挡和帧丢失的情况下尤为显著，成为多模态学习的挑战。为应对上述挑战，我们提出了一种鲁棒方法，即基于不确定性指导的多模态对象重识别（UGG-ReID）图模型。UGG-ReID旨在通过估计局部和样本级的 aleatoric 不确定性及其依赖关系来减轻噪声干扰并促进有效的多模态融合。具体而言，我们首先提出了高斯补丁图表示模型，该模型利用不确定性来量化细粒度局部线索并捕获它们的结构关系。这一过程增强了模态特定信息的表达性，确保生成的嵌入既更具信息性又更鲁棒。随后，我们设计了一种基于不确定性的混合专家策略，该策略动态地将样本路由到不确定性较低的专家。该策略有效地抑制了噪声引起的不稳定性，从而提高了鲁棒性。同时，我们设计了一种基于不确定性的路由策略以加强多模态交互，从而提高性能。UGG-ReID 在五个代表性的多模态对象重识别数据集上进行了全面评估，这些数据集涵盖了多种光谱模态。实验结果表明，所提出的方法在所有数据集上均表现出色，并且在噪声免疫方面显著优于当前方法。我们的代码将在接受后公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve multi-modal object Re-ID by addressing uncertainties such as intra-modal noise and inter-modal conflicts. The UGG-ReID model estimates both local and sample-level aleatoric uncertainty and models their dependencies to mitigate noise interference. Experimental results demonstrate that UGG-ReID outperforms existing methods in terms of noise immunity across five diverse datasets.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决内在缺陷如 intra-modal 噪声和 inter-modal 冲突来提升多模态对象 Re-ID。UGG-ReID 模型通过高斯斑块图表示来估计局部和样本级的 aleatoric 不确定性，并建模它们之间的依赖关系。实验结果表明，UGG-ReID 在五个不同模态的数据集上表现出色，且在噪声免疫方面显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Who&#x27;s the Mole? Modeling and Detecting Intention-Hiding Malicious Agents   in LLM-Based Multi-Agent Systems</div>
<div class="meta-line">Authors: Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu</div>
<div class="meta-line">First: 2025-07-07T07:34:34+00:00 · Latest: 2025-07-07T07:34:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04724v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.04724v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate
remarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit
strong collaborative abilities, the security risks in their communication and
coordination remain underexplored. We bridge this gap by systematically
investigating intention-hiding threats in LLM-MAS, and design four
representative attack paradigms that subtly disrupt task completion while
maintaining high concealment. These attacks are evaluated in centralized,
decentralized, and layered communication structures. Experiments conducted on
six benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,
and biographies, demonstrate that they exhibit strong disruptive capabilities.
To identify these threats, we propose a psychology-based detection framework
AgentXposed, which combines the HEXACO personality model with the Reid
Technique, using progressive questionnaire inquiries and behavior-based
monitoring. Experiments conducted on six types of attacks show that our
detection framework effectively identifies all types of malicious behaviors.
The detection rate for our intention-hiding attacks is slightly lower than that
of the two baselines, Incorrect Fact Injection and Dark Traits Injection,
demonstrating the effectiveness of intention concealment. Our findings reveal
the structural and behavioral risks posed by intention-hiding attacks and offer
valuable insights into securing LLM-based multi-agent systems through
psychological perspectives, which contributes to a deeper understanding of
multi-agent safety. The code and data are available at
https://anonymous.4open.science/r/AgentXposed-F814.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>谁是鼹鼠？在基于大语言模型的多智能体系统中建模和检测意图隐藏的恶意代理</div>
<div class="mono" style="margin-top:8px">由大型语言模型（LLM）驱动的多智能体系统（LLM-MAS）展示了在协作问题解决方面的显著能力。尽管LLM-MAS表现出强大的协作能力，但它们在通信和协调方面的安全风险仍被忽视。我们通过系统地研究LLM-MAS中的意图隐藏威胁，设计了四种代表性的攻击模式，这些模式在维持高度隐蔽性的同时，微妙地破坏任务完成。这些攻击在集中式、分散式和分层通信结构中进行了评估。在包括MMLU、MMLU-Pro、HumanEval、GSM8K、算术和传记在内的六个基准数据集上进行的实验表明，它们具有很强的破坏能力。为了识别这些威胁，我们提出了一种基于心理学的检测框架AgentXposed，该框架结合了HEXACO人格模型和Reid技术，使用渐进式问卷调查和基于行为的监控。在六种类型攻击的实验中，我们的检测框架有效地识别了所有类型的恶意行为。我们的意图隐藏攻击的检测率略低于两个基线，即错误事实注入和暗特质注入，这表明意图隐藏的有效性。我们的研究揭示了意图隐藏攻击带来的结构和行为风险，并从心理学角度提供了确保基于大语言模型的多智能体系统的宝贵见解，有助于更深入地理解多智能体安全。相关代码和数据可在https://anonymous.4open.science/r/AgentXposed-F814获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the security risks in Large Language Model-based Multi-Agent Systems (LLM-MAS) by investigating intention-hiding attacks that subtly disrupt task completion. Four attack paradigms were designed and evaluated in different communication structures, showing strong disruptive capabilities. A detection framework, AgentXposed, combining the HEXACO personality model and the Reid Technique, was proposed to identify malicious behaviors. While the detection rate for intention-hiding attacks is slightly lower than for other types, the framework effectively identifies all malicious behaviors, highlighting the need for psychological perspectives in securing LLM-MAS.</div>
<div class="mono" style="margin-top:8px">该研究通过调查意图隐藏攻击来解决大型语言模型驱动的多智能体系统（LLM-MAS）的安全风险。设计了四种攻击模式，以微妙地破坏任务完成并保持高度隐蔽性。这些攻击在不同的通信结构中进行了评估，并展示了强大的破坏能力。提出的检测框架AgentXposed结合了HEXACO人格模型和Reid技术，有效地识别了恶意行为。然而，意图隐藏攻击的检测率略低于其他类型的攻击。</div>
</details>
</div>
<div class="card">
<div class="title">SCING:Towards More Efficient and Robust Person Re-Identification through   Selective Cross-modal Prompt Tuning</div>
<div class="meta-line">Authors: Yunfei Xie, Yuxuan Cheng, Juncheng Wu, Haoyu Zhang, Yuyin Zhou, Shoudong Han</div>
<div class="meta-line">First: 2025-07-01T07:21:31+00:00 · Latest: 2025-07-01T07:21:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.00506v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.00506v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCING:通过选择性跨模态提示调优迈向更高效和稳健的人再识别</div>
<div class="mono" style="margin-top:8px">近期将CLIP等视觉语言预训练模型适应人再识别(ReID)任务的方法往往依赖于复杂的适配器设计或模态特定调优，而忽视了跨模态交互，导致高计算成本或对齐效果不佳。为解决这些局限性，我们提出了一种简单而有效的框架——选择性跨模态提示调优(SCING)，该框架增强了跨模态对齐并提高了对真实世界干扰的鲁棒性。我们的方法引入了两个关键创新：首先，我们提出了选择性视觉提示融合(SVIP)，这是一种轻量级模块，通过跨模态门控机制动态地将具有区分性的视觉特征注入到文本提示中。此外，我们提出的扰动驱动一致性对齐(PDCA)是一种双路径训练策略，通过正则化原始和增强的跨模态嵌入的一致性，确保在随机图像扰动下的不变特征对齐。我们在Market1501、DukeMTMC-ReID、Occluded-Duke、Occluded-REID和P-DukeMTMC等多个流行基准上进行了广泛的实验，证明了所提方法的出色性能。值得注意的是，我们的框架消除了沉重的适配器，同时保持了高效的推理，实现了性能和计算开销之间的最佳权衡。代码将在接受后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the efficiency and robustness of person re-identification (ReID) tasks by addressing the limitations of complex adapter designs and modality-specific tuning. The proposed SCING framework introduces Selective Visual Prompt Fusion (SVIP) to dynamically inject discriminative visual features into text prompts and Perturbation-Driven Consistency Alignment (PDCA) to enforce invariant feature alignment under image perturbations. Experiments on various benchmarks show that SCING achieves superior performance with efficient inference, maintaining a balance between performance and computational overhead.</div>
<div class="mono" style="margin-top:8px">该论文提出了SCING框架，通过提高跨模态对齐和鲁棒性来增强人员重识别。该框架通过Selective Visual Prompt Fusion动态将视觉特征注入文本提示，并通过Perturbation-Driven Consistency Alignment的双路径训练策略确保在图像扰动下的特征一致性。在多个基准测试上的实验表明，SCING在保持高效推理和计算开销的同时，优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
