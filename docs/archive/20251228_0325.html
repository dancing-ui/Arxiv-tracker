<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-28 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251228_0325</div>
    <div class="row"><div class="card">
<div class="title">Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Menglin Wang, Xiaojin Gong, Jiachen Li, Genlin Ji</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-08T17:42:28+00:00 · Latest: 2025-12-08T17:42:28+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07760v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.07760v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast&#x27; strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模态感知偏差缓解与不变表示学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在无需任何标注的情况下，在可见光和红外摄像机之间匹配个体。由于可见光和红外模态之间存在显著差异，估计可靠的跨模态关联成为USVI-ReID中的主要挑战。现有方法通常采用最优传输来关联内模态聚类，这容易传播局部聚类错误，同时也忽略了全局实例级关系。通过挖掘和关注可见光-红外模态偏差，本文从两个方面关注跨模态学习：偏差缓解的全局关联和模态不变表示学习。受单模态重识别中相机感知距离校正的启发，我们提出模态感知Jaccard距离来缓解由模态差异引起的距离偏差，从而通过全局聚类估计更可靠的跨模态关联。为了进一步提高跨模态表示学习，设计了一种“分割-对比”策略来获得模态特定的全局原型。通过在全局关联指导下显式对齐这些原型，可以实现模态不变但ID区分的表示学习。尽管概念上很简单，但我们的方法在基准VI-ReID数据集上取得了最先进的性能，并显著优于现有方法，验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by focusing on mitigating modality bias and learning modality-invariant representations. It proposes a modality-aware Jaccard distance to improve global clustering and a `split-and-contrast&#x27; strategy to align modality-specific global prototypes. The method achieves state-of-the-art performance on benchmark datasets, demonstrating its effectiveness in addressing cross-modality learning challenges.</div>
<div class="mono" style="margin-top:8px">该论文通过提出一种模态感知的方法来缓解偏见并学习不变表示，解决了无监督的可见光-红外人再识别问题。方法使用模态感知的Jaccard距离来纠正模态间的距离偏差，并采用`分割和对比&#x27;策略在全局关联指导下对模态特定的全局原型进行对齐。实验结果表明，该方法在基准数据集上的表现优于现有方法，证明了其在提高跨模态匹配准确性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Jiaze Li, Yan Lu, Bin Liu, Guojun Yin, Mang Ye</div>
<div class="meta-line">First: 2025-12-03T12:43:16+00:00 · Latest: 2025-12-03T12:43:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03745v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.03745v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双层次模态去偏学习在无监督可见光-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">两阶段学习管道在无监督可见光-红外行人重识别（USL-VI-ReID）中取得了令人鼓舞的结果。它首先进行单模态学习，然后进行跨模态学习以解决模态差异问题。尽管如此，该管道不可避免地引入了模态偏见：单模态训练中学习的模态特定线索自然传播到后续的跨模态学习中，影响身份识别和泛化能力。为了解决这一问题，我们提出了一种双层次模态去偏学习（DMDL）框架，该框架在模型和优化两个层面实施去偏。在模型层面，我们提出了一种因果启发调整干预（CAI）模块，用因果建模替代基于似然的建模，防止由模态引入的虚假模式，从而得到低偏见模型。在优化层面，引入了一种协作无偏训练（CBT）策略，通过集成模态特定增强、标签细化和特征对齐来中断模态偏见在数据、标签和特征之间的传播。在基准数据集上的广泛实验表明，DMDL 可以实现模态不变特征学习和更泛化的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of modality bias in unsupervised visible-infrared person re-identification by proposing a Dual-level Modality Debiasing Learning (DMDL) framework. This framework includes a Causality-inspired Adjustment Intervention (CAI) module at the model level to prevent modality-induced spurious patterns, and a Collaborative Bias-free Training (CBT) strategy at the optimization level to interrupt the propagation of modality bias. Experiments show that DMDL can achieve modality-invariant feature learning and better generalization.</div>
<div class="mono" style="margin-top:8px">论文提出了一种双重模态去偏学习（DMDL）框架，以解决无监督可见-红外行人重识别中的模态偏见问题。该框架在模型层面引入了因果启发调整干预（CAI）模块，在优化层面引入了协作无偏训练（CBT）策略。CAI模块防止了模态特定线索引起的虚假模式，而CBT策略中断了模态偏见在数据、标签和特征之间的传播。实验表明，DMDL提升了模态不变特征学习能力并增强了模型的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2025-09-15T05:10:43+00:00 · Latest: 2025-09-15T05:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.11587v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.11587v1">PDF</a> · <a href="https://github.com/haonanshi0125/HIL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to learn modality-invariant image features from unlabeled cross-modal person datasets by reducing the modality gap while minimizing reliance on costly manual annotations. Existing methods typically address USVI-ReID using cluster-based contrastive learning, which represents a person by a single cluster center. However, they primarily focus on the commonality of images within each cluster while neglecting the finer-grained differences among them. To address the limitation, we propose a Hierarchical Identity Learning (HIL) framework. Since each cluster may contain several smaller sub-clusters that reflect fine-grained variations among images, we generate multiple memories for each existing coarse-grained cluster via a secondary clustering. Additionally, we propose Multi-Center Contrastive Learning (MCCL) to refine representations for enhancing intra-modal clustering and minimizing cross-modal discrepancies. To further improve cross-modal matching quality, we design a Bidirectional Reverse Selection Transmission (BRST) mechanism, which establishes reliable cross-modal correspondences by performing bidirectional matching of pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets demonstrate that the proposed method outperforms existing approaches. The source code is available at: https://github.com/haonanshi0125/HIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外行人重识别的分层身份学习</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在通过减少模态差异并减少对昂贵的手动注释的依赖，从跨模态行人无标签数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来处理USVI-ReID，通过单个聚类中心表示一个人。然而，它们主要关注每个聚类内的图像共性，而忽视了它们之间的细微差异。为了解决这一局限性，我们提出了一种分层身份学习（HIL）框架。由于每个聚类可能包含多个反映图像间细微差异的较小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习（MCCL）以细化表示，增强同模态聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输（BRST）机制，通过双向匹配伪标签来建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法优于现有方法。源代码可在：https://github.com/haonanshi0125/HIL 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Hierarchical Identity Learning (HIL) framework. This framework generates multiple memories from coarse-grained clusters to capture finer-grained variations and uses Multi-Center Contrastive Learning and Bidirectional Reverse Selection Transmission to refine representations and improve cross-modal matching. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing approaches.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有方法仅关注簇内共性的问题，提高无监督的可见光-红外人像再识别。提出了层次身份学习（HIL）框架，为每个簇生成多个记忆，并使用多中心对比学习来细化表示。此外，还设计了双向反向选择传输机制，以增强跨模态匹配质量。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu</div>
<div class="meta-line">First: 2025-05-05T10:36:52+00:00 · Latest: 2025-05-06T07:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.02549v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.02549v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. Existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. In practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. To address this, we introduce a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. To this end, we propose a novel Robust Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning mechanism (RAL) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. Second, to alleviate error accumulation-where the model reinforces its own mistakes-RoDE employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. However, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. To resolve this, we introduce Cluster Consistency Matching (CCM), which aligns clusters across models and modalities by measuring cross-cluster similarity. Extensive experiments on three benchmarks demonstrate the effectiveness of RoDE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒对偶学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（UVI-ReID）旨在无需昂贵标注的情况下，在不同模态中检索行人图像，但面临着模态差距和缺乏监督的挑战。现有方法通常采用自训练结合聚类生成的伪标签，但隐含地假设这些标签总是正确的。然而，在实践中，由于不可避免的伪标签噪声，这一假设会失败，阻碍模型学习。为解决这一问题，我们引入了一种新的学习范式，明确考虑伪标签噪声（PLN），并提出了三个关键挑战：噪声过拟合、错误累积和嘈杂的簇对应关系。为此，我们提出了一种新颖的鲁棒对偶学习框架（RoDE）以减轻噪声伪标签的影响。首先，为了对抗噪声过拟合，我们提出了一种鲁棒自适应学习机制（RAL），动态强调干净样本并降低噪声样本的权重。其次，为了缓解错误累积——即模型强化其自身的错误，RoDE 使用来自彼此伪标签的双模型交替训练，鼓励多样性并防止模型崩溃。然而，这种双模型策略引入了模型和模态之间簇的对齐问题，产生了嘈杂的簇对应关系。为解决这一问题，我们引入了簇一致性匹配（CCM），通过测量跨簇相似性来对齐模型和模态之间的簇。在三个基准上的广泛实验表明了RoDE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a Robust Duality Learning framework (RoDE) that explicitly handles pseudo-label noise. RoDE includes a Robust Adaptive Learning mechanism to mitigate noise overfitting and dual distinct models to prevent error accumulation. To align clusters across models and modalities, Cluster Consistency Matching is introduced. Experiments on three benchmarks show RoDE&#x27;s effectiveness in improving UVI-ReID performance despite noisy pseudo-labels.</div>
<div class="mono" style="margin-top:8px">论文通过引入一个明确考虑伪标签噪声的新学习范式，解决了无监督可见红外行人重识别（UVI-ReID）的问题。它提出了一个稳健的二元学习框架（RoDE），包括一种稳健自适应学习机制来处理噪声过拟合问题，以及双独立模型来防止错误累积。此外，还引入了集群一致性匹配（CCM）来跨模型和模态对齐集群。在三个基准上的实验表明，RoDE在缓解伪标签噪声影响方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at \href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于协作精炼的语义对齐学习在无监督VI-ReID中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在通过跨模态图像的伪标签关联算法统一伪标签，然后设计对比学习框架进行全局特征学习，以匹配不同模态下同一个体的行人图像，而无需人工注释。先前的方法忽略了由细粒度模式带来的特征表示和伪标签分布的跨模态变化，这导致仅优化全局特征时模态共享学习不足。为解决这一问题，我们提出了一种基于协作精炼的语义对齐学习（SALCR）框架，该框架为每个模态强调的特定细粒度模式构建优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一个双向伪标签统一模块（DAGI），然后执行细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。基于语义对齐特征及其相应的标签空间，构建优化目标。为缓解由噪声伪标签引起的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集，并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有最佳方法。我们的代码可在\href{https://github.com/FranklinLingfeng/code-for-SALCR}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of previous methods that focus only on global features. The proposed Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework introduces a Dual Association with Global Learning (DAGI) module to unify cross-modality pseudo-labels and a Fine-Grained Semantic-Aligned Learning (FGSAL) module to explore part-level patterns. Additionally, a Global-Part Collaborative Refinement (GPCR) module is used to refine global and part features. Experiments show that SALCR outperforms existing methods in terms of performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种语义对齐学习与协作精炼（SALCR）框架，以解决无监督可见-红外行人重识别的挑战。该框架引入了双向统一跨模态伪标签的Dual Association with Global Learning (DAGI) 模块，以及探索每个模态强调的部分级语义对齐模式的Fine-Grained Semantic-Aligned Learning (FGSAL) 模块。此外，还提出了一种Global-Part Collaborative Refinement (GPCR) 模块，动态挖掘全局和部分特征的可靠正样本集，优化实例间关系。实验表明，SALCR在该领域优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Extended Cross-Modality United Learning for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-26T09:30:26+00:00 · Latest: 2024-12-26T09:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.19134v1">Abs</a> · <a href="https://arxiv.org/pdf/2412.19134v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims to learn modality-invariant features from unlabeled cross-modality datasets and reduce the inter-modality gap. However, the existing methods lack cross-modality clustering or excessively pursue cluster-level association, which makes it difficult to perform reliable modality-invariant features learning. To deal with this issue, we propose a Extended Cross-Modality United Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we design ECUL to naturally integrates intra-modality clustering, inter-modality clustering and inter-modality instance selection, establishing compact and accurate cross-modality associations while reducing the introduction of noisy labels. Moreover, EMCC captures and filters the neighborhood relationships by extending the encoding vector, which further promotes the learning of modality-invariant and camera-invariant knowledge in terms of clustering algorithm. Finally, TSMem provides accurate and generalized proxy points for contrastive learning by updating the memory in stages. Extensive experiments results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL shows promising performance and even outperforms certain supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展跨模态联合学习在无监督可见光-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见光-红外行人重识别（USL-VI-ReID）旨在从跨模态的未标记数据集中学习模态不变特征并减少模态间差距。然而，现有方法缺乏跨模态聚类或过度追求聚类级别的关联，这使得可靠地学习模态不变特征变得困难。为了解决这一问题，我们提出了一种扩展跨模态联合学习（ECUL）框架，结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。具体而言，我们设计ECUL以自然地整合模态内聚类、模态间聚类和模态间实例选择，建立紧凑且准确的跨模态关联，同时减少引入噪声标签。此外，EMCC通过扩展编码向量来捕获和过滤邻域关系，进一步促进聚类算法中模态不变和相机不变知识的学习。最后，TSMem通过分阶段更新记忆为对比学习提供准确且通用的代理点。在SYSU-MM01和RegDB数据集上的广泛实验结果表明，提出的ECUL表现出色，甚至优于某些监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised learning for visible-infrared person re-identification by proposing an Extended Cross-Modality United Learning (ECUL) framework. This framework includes Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem) modules to improve modality-invariant feature learning. The experiments on SYSU-MM01 and RegDB datasets show that ECUL outperforms certain supervised methods and effectively reduces the inter-modality gap.</div>
<div class="mono" style="margin-top:8px">论文提出了一种扩展跨模态联合学习（ECUL）框架来解决无监督可见-红外行人再识别的挑战。该框架包括扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块，以增强跨模态聚类和实例选择，从而减少模态间差异。实验结果表明，ECUL在SYSU-MM01和RegDB数据集上表现出色，甚至优于某些监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Relieving Universal Label Noise for Unsupervised Visible-Infrared Person Re-Identification by Inferring from Neighbors</div>
<div class="meta-line">Authors: Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin</div>
<div class="meta-line">First: 2024-12-16T04:04:41+00:00 · Latest: 2024-12-16T04:04:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.12220v1">Abs</a> · <a href="https://arxiv.org/pdf/2412.12220v1">PDF</a> · <a href="https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of great research and practical significance yet remains challenging due to the absence of annotations. Existing approaches aim to learn modality-invariant representations in an unsupervised setting. However, these methods often encounter label noise within and across modalities due to suboptimal clustering results and considerable modality discrepancies, which impedes effective training. To address these challenges, we propose a straightforward yet effective solution for USL-VI-ReID by mitigating universal label noise using neighbor information. Specifically, we introduce the Neighbor-guided Universal Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in both homogeneous and heterogeneous spaces with soft labels derived from neighboring samples to reduce label noise. Additionally, we present the Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability by minimizing the influence of unreliable samples. Extensive experiments on the RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing USL-VI-ReID approaches, despite its simplicity. The source code is available at: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过从邻居推断缓解无监督可见-红外行人重识别的通用标签噪声</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）在研究和实际应用中具有重要意义，但由于缺乏标注数据，仍面临挑战。现有方法旨在在无监督设置中学习模态不变表示。然而，这些方法常常由于次优聚类结果和显著的模态差异而遇到模态内和跨模态的标签噪声，这阻碍了有效的训练。为应对这些挑战，我们提出了一种简单而有效的解决方案，通过利用邻居信息缓解USL-VI-ReID中的通用标签噪声。具体而言，我们引入了邻居引导的通用标签校准（N-ULC）模块，该模块用来自邻近样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们提出了邻居引导的动态加权（N-DW）模块，以通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，但我们的方法仍优于现有的USL-VI-ReID方法。源代码可在：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by proposing a method to mitigate universal label noise. It introduces the Neighbor-guided Universal Label Calibration (N-ULC) module, which uses soft labels from neighboring samples instead of hard pseudo labels to reduce label noise. Additionally, the Neighbor-guided Dynamic Weighting (N-DW) module is used to enhance training stability. Experiments on RegDB and SYSU-MM01 datasets show that this method outperforms existing approaches despite its simplicity.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决标签噪声问题来提升无监督的可见光-红外行人再识别。提出了邻域导向的通用标签校准（N-ULC）模块，从邻近样本生成软标签，并提出了邻域导向的动态加权（N-DW）模块以减少不可靠样本的影响。实验表明，该方法在RegDB和SYSU-MM01数据集上优于现有方法，尽管其结构简单。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modality-Camera Invariant Clustering for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yiming Yang, Weipeng Hu, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-11T09:31:03+00:00 · Latest: 2024-12-11T09:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.08231v1">Abs</a> · <a href="https://arxiv.org/pdf/2412.08231v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) offers a more flexible and cost-effective alternative compared to supervised methods. This field has gained increasing attention due to its promising potential. Existing methods simply cluster modality-specific samples and employ strong association techniques to achieve instance-to-cluster or cluster-to-cluster cross-modality associations. However, they ignore cross-camera differences, leading to noticeable issues with excessive splitting of identities. Consequently, this undermines the accuracy and reliability of cross-modal associations. To address these issues, we propose a novel Dynamic Modality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID. Specifically, our DMIC naturally integrates Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive Learning (HMCL) into a unified framework, which eliminates both the cross-modality and cross-camera discrepancies in clustering. MIE fuses inter-modal and inter-camera distance coding to bridge the gaps between modalities and cameras at the clustering level. DNC employs two dynamic search strategies to refine the network&#x27;s optimization objective, transitioning from improving discriminability to enhancing cross-modal and cross-camera generalizability. Moreover, HMCL is designed to optimize instance-level and cluster-level distributions. Memories for intra-modality and inter-modality training are updated using randomly selected samples, facilitating real-time exploration of modality-invariant representations. Extensive experiments have demonstrated that our DMIC addresses the limitations present in current clustering approaches and achieve competitive performance, which significantly reduces the performance gap with supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态模态-相机不变聚类在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）相比监督方法提供了更灵活和成本效益更高的替代方案。该领域因其潜在的前景而越来越受到关注。现有方法简单地聚类模态特定样本，并采用强关联技术实现跨模态实例到聚类或聚类到聚类的关联。然而，它们忽略了跨相机差异，导致身份分割过度，严重影响了跨模态关联的准确性和可靠性。为解决这些问题，我们提出了一种新颖的动态模态-相机不变聚类（DMIC）框架用于USL-VI-ReID。具体而言，我们的DMIC将模态-相机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）统一到一个框架中，消除了聚类中的跨模态和跨相机差异。MIE融合了跨模态和跨相机的距离编码，在聚类层面弥合了模态和相机之间的差距。DNC采用两种动态搜索策略来细化网络的优化目标，从提高可区分性过渡到增强跨模态和跨相机的一般性。此外，HMCL旨在优化实例级和聚类级的分布。使用随机选择的样本更新模态内和跨模态训练的记忆，促进模态不变表示的实时探索。大量实验表明，我们的DMIC解决了当前聚类方法的局限性，实现了竞争力的性能，显著缩小了与监督方法的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a Dynamic Modality-Camera Invariant Clustering (DMIC) framework for unsupervised visible-infrared person re-identification (USL-VI-ReID) to address the limitations of existing methods, such as excessive identity splitting due to cross-camera differences. DMIC integrates Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC), and Hybrid Modality Contrastive Learning (HMCL) to eliminate cross-modality and cross-camera discrepancies. Experiments show that DMIC improves clustering accuracy and reliability, reducing the performance gap with supervised methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有方法在跨摄像头差异和身份过度分割方面的问题，提高无监督可见红外行人重识别的性能。提出的DMIC框架整合了MIE、DNC和HMCL，以消除跨模态和跨摄像头的差异。MIE在聚类层面融合了跨模态和跨摄像头的距离编码，DNC通过动态搜索策略优化网络目标，从提高可区分性转向增强跨模态和跨摄像头的一般性，而HMCL优化实例和聚类分布。实验表明，DMIC在性能上优于现有方法，并显著缩小了与监督方法的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2024-02-01T15:33:17+00:00 · Latest: 2024-12-04T03:55:35+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.00672v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.00672v4">PDF</a> · <a href="https://github.com/FranklinLingfeng/code_for_MULT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) endeavors to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency between the feature space and the pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to quantify the inconsistency between the pseudo-label space and the feature space, subsequently minimizing it. The proposed MULT ensures that the generated pseudo-labels maintain alignment across modalities while upholding structural consistency within intra-modality. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the side effects of noisy pseudo-labels while simultaneously aligning different modalities, coupled with an Alternative Modality-Invariant Representation Learning (AMIRL) framework. Experiments demonstrate that our proposed method outperforms existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. Code is available at https://github.com/FranklinLingfeng/code_for_MULT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索一致标签关联的同质性和异质性在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在在没有标注的情况下，从不同模态中检索同一身份的行人图像。尽管先前的工作集中在建立跨模态的伪标签关联以弥补模态差距，但它们忽略了在特征空间和伪标签空间之间保持实例级别的同质性和异质一致性，导致粗略的关联。为此，我们引入了一个统一模态标签转移（MULT）模块，该模块同时考虑了同质性和异质性的细粒度实例结构，从而产生高质量的跨模态标签关联。该模块利用同质性和异质性亲和力来量化伪标签空间与特征空间之间的不一致性，随后最小化这种不一致性。所提出的MULT确保生成的伪标签在不同模态之间保持对齐，同时在同模态内保持结构一致性。此外，我们还提出了一种简单的即插即用在线跨记忆标签精炼（OCLR）模块，以进一步减轻噪声伪标签的副作用，同时同时对齐不同模态，结合了一个交替模态不变表示学习（AMIRL）框架。实验表明，我们提出的方法优于现有的USL-VI-ReID方法，突显了我们MULT在与其他跨模态关联方法相比的优势。代码可在https://github.com/FranklinLingfeng/code_for_MULT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the coarse cross-modality pseudo-label associations. The study introduces a Modality-Unified Label Transfer (MULT) module that considers both homogeneous and heterogeneous instance-level structures, leading to high-quality label associations. Experimental results show that the proposed method outperforms existing state-of-the-art methods in USL-VI-ReID tasks, demonstrating the effectiveness of the MULT module in maintaining structural consistency within and across modalities. Additionally, an Online Cross-memory Label Refinement (OCLR) module and an Alternative Modality-Invariant Representation Learning (AMIRL) framework are proposed to further refine pseudo-labels and align different modalities.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决粗略的跨模态伪标签关联问题来提高无监督的可见光-红外行人再识别。研究引入了一个模态统一标签转移（MULT）模块，同时考虑了同质性和异质性实例级结构，从而生成高质量的标签关联。实验结果表明，所提出的方法在USL-VI-ReID任务中优于现有最先进的方法，证明了MULT模块在保持跨模态和模内结构一致性方面的有效性。此外，还提出了在线跨记忆标签精炼（OCLR）模块和替代模态不变表示学习（AMIRL）框架，以进一步细化伪标签并使不同模态对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement</div>
<div class="meta-line">Authors: De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T04:40:30+00:00 · Latest: 2024-11-03T13:50:53+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.12711v4">Abs</a> · <a href="https://arxiv.org/pdf/2305.12711v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under the assumption that the prediction or label distribution of each example should be similar to its nearest neighbors. Extensive experimental results on the public SYSU-MM01 and RegDB datasets demonstrate the effectiveness of the proposed method, surpassing existing state-of-the-art approach by a large margin of 7.76% mAP on average, which even surpasses some supervised VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作学习与邻域引导标签精炼的无监督可见-红外人员重识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外人员重识别（USL-VI-ReID）旨在从跨模态的未标记数据集中学习模态不变特征，这对于视频监控系统中的实际应用至关重要。解决USL-VI-ReID任务的关键在于解决跨模态数据关联问题，以便进一步进行异质联合学习。为了解决这一问题，我们提出了一种双最优传输标签分配（DOTLA）框架，以同时将一种模态生成的标签分配给其对应的模态。提出的DOTLA机制通过相互强化和高效的解决方案来解决跨模态数据关联问题，从而有效减少了某些不足和噪声标签关联的副作用。此外，我们还提出了一种跨模态邻域一致性引导的标签精炼和正则化模块，在假设每个示例的预测或标签分布应与其最近邻相似的前提下，消除不准确的监督信号带来的负面影响。在公开的SYSU-MM01和RegDB数据集上的广泛实验结果表明，所提出的方法具有有效性，平均mAP值比现有最先进的方法高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the unsupervised visible-infrared person re-identification (USL-VI-ReID) task by proposing a Dual Optimal Transport Label Assignment (DOTLA) framework to solve the cross-modality data association problem. The method also includes a cross-modality neighbor consistency guided label refinement and regularization module. Experiments on public datasets SYSU-MM01 and RegDB show that the proposed method outperforms existing state-of-the-art approaches by 7.76% in mean average precision (mAP).</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决跨模态数据关联问题来解决无监督的可见光-红外行人再识别（USL-VI-ReID）任务。作者提出了一个双最优传输标签分配（DOTLA）框架来在模态间分配标签，并提出了一种跨模态邻居一致性引导的标签精炼模块以提高标签准确性。实验结果表明，该方法在SYSU-MM01和RegDB数据集上的平均精度（mAP）上优于现有方法，提高了7.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T03:27:46+00:00 · Latest: 2024-11-03T13:48:21+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.12673v4">Abs</a> · <a href="https://arxiv.org/pdf/2305.12673v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to match pedestrian images of the same identity from different modalities without annotations. Existing works mainly focus on alleviating the modality gap by aligning instance-level features of the unlabeled samples. However, the relationships between cross-modality clusters are not well explored. To this end, we propose a novel bilateral cluster matching-based learning framework to reduce the modality gap by matching cross-modality clusters. Specifically, we design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm through optimizing the maximum matching problem in a bipartite graph. Then, the matched pairwise clusters utilize shared visible and infrared pseudo-labels during the model training. Under such a supervisory signal, a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework is proposed to align features jointly at a cluster-level. Meanwhile, the cross-modality Consistency Constraint (CC) is proposed to explicitly reduce the large modality discrepancy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the effectiveness of the proposed method, surpassing state-of-the-art approaches by a large margin of 8.76% mAP on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效双边跨模态簇匹配的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，匹配不同模态下同一身份的行人图像。现有工作主要集中在通过对齐未标注样本的实例级特征来缓解模态差异。然而，跨模态簇之间的关系尚未得到充分探索。为此，我们提出了一种新颖的双边簇匹配学习框架，通过匹配跨模态簇来减少模态差异。具体地，我们通过在二分图中优化最大匹配问题设计了一种多对多双边跨模态簇匹配（MBCCM）算法。然后，匹配的成对簇在模型训练过程中利用共享的可见光和红外伪标签。在这样的监督信号下，我们提出了一种模态特定和模态无关的对比学习框架，以在簇级联合对齐特征。同时，我们提出了跨模态一致性约束（CC）以显式地减少模态差异。在公开的SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法具有很高的有效性，平均mAP比最先进的方法高出8.76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by proposing a novel bilateral cluster matching framework. It introduces a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm to match cross-modality clusters and a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework to align features at a cluster-level. The cross-modality consistency constraint is also proposed to reduce modality discrepancy. Experiments on SYSU-MM01 and RegDB datasets show significant improvement over existing methods, with an average mAP increase of 8.76%.</div>
<div class="mono" style="margin-top:8px">论文提出了一种新颖的双边簇匹配框架，以解决无监督的可见光-红外行人再识别问题。该框架通过匹配跨模态簇来减少模态差距，使用了Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) 算法和Modality-Specific and Modality-Agnostic (MSMA) 对比学习框架。该方法取得了优异的效果，平均mAP提高了8.76%，超过了现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-02-29T10:37:49+00:00 · Latest: 2024-10-24T09:00:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.19026v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.19026v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotations, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on commonality, overlooking divergence and variety. To address the problem, we propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes method for USVI-ReID. In brief, we generate the hard prototype by selecting the sample with the maximum distance from the cluster center. We theoretically show that the hard prototype is used in the contrastive loss to emphasize divergence. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. The dynamic prototype is used to encourage the variety. Finally, we introduce a progressive learning strategy to gradually shift the model&#x27;s attention towards divergence and variety, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习共性、差异性和多样性以实现无监督可见光-红外人体重识别</div>
<div class="mono" style="margin-top:8px">无监督可见光-红外人体重识别（USVI-ReID）旨在无需标注的情况下，在红外图像中匹配指定的人并在可见图像中进行重识别，反之亦然。USVI-ReID 是一个具有挑战性但尚未充分探索的任务。现有大多数方法使用基于聚类的对比学习来解决 USVI-ReID，简单地将聚类中心作为人的表示。然而，聚类中心主要关注共性，忽视了差异性和多样性。为了解决这个问题，我们提出了一种渐进对比学习与硬动态原型方法以实现 USVI-ReID。简而言之，我们通过选择与聚类中心距离最大的样本生成硬原型。我们从理论上证明，硬原型用于对比损失中以强调差异性。此外，我们不是将查询图像严格对齐到特定的原型，而是通过在聚类内随机选择样本生成动态原型。动态原型用于鼓励多样性。最后，我们引入了一种渐进学习策略，逐步将模型的注意力转向差异性和多样性，避免聚类退化。在公开的 SYSU-MM01 和 RegDB 数据集上进行的大量实验验证了所提出方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID), focusing on matching individuals between visible and infrared images without annotations. To overcome the limitations of existing cluster-based methods that primarily capture commonality while neglecting divergence and variety, the authors propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes method. This method introduces hard prototypes to emphasize divergence and dynamic prototypes to encourage variety. Additionally, a progressive learning strategy is employed to shift the model&#x27;s focus towards these aspects. Experiments on SYSU-MM01 and RegDB datasets demonstrate the proposed method&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">论文提出了一种渐进对比学习与硬动态原型方法，用于解决无监督可见红外行人重识别（USVI-ReID）问题。该方法通过生成硬原型强调差异性，动态原型鼓励多样性，并采用渐进学习策略逐步将模型的关注点转向这些方面。在SYSU-MM01和RegDB数据集上的实验验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-01-12T01:24:04+00:00 · Latest: 2024-07-29T09:40:11+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.06825v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.06825v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a promising yet challenging retrieval task. The key challenges in USL-VI-ReID are to effectively generate pseudo-labels and establish pseudo-label correspondences across modalities without relying on any prior annotations. Recently, clustered pseudo-label methods have gained more attention in USL-VI-ReID. However, previous methods fell short of fully exploiting the individual nuances, as they simply utilized a single memory that represented an identity to establish cross-modality correspondences, resulting in ambiguous cross-modality correspondences. To address the problem, we propose a Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a Cross-Modality Clustering (CMC) module to generate the pseudo-labels through clustering together both two modality samples. To associate cross-modality clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM) module, ensuring that optimization explicitly focuses on the nuances of individual perspectives and establishes reliable cross-modality correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module to narrow the modality gap while mitigating the effect of noise pseudo-labels through a soft many-to-many alignment strategy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the reliability of the established cross-modality correspondences and the effectiveness of our MMM. The source codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多记忆匹配在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）是一项有前景但极具挑战性的检索任务。USL-VI-ReID 的关键挑战在于有效生成伪标签，并在不依赖任何先验注释的情况下在模态间建立伪标签对应关系。最近，聚类伪标签方法在 USL-VI-ReID 中获得了更多关注。然而，之前的方法未能充分利用个体差异，因为它们仅利用单一记忆来表示身份以建立跨模态对应关系，导致了模态间的模糊对应关系。为了解决这一问题，我们提出了一种多记忆匹配（MMM）框架用于 USL-VI-ReID。我们首先设计了一个跨模态聚类（CMC）模块，通过聚类两种模态样本来生成伪标签。为了关联跨模态聚类伪标签，我们设计了一个多记忆学习和匹配（MMLM）模块，确保优化明确关注个体视角的差异，并建立可靠的跨模态对应关系。最后，我们设计了一个软聚类级对齐（SCA）模块，通过软的多对多对齐策略缩小模态差距并减轻噪声伪标签的影响。在公开的 SYSU-MM01 和 RegDB 数据集上的广泛实验表明，所建立的跨模态对应关系的可靠性以及我们提出的 MMM 的有效性。源代码将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Multi-Memory Matching (MMM) framework. It introduces a Cross-Modality Clustering (CMC) module to generate pseudo-labels and a Multi-Memory Learning and Matching (MMLM) module to establish reliable cross-modality correspondences. Additionally, a Soft Cluster-level Alignment (SCA) module is designed to reduce the modality gap and mitigate noise in pseudo-labels. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the proposed method in establishing accurate cross-modality correspondences and improving re-identification performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多记忆匹配（MMM）框架来解决无监督可见红外行人再识别的挑战。它引入了跨模态聚类（CMC）模块生成伪标签，并设计了多记忆学习和匹配（MMLM）模块以建立可靠的跨模态对应关系。此外，还设计了软簇级对齐（SCA）模块以减少模态差距并减轻伪标签噪声的影响。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法在建立准确的跨模态对应关系方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="https://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a challenging retrieval task that aims to retrieve cross-modality pedestrian images without using any label information. In this task, the large cross-modality variance makes it difficult to generate reliable cross-modality labels, and the lack of annotations also provides additional difficulties for learning modality-invariant features. In this paper, we first deduce an optimization objective for unsupervised VI-ReID based on the mutual information between the model&#x27;s cross-modality input and output. With equivalent derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable cross-modality matching) are obtained. Under their guidance, we design a loop iterative training strategy alternating between model training and cross-modality matching. In the matching stage, a uniform prior guided optimal transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched visible and infrared prototypes. In the training stage, we utilize this matching information to introduce prototype-based contrastive learning for minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive experimental results on benchmarks demonstrate the effectiveness of our method, e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人重识别（USVI-ReID）是一项具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出一个无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一种交替进行模型训练和跨模态匹配的循环迭代训练策略。在匹配阶段，我们提出了一种均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化内模态和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如，在SYSU-MM01和RegDB上，无任何注释的情况下，Rank-1精度分别为60.6%和90.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: &#x27;Sharpness&#x27; (entropy minimization), &#x27;Fairness&#x27; (uniform label distribution), and &#x27;Fitness&#x27; (reliable cross-modality matching). The method employs a loop iterative training strategy that alternates between model training and cross-modality matching. Specifically, it uses a uniform prior guided optimal transport assignment to select matched visible and infrared prototypes in the matching stage, and introduces prototype-based contrastive learning in the training stage to minimize intra- and cross-modality entropy. The results show significant improvements, achieving 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB benchmarks without any annotations.</div>
<div class="mono" style="margin-top:8px">本文通过基于互信息的方法解决了无监督可见红外行人重识别的挑战，提出了三个学习原则：“Sharpness”用于最小化熵，“Fairness”用于均匀标签分布，“Fitness”用于可靠的跨模态匹配。方法采用交替进行模型训练和跨模态匹配的循环迭代训练策略，使用均匀先验引导的最优运输分配来选择匹配的原型，并通过原型为基础的对比学习来最小化熵。实验结果表明，在SYSU-MM01和RegDB基准上，该方法在无任何注释的情况下显著提高了Rank-1准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Xiangbo Yin, Jiangming Shi, Yachao Zhang, Yang Lu, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-05-09T08:17:06+00:00 · Latest: 2024-05-09T08:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.05613v1">Abs</a> · <a href="https://arxiv.org/pdf/2405.05613v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a formidable challenge, which aims to match pedestrian images across visible and infrared modalities without any annotations. Recently, clustered pseudo-label methods have become predominant in USVI-ReID, although the inherent noise in pseudo-labels presents a significant obstacle. Most existing works primarily focus on shielding the model from the harmful effects of noise, neglecting to calibrate noisy pseudo-labels usually associated with hard samples, which will compromise the robustness of the model. To address this issue, we design a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for USVI-ReID. To be specific, we first introduce a straightforward yet potent Noisy Pseudo-label Calibration module to correct noisy pseudo-labels. Due to the high intra-class variations, noisy pseudo-labels are difficult to calibrate completely. Therefore, we introduce a Neighbor Relation Learning module to reduce high intra-class variations by modeling potential interactions between all samples. Subsequently, we devise an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences. On that basis, we design a Memory Hybrid Learning module to jointly learn modality-specific and modality-invariant information. Comprehensive experiments conducted on two widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR outperforms the current state-of-the-art GUR with an average Rank-1 improvement of 10.3%. The source codes will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒的邻居关系伪标签学习方法在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）面临着巨大的挑战，其目标是在可见和红外模态之间匹配行人图像，而无需任何注释。最近，聚类伪标签方法在USVI-ReID中占据了主导地位，尽管伪标签中的固有噪声构成了一个重大障碍。现有的大多数工作主要集中在保护模型免受噪声的有害影响，而忽视了对通常与硬样本相关的噪声伪标签进行校准，这将损害模型的鲁棒性。为了解决这一问题，我们设计了一种鲁棒的邻居关系伪标签学习框架（RPNR）用于USVI-ReID。具体而言，我们首先引入了一个简单而有效的噪声伪标签校准模块来纠正噪声伪标签。由于类内变异性高，噪声伪标签难以完全校准。因此，我们引入了一个邻居关系学习模块来通过建模所有样本之间的潜在交互来降低高类内变异性。随后，我们设计了一个最优传输原型匹配模块来建立可靠的跨模态对应关系。在此基础上，我们设计了一个记忆混合学习模块来联合学习模态特定和模态不变信息。在两个广泛认可的基准数据集SYSU-MM01和RegDB上进行的全面实验表明，RPNR在平均Rank-1上比当前最先进的GUR提高了10.3%。源代码将很快发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework. It introduces a Noisy Pseudo-label Calibration module to correct noisy pseudo-labels and a Neighbor Relation Learning module to model interactions between samples, reducing intra-class variations. Additionally, it uses an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences and a Memory Hybrid Learning module to learn both modality-specific and invariant information. Experiments on SYSU-MM01 and RegDB benchmarks show that RPNR outperforms the current state-of-the-art method GUR by 10.3% in terms of Rank-1 accuracy.</div>
<div class="mono" style="margin-top:8px">论文提出了一种鲁棒的伪标签学习与邻域关系框架（RPNR），以解决无监督可见红外行人重识别（USVI-ReID）的挑战。该框架包括一个伪标签校准模块来纠正噪声标签、一个邻域关系学习模块来建模样本间的潜在交互关系，以及一个最优传输原型匹配模块来建立跨模态对应关系。在SYSU-MM01和RegDB基准上的实验表明，RPNR在Rank-1指标上比当前最先进的方法GUR提高了10.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment</div>
<div class="meta-line">Authors: Yexin Liu, Weiming Zhang, Athanasios V. Vasilakos, Lin Wang</div>
<div class="meta-line">First: 2024-04-10T02:03:14+00:00 · Latest: 2024-04-10T02:03:14+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.06683v1">Abs</a> · <a href="https://arxiv.org/pdf/2404.06683v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) has recently gained great attention due to its potential for enhancing human detection in diverse environments without labeling. Previous methods utilize intra-modality clustering and cross-modality feature matching to achieve UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be generated in the clustering process, and 2) the cross-modality feature alignment via matching the marginal distribution of visible and infrared modalities may misalign the different identities from two modalities. In this paper, we first conduct a theoretic analysis where an interpretable generalization upper bound is introduced. Based on the analysis, we then propose a novel unsupervised cross-modality person re-identification framework (PRAISE). Specifically, to address the first challenge, we propose a pseudo-label correction strategy that utilizes a Beta Mixture Model to predict the probability of mis-clustering based network&#x27;s memory effect and rectifies the correspondence by adding a perceptual term to contrastive learning. Next, we introduce a modality-level alignment strategy that generates paired visible-infrared latent features and reduces the modality gap by aligning the labeling function of visible and infrared features to learn identity discriminative and modality-invariant features. Experimental results on two benchmark datasets demonstrate that our method achieves state-of-the-art performance than the unsupervised visible-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于伪标签校正和模态级对齐的无监督可见光-红外ReID</div>
<div class="mono" style="margin-top:8px">无监督可见光-红外ReID（UVI-ReID）由于其在不同环境中增强人类检测方面的潜力而引起了广泛关注，无需标注。先前的方法利用同模态聚类和跨模态特征匹配来实现UVI-ReID。然而，存在两个挑战：1）聚类过程中可能会生成噪声伪标签；2）通过匹配可见光和红外模态的边缘分布来进行跨模态特征对齐可能会导致两个模态不同身份的对齐错误。在本文中，我们首先进行理论分析，引入了一个可解释的泛化上界。基于分析，我们提出了一种新颖的无监督跨模态ReID框架（PRAISE）。具体而言，为了解决第一个挑战，我们提出了一种伪标签校正策略，利用Beta混合模型预测基于网络记忆效应的误聚类概率，并通过在对比学习中添加感知项来纠正对应关系。接下来，我们引入了一种模态级对齐策略，生成可见光-红外配对的潜在特征，并通过对齐可见光和红外特征的标签函数来减少模态差距，从而学习身份判别性和模态不变性的特征。在两个基准数据集上的实验结果表明，我们的方法在无监督可见光-ReID方法中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a novel framework called PRAISE. It introduces a pseudo-label correction strategy using a Beta Mixture Model to rectify mis-clustered samples and a modality-level alignment strategy to align the labeling functions of visible and infrared features. The method achieves state-of-the-art performance on two benchmark datasets.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的无监督可见-红外行人再识别框架（PRAISE），包括伪标签校正策略和模态级对齐策略。伪标签校正使用Beta混合模型预测误聚类并进行修正，而模态级对齐生成配对的潜在特征以减少模态差距。实验结果显示，所提出的方法在两个基准数据集上的性能优于现有无监督可见-ReID方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251227_0323.html">20251227_0323</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0324.html">20251225_0324</a>
<a href="archive/20251224_0325.html">20251224_0325</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0322.html">20251222_0322</a>
<a href="archive/20251221_0323.html">20251221_0323</a>
<a href="archive/20251220_0324.html">20251220_0324</a>
<a href="archive/20251219_0324.html">20251219_0324</a>
<a href="archive/20251218_0333.html">20251218_0333</a>
<a href="archive/20251217_0326.html">20251217_0326</a>
<a href="archive/20251216_0327.html">20251216_0327</a>
<a href="archive/20251215_0325.html">20251215_0325</a>
<a href="archive/20251214_0323.html">20251214_0323</a>
<a href="archive/20251213_0324.html">20251213_0324</a>
<a href="archive/20251212_0326.html">20251212_0326</a>
<a href="archive/20251211_0325.html">20251211_0325</a>
<a href="archive/20251210_0321.html">20251210_0321</a>
<a href="archive/20251209_0324.html">20251209_0324</a>
<a href="archive/20251208_0325.html">20251208_0325</a>
<a href="archive/20251207_0325.html">20251207_0325</a>
<a href="archive/20251206_0327.html">20251206_0327</a>
<a href="archive/20251205_0328.html">20251205_0328</a>
<a href="archive/20251204_0328.html">20251204_0328</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0332.html">20251202_0332</a>
<a href="archive/20251201_0325.html">20251201_0325</a>
<a href="archive/20251130_0324.html">20251130_0324</a>
<a href="archive/20251129_0326.html">20251129_0326</a>
<a href="archive/20251128_0325.html">20251128_0325</a>
<a href="archive/20251127_0325.html">20251127_0325</a>
<a href="archive/20251126_0326.html">20251126_0326</a>
<a href="archive/20251125_0324.html">20251125_0324</a>
<a href="archive/20251124_0325.html">20251124_0325</a>
<a href="archive/20251123_0324.html">20251123_0324</a>
<a href="archive/20251122_0326.html">20251122_0326</a>
<a href="archive/20251121_0325.html">20251121_0325</a>
<a href="archive/20251120_0326.html">20251120_0326</a>
<a href="archive/20251119_0325.html">20251119_0325</a>
<a href="archive/20251118_0325.html">20251118_0325</a>
<a href="archive/20251117_0323.html">20251117_0323</a>
<a href="archive/20251116_0322.html">20251116_0322</a>
<a href="archive/20251115_0325.html">20251115_0325</a>
<a href="archive/20251114_0325.html">20251114_0325</a>
<a href="archive/20251113_0326.html">20251113_0326</a>
<a href="archive/20251112_0325.html">20251112_0325</a>
<a href="archive/20251111_0322.html">20251111_0322</a>
<a href="archive/20251110_0320.html">20251110_0320</a>
<a href="archive/20251109_0320.html">20251109_0320</a>
<a href="archive/20251108_0322.html">20251108_0322</a>
<a href="archive/20251107_0322.html">20251107_0322</a>
<a href="archive/20251106_0323.html">20251106_0323</a>
<a href="archive/20251105_0322.html">20251105_0322</a>
<a href="archive/20251104_0322.html">20251104_0322</a>
<a href="archive/20251103_0320.html">20251103_0320</a>
<a href="archive/20251102_0319.html">20251102_0319</a>
<a href="archive/20251101_0329.html">20251101_0329</a>
<a href="archive/20251031_0322.html">20251031_0322</a>
<a href="archive/20251030_0325.html">20251030_0325</a>
<a href="archive/20251029_0324.html">20251029_0324</a>
<a href="archive/20251028_1054.html">20251028_1054</a>
<a href="archive/20251028_0320.html">20251028_0320</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0325.html">20251026_0325</a>
<a href="archive/20251025_0327.html">20251025_0327</a>
<a href="archive/20251024_0327.html">20251024_0327</a>
<a href="archive/20251023_0327.html">20251023_0327</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0329.html">20251021_0329</a>
<a href="archive/20251020_0318.html">20251020_0318</a>
<a href="archive/20251019_0327.html">20251019_0327</a>
<a href="archive/20251018_0326.html">20251018_0326</a>
<a href="archive/20251017_0325.html">20251017_0325</a>
<a href="archive/20251016_0321.html">20251016_0321</a>
<a href="archive/20251015_0327.html">20251015_0327</a>
<a href="archive/20251014_0326.html">20251014_0326</a>
<a href="archive/20251012_0325.html">20251012_0325</a>
<a href="archive/20251011_0327.html">20251011_0327</a>
<a href="archive/20251010_0328.html">20251010_0328</a>
<a href="archive/20251009_0319.html">20251009_0319</a>
<a href="archive/20251008_0344.html">20251008_0344</a>
<a href="archive/20251007_0345.html">20251007_0345</a>
<a href="archive/20251006_0350.html">20251006_0350</a>
<a href="archive/20251005_0349.html">20251005_0349</a>
<a href="archive/20251004_0351.html">20251004_0351</a>
<a href="archive/20251003_0351.html">20251003_0351</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0340.html">20251001_0340</a>
<a href="archive/20250930_0357.html">20250930_0357</a>
<a href="archive/20250929_0354.html">20250929_0354</a>
<a href="archive/20250928_1405.html">20250928_1405</a>
<a href="archive/20250928_0338.html">20250928_0338</a>
<a href="archive/20250927_2233.html">20250927_2233</a>
<a href="archive/20250925_0328.html">20250925_0328</a>
<a href="archive/20250924_0337.html">20250924_0337</a>
<a href="archive/20250923_0336.html">20250923_0336</a>
<a href="archive/20250922_0334.html">20250922_0334</a>
<a href="archive/20250921_0333.html">20250921_0333</a>
<a href="archive/20250920_0334.html">20250920_0334</a>
<a href="archive/20250919_1904.html">20250919_1904</a>
<a href="archive/20250919_1023.html">20250919_1023</a>
<a href="archive/20250919_0332.html">20250919_0332</a>
<a href="archive/20250918_2047.html">20250918_2047</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250918_0326.html">20250918_0326</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250917_0326.html">20250917_0326</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250916_0328.html">20250916_0328</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
