<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-11 03:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250911_0319</div>
    <div class="row"><div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised   VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义对齐学习与协作精炼的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需人工注释的情况下，匹配不同模态下同一个体的行人图像。先前的方法通过标签关联算法统一跨模态图像的伪标签，然后设计对比学习框架进行全局特征学习。然而，这些方法忽略了由细粒度模式带来的特征表示和伪标签分布的跨模态变化。这种洞察导致仅优化全局特征时，模态共享学习不足。为解决这一问题，我们提出了一种语义对齐学习与协作精炼（SALCR）框架，该框架为每个模态强调的特定细粒度模式建立优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一种双向统一跨模态实例伪标签的双关联与全局学习（DAGI）模块。随后，执行了一种细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。然后基于语义对齐特征及其相应的标签空间，构建优化目标。为缓解来自噪声伪标签的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集，并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有方法。我们的代码可在https://github.com/FranklinLingfeng/code-for-SALCR 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="http://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a
challenging retrieval task that aims to retrieve cross-modality pedestrian
images without using any label information. In this task, the large
cross-modality variance makes it difficult to generate reliable cross-modality
labels, and the lack of annotations also provides additional difficulties for
learning modality-invariant features. In this paper, we first deduce an
optimization objective for unsupervised VI-ReID based on the mutual information
between the model&#x27;s cross-modality input and output. With equivalent
derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy
minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable
cross-modality matching) are obtained. Under their guidance, we design a loop
iterative training strategy alternating between model training and
cross-modality matching. In the matching stage, a uniform prior guided optimal
transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched
visible and infrared prototypes. In the training stage, we utilize this
matching information to introduce prototype-based contrastive learning for
minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive
experimental results on benchmarks demonstrate the effectiveness of our method,
e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any
annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人再识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人再识别（USVI-ReID）是一个具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出一个无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一种交替进行模型训练和跨模态匹配的循环训练策略。在匹配阶段，我们提出了一种均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见光和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化类内和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如在SYSU-MM01和RegDB上分别达到了60.6%和90.3%的Rank-1准确率，且没有任何注释。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: Sharpness, Fairness, and Fitness, and proposes a loop iterative training strategy that alternates between model training and cross-modality matching. The method uses a uniform prior guided optimal transport assignment to select matched visible and infrared prototypes and introduces prototype-based contrastive learning to minimize entropy. The experiments show significant improvements, achieving 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div>
<div class="mono" style="margin-top:8px">本文通过基于互信息的方法解决了无监督的可见光-红外行人重识别挑战，提出了三个学习原则：Sharpness（熵最小化）、Fairness（均匀标签分布）和Fitness（可靠的跨模态匹配）。方法采用交替进行模型训练和跨模态匹配的循环迭代训练策略。具体来说，在匹配阶段使用均匀先验引导的最优运输分配来选择匹配的可见光和红外原型。在训练阶段，通过原型对比学习来最小化类内和跨模态的熵。实验结果表明该方法的有效性，分别在SYSU-MM01和RegDB上达到了60.6%和90.3%的Rank-1准确率，且无需任何标注。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
