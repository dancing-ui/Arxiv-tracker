<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-07 03:31</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250907_0331</div>
    <div class="row"><div class="card">
<div class="title">L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved   Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Yan Jiang, Hao Yu, Mengting Wei, Zhaodong Sun, Haoyu Chen, Xu Cheng, Guoying Zhao</div>
<div class="meta-line">First: 2025-03-15T18:56:29+00:00 · Latest: 2025-08-28T14:32:17+00:00</div>
<div class="meta-line">Comments: Extended Version of L2RW. We extend it from image to video data</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.12232v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.12232v2">PDF</a> · <a href="https://github.com/Joey623/L2RW">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) is a challenging task
that aims to match pedestrian images captured under varying lighting
conditions, which has drawn intensive research attention and achieved promising
results. However, existing methods adopt the centralized training, ignoring the
potential privacy concerns as the data is distributed across multiple devices
or entities in reality. In this paper, we propose L2RW+, a benchmark that
brings VI-ReID closer to real-world applications. The core rationale behind
L2RW+ is that incorporating decentralized training into VI-ReID can address
privacy concerns in scenarios with limited data-sharing constrains.
Specifically, we design protocols and corresponding algorithms for different
privacy sensitivity levels. In our new benchmark, we simulate the training
under real-world data conditions that: 1) data from each camera is completely
isolated, or 2) different data entities (e.g., data controllers of a certain
region) can selectively share the data. In this way, we simulate scenarios with
strict privacy restrictions, which is closer to real-world conditions.
Comprehensive experiments show the feasibility and potential of decentralized
VI-ReID training at both image and video levels. In particular, with increasing
data scales, the performance gap between decentralized and centralized training
decreases, especially in video-level VI-ReID. In unseen domains, decentralized
training even achieves performance comparable to SOTA centralized methods. This
work offers a novel research entry for deploying VI-ReID into real-world
scenarios and can benefit the community. Code is available at:
https://github.com/Joey623/L2RW.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>L2RW+: 针对隐私保护的可见光-红外行人重识别综合基准</div>
<div class="mono" style="margin-top:8px">可见光-红外行人重识别（VI-ReID）是一项具有挑战性的任务，旨在在不同光照条件下匹配行人图像，已引起广泛关注并取得显著成果。然而，现有方法采用集中式训练，忽略了实际中数据分布在多个设备或实体之间的潜在隐私问题。本文提出L2RW+基准，旨在使VI-ReID更接近实际应用。L2RW+的核心理念是，在数据共享受限的场景中，将分散式训练纳入VI-ReID可以解决隐私问题。具体地，我们为不同隐私敏感度级别设计了协议和相应算法。在我们的新基准中，我们模拟了在实际数据条件下进行训练的情况：1）每个摄像头的数据完全隔离，或2）不同的数据实体（例如，某一地区的数据控制者）可以选择性地共享数据。这样，我们模拟了具有严格隐私限制的场景，更接近实际条件。全面的实验表明，分散式VI-ReID训练在图像和视频级别上具有可行性和潜力。特别是，随着数据规模的增加，分散式和集中式训练之间的性能差距减小，尤其是在视频级别的VI-ReID中。在未见领域，分散式训练甚至达到了与最新集中式方法相当的性能。这项工作为部署VI-ReID到实际场景提供了新的研究切入点，并可惠及社区。代码可在：https://github.com/Joey623/L2RW 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address privacy concerns in visible-infrared person re-identification (VI-ReID) by proposing L2RW+, which incorporates decentralized training. The study designs protocols for different privacy sensitivity levels and simulates real-world data conditions. Experimental results show that decentralized training can achieve performance comparable to centralized methods, especially in video-level VI-ReID, and closes the performance gap with increasing data scales. This work provides a new approach for deploying VI-ReID in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">论文提出了L2RW+基准，通过引入去中心化训练来解决可见红外行人重识别（VI-ReID）中的隐私问题。研究显示，去中心化训练可以在视频级别VI-ReID中达到与中心化训练相当甚至更好的性能，并且在未见过的领域中甚至超过了最先进的中心化方法。全面的实验验证了去中心化VI-ReID训练在真实数据条件下的可行性和潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Spectral Body Recognition with Side Information Embedding:   Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF</div>
<div class="meta-line">Authors: Anirudh Nanduri, Siyuan Huang, Rama Chellappa</div>
<div class="meta-line">First: 2025-06-10T16:20:52+00:00 · Latest: 2025-06-10T16:20:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.08953v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.08953v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨光谱人体识别中的侧信息嵌入：LLCM基准及IJB-MDF上的范围诱导遮挡分析</div>
<div class="mono" style="margin-top:8px">视觉变换器（ViTs）在生物特征识别任务中表现出色，包括面部和人体识别。在本工作中，我们将预训练在可见光（VIS）图像上的ViT模型应用于具有挑战性的跨光谱人体识别问题，该问题涉及可见光和红外（IR）域中捕获的图像匹配。最近的ViT架构探索了引入超出传统位置嵌入的其他嵌入。在此基础上，我们整合了侧信息嵌入（SIE），并研究了编码领域和相机信息以增强跨光谱匹配的影响。令人惊讶的是，我们的结果表明，仅编码相机信息而不显式地引入领域信息，在LLCM数据集上实现了最先进的性能。虽然在可见光谱的人再识别（Re-ID）中已经广泛研究了遮挡处理，但在可见光-红外（VI）Re-ID中的遮挡处理仍然很大程度上被忽视——主要是因为现有的VI-ReID数据集，如LLCM、SYSU-MM01和RegDB，主要包含全身、未遮挡的图像。为了解决这一差距，我们使用IARPA Janus基准多域面部（IJB-MDF）数据集分析了范围诱导遮挡的影响，该数据集提供了在不同距离捕获的多样化的可见光和红外图像，使我们能够进行跨范围、跨光谱评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores the use of Vision Transformers (ViTs) for cross-spectral body recognition, adapting a ViT pretrained on visible imagery to match visible and infrared images. By integrating Side Information Embedding (SIE), the authors examine the impact of encoding domain and camera information. Surprisingly, encoding only camera information without domain information achieves state-of-the-art performance on the LLCM dataset. Additionally, the study analyzes range-induced occlusions using the IJB-MDF dataset, providing insights into visible-infrared re-identification challenges.</div>
<div class="mono" style="margin-top:8px">该研究探讨了使用Vision Transformers (ViTs)进行跨光谱人体识别的方法，将预训练在可见光图像上的ViT应用于匹配可见光和红外图像。通过集成Side Information Embedding (SIE)，作者研究了编码领域和相机信息的影响。令人惊讶的是，仅编码相机信息而不编码领域信息在LLCM数据集上达到了最先进的性能。此外，该研究使用IJB-MDF数据集分析了距离引起的遮挡问题，提供了可见光-红外重识别挑战的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised   VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作精炼的语义对齐学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需人类注释的情况下，匹配不同模态下同一个体的行人图像。先前的方法通过标签关联算法统一跨模态图像的伪标签，然后设计对比学习框架进行全局特征学习。然而，这些方法忽略了由细粒度模式带来的特征表示和伪标签分布的跨模态变化。这种洞察导致仅优化全局特征时，模态间共享学习不足。为解决这一问题，我们提出了一种语义对齐学习与协作精炼（SALCR）框架，该框架为每个模态强调的特定细粒度模式构建优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一种双向全局学习关联（DAGI）模块，以双向方式统一跨模态实例的伪标签。随后，执行细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。基于语义对齐特征及其相应的标签空间，构建优化目标。为缓解来自噪声伪标签的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集，并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有方法。我们的代码可在https://github.com/FranklinLingfeng/code-for-SALCR 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Diverse Semantics-Guided Feature Alignment and Decoupling for   Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Neng Dong, Shuanglin Yan, Liyan Zhang, Jinhui Tang</div>
<div class="meta-line">First: 2025-05-01T15:55:38+00:00 · Latest: 2025-05-01T15:55:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.00619v1">Abs</a> · <a href="http://arxiv.org/pdf/2505.00619v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due
to the large modality discrepancy between visible and infrared images, which
complicates the alignment of their features into a suitable common space.
Moreover, style noise, such as illumination and color contrast, reduces the
identity discriminability and modality invariance of features. To address these
challenges, we propose a novel Diverse Semantics-guided Feature Alignment and
Decoupling (DSFAD) network to align identity-relevant features from different
modalities into a textual embedding space and disentangle identity-irrelevant
features within each modality. Specifically, we develop a Diverse
Semantics-guided Feature Alignment (DSFA) module, which generates pedestrian
descriptions with diverse sentence structures to guide the cross-modality
alignment of visual features. Furthermore, to filter out style information, we
propose a Semantic Margin-guided Feature Decoupling (SMFD) module, which
decomposes visual features into pedestrian-related and style-related
components, and then constrains the similarity between the former and the
textual embeddings to be at least a margin higher than that between the latter
and the textual embeddings. Additionally, to prevent the loss of pedestrian
semantics during feature decoupling, we design a Semantic Consistency-guided
Feature Restitution (SCFR) module, which further excavates useful information
for identification from the style-related features and restores it back into
the pedestrian-related features, and then constrains the similarity between the
features after restitution and the textual embeddings to be consistent with
that between the features before decoupling and the textual embeddings.
Extensive experiments on three VI-ReID datasets demonstrate the superiority of
our DSFAD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态语义引导特征对齐与解耦合在可见光-红外人体重识别中的应用</div>
<div class="mono" style="margin-top:8px">可见光-红外人体重识别（VI-ReID）由于可见光和红外图像之间存在较大的模态差异而成为一个具有挑战性的任务，这使得特征难以对齐到一个合适的共同空间。此外，诸如光照和色彩对比度等风格噪声降低了特征的身份可区分性和模态不变性。为了解决这些挑战，我们提出了一种新颖的多模态语义引导特征对齐与解耦合（DSFAD）网络，将不同模态的身份相关特征对齐到文本嵌入空间，并在每个模态内解耦身份无关特征。具体而言，我们开发了一种多模态语义引导特征对齐（DSFA）模块，该模块生成具有多种句法结构的行人描述，以引导跨模态视觉特征的对齐。此外，为了过滤掉风格信息，我们提出了一种语义边界引导特征解耦（SMFD）模块，该模块将视觉特征分解为行人相关和风格相关组件，然后约束前者与文本嵌入之间的相似度至少比后者与文本嵌入之间的相似度高出一个边界。此外，为了防止在特征解耦过程中丢失行人语义，我们设计了一种语义一致性引导特征恢复（SCFR）模块，该模块进一步从风格相关特征中挖掘出有助于识别的有用信息，并将其恢复回行人相关特征，然后约束恢复后的特征与文本嵌入之间的相似度与解耦前的特征与文本嵌入之间的相似度保持一致。在三个VI-ReID数据集上的广泛实验表明了我们DSFAD的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges in Visible-Infrared Person Re-Identification (VI-ReID) by proposing a DSFAD network. This network includes a DSFA module for cross-modality feature alignment and an SMFD module for feature decoupling to filter out style information. Additionally, a SCFR module is designed to maintain pedestrian semantics during decoupling. Experiments on three VI-ReID datasets show the effectiveness of the proposed method.</div>
<div class="mono" style="margin-top:8px">论文提出DSFAD网络以解决可见光-红外人像再识别的挑战。该网络包括DSFA模块进行跨模态特征对齐到文本嵌入空间，以及SMFD模块进行身份相关和身份无关特征的解耦。SCFR模块进一步通过从风格相关特征中提取有用信息并恢复到身份相关特征中，增强身份相关特征。实验结果显示该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Shape-centered Representation Learning for Visible-Infrared Person   Re-identification</div>
<div class="meta-line">Authors: Shuang Li, Jiaxu Leng, Ji Gan, Mengjingcheng Mo, Xinbo Gao</div>
<div class="meta-line">First: 2023-10-27T07:57:24+00:00 · Latest: 2025-04-28T03:40:54+00:00</div>
<div class="meta-line">Comments: Accepted for Pattern Recognition. Code:
  https://github.com/Visuang/ScRL</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2310.17952v3">Abs</a> · <a href="http://arxiv.org/pdf/2310.17952v3">PDF</a> · <a href="https://github.com/Visuang/ScRL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-Infrared Person Re-Identification (VI-ReID) plays a critical role in
all-day surveillance systems. However, existing methods primarily focus on
learning appearance features while overlooking body shape features, which not
only complement appearance features but also exhibit inherent robustness to
modality variations. Despite their potential, effectively integrating shape and
appearance features remains challenging. Appearance features are highly
susceptible to modality variations and background noise, while shape features
often suffer from inaccurate infrared shape estimation due to the limitations
of auxiliary models. To address these challenges, we propose the Shape-centered
Representation Learning (ScRL) framework, which enhances VI-ReID performance by
innovatively integrating shape and appearance features. Specifically, we
introduce Infrared Shape Restoration (ISR) to restore inaccuracies in infrared
body shape representations at the feature level by leveraging infrared
appearance features. In addition, we propose Shape Feature Propagation (SFP),
which enables the direct extraction of shape features from original images
during inference with minimal computational complexity. Furthermore, we design
Appearance Feature Enhancement (AFE), which utilizes shape features to
emphasize shape-related appearance features while effectively suppressing
identity-unrelated noise. Benefiting from the effective integration of shape
and appearance features, ScRL demonstrates superior performance through
extensive experiments. On the SYSU-MM01, HITSZ-VCM, and RegDB datasets, it
achieves Rank-1 (mAP) accuracies of 76.1% (72.6%), 71.2% (52.9%), and 92.4%
(86.7%), respectively, surpassing existing state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于形状中心表示学习的可见光-红外人体重识别</div>
<div class="mono" style="margin-top:8px">可见光-红外人体重识别（VI-ReID）在全天候监控系统中起着关键作用。然而，现有方法主要关注学习外观特征，而忽视了身体形状特征，这些特征不仅补充了外观特征，还表现出对模态变化的固有鲁棒性。尽管具有潜力，但有效整合形状和外观特征仍然具有挑战性。外观特征对模态变化和背景噪声高度敏感，而形状特征则常常由于辅助模型的限制而产生不准确的红外形状估计。为了解决这些挑战，我们提出了形状中心表示学习（ScRL）框架，通过创新地整合形状和外观特征来提升VI-ReID性能。具体而言，我们引入了红外形状恢复（ISR），通过利用红外外观特征在特征级别恢复红外身体形状表示的不准确性。此外，我们提出了形状特征传播（SFP），在推理过程中能够直接从原始图像中提取形状特征，且计算复杂度较低。进一步地，我们设计了外观特征增强（AFE），利用形状特征来强调与形状相关的外观特征，同时有效抑制与身份无关的噪声。得益于形状和外观特征的有效整合，ScRL在大量实验中表现出优越的性能。在SYSU-MM01、HITSZ-VCM和RegDB数据集上，其Rank-1（mAP）准确率分别为76.1%（72.6%）、71.2%（52.9%）和92.4%（86.7%），超越了现有最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of Visible-Infrared Person Re-Identification (VI-ReID) by proposing the Shape-centered Representation Learning (ScRL) framework. This framework integrates shape and appearance features to enhance performance, overcoming the limitations of each feature type. Key components include Infrared Shape Restoration (ISR) for feature-level shape restoration, Shape Feature Propagation (SFP) for direct shape feature extraction, and Appearance Feature Enhancement (AFE) for emphasizing shape-related appearance features. Extensive experiments show that ScRL outperforms existing methods, achieving high Rank-1 (mAP) accuracies on various datasets.</div>
<div class="mono" style="margin-top:8px">论文提出了基于形状的表示学习（ScRL）框架，以解决可见光-红外人再识别（VI-ReID）的挑战。ScRL通过整合形状和外观特征来提升性能，克服了每种特征类型的局限性。关键组件包括红外形状恢复（ISR）用于恢复形状特征，形状特征传播（SFP）用于直接从原始图像中提取形状特征，以及外观特征增强（AFE）利用形状信息改进外观特征。广泛的实验表明，ScRL在SYSU-MM01、HITSZ-VCM和RegDB数据集上超越了现有方法，取得了较高的Rank-1（mAP）准确率。</div>
</details>
</div>
<div class="card">
<div class="title">MSCMNet: Multi-scale Semantic Correlation Mining for Visible-Infrared   Person Re-Identification</div>
<div class="meta-line">Authors: Xuecheng Hua, Ke Cheng, Hu Lu, Juanjuan Tu, Yuanquan Wang, Shitong Wang</div>
<div class="meta-line">Venue: Pattern Recognition 159, 111090 (2025), ISSN: 0031-3203</div>
<div class="meta-line">First: 2023-11-24T10:23:57+00:00 · Latest: 2025-04-01T13:34:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2311.14395v2">Abs</a> · <a href="http://arxiv.org/pdf/2311.14395v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The main challenge in the Visible-Infrared Person Re-Identification (VI-ReID)
task lies in how to extract discriminative features from different modalities
for matching purposes. While the existing well works primarily focus on
minimizing the modal discrepancies, the modality information can not thoroughly
be leveraged. To solve this problem, a Multi-scale Semantic Correlation Mining
network (MSCMNet) is proposed to comprehensively exploit semantic features at
multiple scales and simultaneously reduce modality information loss as small as
possible in feature extraction. The proposed network contains three novel
components. Firstly, after taking into account the effective utilization of
modality information, the Multi-scale Information Correlation Mining Block
(MIMB) is designed to explore semantic correlations across multiple scales.
Secondly, in order to enrich the semantic information that MIMB can utilize, a
quadruple-stream feature extractor (QFE) with non-shared parameters is
specifically designed to extract information from different dimensions of the
dataset. Finally, the Quadruple Center Triplet Loss (QCT) is further proposed
to address the information discrepancy in the comprehensive features. Extensive
experiments on the SYSU-MM01, RegDB, and LLCM datasets demonstrate that the
proposed MSCMNet achieves the greatest accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MSCMNet：多尺度语义相关性挖掘在可见光-红外人体重识别中的应用</div>
<div class="mono" style="margin-top:8px">可见光-红外人体重识别（VI-ReID）任务的主要挑战在于如何从不同模态中提取具有区分性的特征以供匹配使用。现有研究主要集中在减少模态差异上，但未能充分利用模态信息。为了解决这一问题，提出了一种多尺度语义相关性挖掘网络（MSCMNet），该网络旨在全面利用多尺度语义特征，并尽可能减少特征提取中的模态信息损失。所提出的网络包含三个新颖组件。首先，在考虑模态信息的有效利用后，设计了多尺度信息相关性挖掘块（MIMB），以探索多尺度下的语义相关性。其次，为了丰富MIMB可以利用的语义信息，专门设计了一个具有非共享参数的四流特征提取器（QFE），以从数据的不同维度中提取信息。最后，提出了四中心三重损失（QCT），以解决综合特征中的信息差异。在SYSU-MM01、RegDB和LLCM数据集上的广泛实验表明，所提出的MSCMNet实现了最高的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the accuracy of Visible-Infrared Person Re-Identification by addressing the challenge of extracting discriminative features from different modalities. The proposed MSCMNet utilizes a Multi-scale Information Correlation Mining Block, a quadruple-stream feature extractor, and a Quadruple Center Triplet Loss to comprehensively exploit semantic features across multiple scales and minimize modality information loss. Experimental results on SYSU-MM01, RegDB, and LLCM datasets show that MSCMNet achieves the highest accuracy compared to existing methods.</div>
<div class="mono" style="margin-top:8px">研究旨在解决从可见光和红外模态中提取具有区分性的特征以进行行人重识别的挑战。提出的MSCMNet引入了多尺度信息相关性挖掘模块、四流特征提取器和四中心三重损失，以全面利用多尺度的语义特征并最小化模态信息损失。在SYSU-MM01、RegDB和LLCM数据集上的实验结果表明，MSCMNet在现有方法中取得了最高的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion-based Synthetic Data Generation for Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Wenbo Dai, Lijing Lu, Zhihang Li</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2025-03-16T11:54:37+00:00 · Latest: 2025-03-16T11:54:37+00:00</div>
<div class="meta-line">Comments: AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.12472v1">Abs</a> · <a href="http://arxiv.org/pdf/2503.12472v1">PDF</a> · <a href="https://github.com/BorgDiven/DiVE">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of models is intricately linked to the abundance of training
data. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting
and annotating large-scale images of each individual under various cameras and
modalities is tedious, time-expensive, costly and must comply with data
protection laws, posing a severe challenge in meeting dataset requirements.
Current research investigates the generation of synthetic data as an efficient
and privacy-ensuring alternative to collecting real data in the field. However,
a specific data synthesis technique tailored for VI-ReID models has yet to be
explored. In this paper, we present a novel data generation framework, dubbed
Diffusion-based VI-ReID data Expansion (DiVE), that automatically obtain
massive RGB-IR paired images with identity preserving by decoupling identity
and modality to improve the performance of VI-ReID models. Specifically,
identity representation is acquired from a set of samples sharing the same ID,
whereas the modality of images is learned by fine-tuning the Stable Diffusion
(SD) on modality-specific data. DiVE extend the text-driven image synthesis to
identity-preserving RGB-IR multimodal image synthesis. This approach
significantly reduces data collection and annotation costs by directly
incorporating synthetic data into ReID model training. Experiments have
demonstrated that VI-ReID models trained on synthetic data produced by DiVE
consistently exhibit notable enhancements. In particular, the state-of-the-art
method, CAJ, trained with synthetic images, achieves an improvement of about
$9\%$ in mAP over the baseline on the LLCM dataset. Code:
https://github.com/BorgDiven/DiVE</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散的可见-红外行人再识别合成数据生成</div>
<div class="mono" style="margin-top:8px">模型的性能与训练数据的丰富程度密切相关。在可见-红外行人再识别（VI-ReID）任务中，收集和标注不同摄像头和模态下每个个体的大规模图像既繁琐又耗时且成本高昂，且必须遵守数据保护法律，这给数据集需求的满足带来了严重挑战。当前研究正在探索生成合成数据作为收集真实数据的高效且隐私保护的替代方案。然而，针对VI-ReID模型的特定数据合成技术尚未被研究。本文提出了一种名为基于扩散的VI-ReID数据扩展（DiVE）的新数据生成框架，通过解耦身份和模态，自动获取大量保持身份一致的RGB-IR配对图像，以提高VI-ReID模型的性能。具体而言，身份表示是从具有相同ID的一组样本中获得的，而图像的模态则是通过在模态特定数据上微调稳定扩散（SD）来学习的。DiVE将文本驱动的图像合成扩展到保持身份的RGB-IR多模态图像合成。这种方法通过直接将合成数据纳入再识别模型训练中，显著降低了数据收集和标注的成本。实验表明，使用DiVE生成的合成数据训练的VI-ReID模型表现出显著的提升。特别是，最先进的方法CAJ在LLCM数据集上使用合成图像训练时，mAP提高了约9%。代码：https://github.com/BorgDiven/DiVE</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of collecting sufficient training data for Visible-Infrared person Re-IDentification (VI-ReID) tasks by proposing a novel data generation framework called Diffusion-based VI-ReID data Expansion (DiVE). DiVE decouples identity and modality to generate massive RGB-IR paired images with preserved identity, improving VI-ReID model performance. Experiments show that VI-ReID models trained with DiVE-generated synthetic data outperform the baseline, with the CAJ method achieving a 9% improvement in mAP on the LLCM dataset.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为DiVE的新数据生成框架，以解决可见红外人再识别任务中收集足够训练数据的挑战。DiVE通过扩散方法将身份和模态解耦，生成大量具有身份保留的RGB-IR配对图像。实验表明，使用DiVE生成的合成数据训练的模型在LLCM数据集上的mAP指标比基线方法提高了约9%。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Domain Biometric Recognition using Body Embeddings</div>
<div class="meta-line">Authors: Anirudh Nanduri, Siyuan Huang, Rama Chellappa</div>
<div class="meta-line">First: 2025-03-13T22:38:18+00:00 · Latest: 2025-03-13T22:38:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.10931v1">Abs</a> · <a href="http://arxiv.org/pdf/2503.10931v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Biometric recognition becomes increasingly challenging as we move away from
the visible spectrum to infrared imagery, where domain discrepancies
significantly impact identification performance. In this paper, we show that
body embeddings perform better than face embeddings for cross-spectral person
identification in medium-wave infrared (MWIR) and long-wave infrared (LWIR)
domains. Due to the lack of multi-domain datasets, previous research on
cross-spectral body identification - also known as Visible-Infrared Person
Re-Identification (VI-ReID) - has primarily focused on individual infrared
bands, such as near-infrared (NIR) or LWIR, separately. We address the
multi-domain body recognition problem using the IARPA Janus Benchmark
Multi-Domain Face (IJB-MDF) dataset, which enables matching of short-wave
infrared (SWIR), MWIR, and LWIR images against RGB (VIS) images. We leverage a
vision transformer architecture to establish benchmark results on the IJB-MDF
dataset and, through extensive experiments, provide valuable insights into the
interrelation of infrared domains, the adaptability of VIS-pretrained models,
the role of local semantic features in body-embeddings, and effective training
strategies for small datasets. Additionally, we show that finetuning a body
model, pretrained exclusively on VIS data, with a simple combination of
cross-entropy and triplet losses achieves state-of-the-art mAP scores on the
LLCM dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用身体嵌入的多域生物识别</div>
<div class="mono" style="margin-top:8px">随着我们从可见光谱转向红外成像，生物识别变得越来越具有挑战性，其中域差异显著影响识别性能。在本文中，我们展示了在中波红外（MWIR）和长波红外（LWIR）域中，身体嵌入比面部嵌入更适合跨光谱的人体识别。由于缺乏多域数据集，之前关于跨光谱身体识别的研究——也称为可见-红外人体重识别（VI-ReID）——主要集中在单独的红外波段，如近红外（NIR）或LWIR。我们使用IARPA Janus基准多域面部（IJB-MDF）数据集来解决多域身体识别问题，该数据集允许将短波红外（SWIR）、MWIR和LWIR图像与RGB（VIS）图像进行匹配。我们利用视觉变换器架构在IJB-MDF数据集上建立基准结果，并通过大量实验提供了有关红外域之间的相互关系、VIS预训练模型的适应性、身体嵌入中局部语义特征的作用以及小数据集的有效训练策略的宝贵见解。此外，我们展示了仅使用可见光数据预训练的身体模型，通过简单的交叉熵和三重损失组合进行微调，可以在LLCM数据集上达到最先进的mAP分数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of biometric recognition in infrared imagery by leveraging body embeddings, which outperform face embeddings for cross-spectral person identification in MWIR and LWIR domains. The authors use the IJB-MDF dataset to explore multi-domain body recognition, employing a vision transformer to achieve benchmark results. Key findings include the importance of local semantic features and the effectiveness of finetuning a VIS-pretrained model with simple loss functions for small datasets.</div>
<div class="mono" style="margin-top:8px">本文通过使用身体嵌入解决了不同红外域的生物识别挑战，发现在MWIR和LWIR中的跨光谱人识别中，身体嵌入优于面部嵌入。作者利用IJB-MDF数据集探索了红外域之间的相互关系以及VIS预训练模型的适应性。研究发现，仅使用交叉熵和三重损失对仅在VIS数据上预训练的身体模型进行微调，可以在LLCM数据集上达到最先进的mAP分数。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Illumination-Invariant Synergistic Feature Integration in a   Stratified Granular Framework for Visible-Infrared Re-Identification</div>
<div class="meta-line">Authors: Yuheng Jia, Wesley Armour</div>
<div class="meta-line">First: 2025-02-28T15:42:58+00:00 · Latest: 2025-02-28T15:42:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.21163v1">Abs</a> · <a href="http://arxiv.org/pdf/2502.21163v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-Infrared Person Re-Identification (VI-ReID) plays a crucial role in
applications such as search and rescue, infrastructure protection, and
nighttime surveillance. However, it faces significant challenges due to
modality discrepancies, varying illumination, and frequent occlusions. To
overcome these obstacles, we propose \textbf{AMINet}, an Adaptive Modality
Interaction Network. AMINet employs multi-granularity feature extraction to
capture comprehensive identity attributes from both full-body and upper-body
images, improving robustness against occlusions and background clutter. The
model integrates an interactive feature fusion strategy for deep intra-modal
and cross-modal alignment, enhancing generalization and effectively bridging
the RGB-IR modality gap. Furthermore, AMINet utilizes phase congruency for
robust, illumination-invariant feature extraction and incorporates an adaptive
multi-scale kernel MMD to align feature distributions across varying scales.
Extensive experiments on benchmark datasets demonstrate the effectiveness of
our approach, achieving a Rank-1 accuracy of $74.75\%$ on SYSU-MM01, surpassing
the baseline by $7.93\%$ and outperforming the current state-of-the-art by
$3.95\%$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分层粒度框架中的自适应照明不变协同特征整合以实现可见光-红外重识别</div>
<div class="mono" style="margin-top:8px">可见光-红外人员重识别（VI-ReID）在搜索和救援、基础设施保护和夜间监控等应用中发挥着重要作用。然而，由于模态差异、光照变化和频繁遮挡，它面临着重大挑战。为克服这些障碍，我们提出了一种自适应模态交互网络AMINet。AMINet采用多粒度特征提取，从全身和上半身图像中捕获全面的身份属性，提高对遮挡和背景杂乱的鲁棒性。该模型结合了交互特征融合策略，实现深层次的模内和跨模态对齐，增强泛化能力并有效弥合RGB-IR模态差距。此外，AMINet利用相位一致性的鲁棒、光照不变特征提取，并结合自适应多尺度核MMD对齐不同尺度下的特征分布。在基准数据集上的广泛实验表明，我们的方法有效，SYSU-MM01上的Rank-1准确率为74.75%，比基线高出7.93%，并优于当前最先进的方法3.95%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of Visible-Infrared Person Re-Identification (VI-ReID) by proposing AMINet, which uses multi-granularity feature extraction and adaptive multi-scale kernel MMD to improve robustness against occlusions and illumination variations. AMINet achieves a Rank-1 accuracy of 74.75% on SYSU-MM01, outperforming the current state-of-the-art by 3.95%.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出AMINet，利用多粒度特征提取和自适应多尺度核MMD来解决可见光-红外人再识别（VI-ReID）中的遮挡和光照变化问题。该模型在SYSU-MM01数据集上达到74.75%的Rank-1准确率，显著超越了基线和当前最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</div>
<div class="meta-line">Authors: Mahdi Alehdaghi, Rajarshi Bhattacharya, Pourya Shamsolmoali, Rafael M. O. Cruz, Eric Granger</div>
<div class="meta-line">First: 2025-01-23T01:28:05+00:00 · Latest: 2025-01-23T01:28:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.13307v1">Abs</a> · <a href="http://arxiv.org/pdf/2501.13307v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) aims to match individuals
across different camera modalities, a critical task in modern surveillance
systems. While current VI-ReID methods focus on cross-modality matching,
real-world applications often involve mixed galleries containing both V and I
images, where state-of-the-art methods show significant performance limitations
due to large domain shifts and low discrimination across mixed modalities. This
is because gallery images from the same modality may have lower domain gaps but
correspond to different identities. This paper introduces a novel mixed-modal
ReID setting, where galleries contain data from both modalities. To address the
domain shift among inter-modal and low discrimination capacity in intra-modal
matching, we propose the Mixed Modality-Erased and -Related (MixER) method. The
MixER learning approach disentangles modality-specific and modality-shared
identity information through orthogonal decomposition, modality-confusion, and
ID-modality-related objectives. MixER enhances feature robustness across
modalities, improving cross-modal and mixed-modal settings performance. Our
extensive experiments on the SYSU-MM01, RegDB and LLMC datasets indicate that
our approach can provide state-of-the-art results using a single backbone, and
showcase the flexibility of our approach in mixed gallery applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从跨模态到混合模态可见红外再识别</div>
<div class="mono" style="margin-top:8px">可见红外人员再识别（VI-ReID）旨在跨不同摄像模态匹配个体，是现代监控系统中的关键任务。当前的VI-ReID方法主要关注跨模态匹配，但在实际应用中，混合画廊中通常包含可见光和红外图像，最先进的方法在混合模态下由于领域偏移大和低区分度表现出显著的性能限制。这是因为同一模态的画廊图像可能具有较小的领域差距，但对应不同的身份。本文引入了一种新的混合模态再识别设置，其中画廊包含两种模态的数据。为了解决跨模态领域偏移和同模态低区分度的问题，我们提出了混合模态擦除和相关（MixER）方法。MixER学习方法通过正交分解、模态混淆和ID-模态相关目标来分离模态特有和模态共享的身份信息。MixER增强了跨模态和混合模态设置下的特征鲁棒性。我们在SYSU-MM01、RegDB和LLMC数据集上的广泛实验表明，我们的方法可以使用单一骨干网络达到最先进的效果，并展示了我们的方法在混合画廊应用中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of visible-infrared person re-identification (VI-ReID) in mixed-modal galleries, where current methods struggle due to domain shifts and low discrimination. The authors propose MixER, a method that disentangles modality-specific and shared identity information, enhancing cross-modal and mixed-modal performance. Experiments on SYSU-MM01, RegDB, and LLMC datasets show that MixER achieves state-of-the-art results with a single backbone and demonstrates flexibility in handling mixed galleries.</div>
<div class="mono" style="margin-top:8px">本文针对当前可见光-红外人再识别（VI-ReID）方法在处理包含可见光和红外图像的混合画廊时的局限性，提出了混合模态擦除和相关（MixER）方法，该方法通过解耦模态特定和共享的身份信息来增强特征的鲁棒性。在SYSU-MM01、RegDB和LLMC数据集上的实验表明，MixER使用单个骨干网络实现了最先进的结果，并在跨模态和混合模态设置中提高了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible   Person Re-Identification</div>
<div class="meta-line">Authors: Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang</div>
<div class="meta-line">First: 2024-12-26T08:03:53+00:00 · Latest: 2025-01-02T11:22:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.19111v2">Abs</a> · <a href="http://arxiv.org/pdf/2412.19111v2">PDF</a> · <a href="https://github.com/1024AILab/ReID-SEPG">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of deep learning has facilitated the application of person
re-identification (ReID) technology in intelligent security. Visible-infrared
person re-identification (VI-ReID) aims to match pedestrians across infrared
and visible modality images enabling 24-hour surveillance. Current studies
relying on unsupervised modality transformations as well as inefficient
embedding constraints to bridge the spectral differences between infrared and
visible images, however, limit their potential performance. To tackle the
limitations of the above approaches, this paper introduces a simple yet
effective Spectral Enhancement and Pseudo-anchor Guidance Network, named
SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement
scheme based on frequency domain information and greyscale space, which avoids
the information loss typically caused by inefficient modality transformations.
Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is
introduced to bridge local modality discrepancies while better preserving
discriminative identity embeddings. Experimental results on two public
benchmark datasets demonstrate the superior performance of SEPG-Net against
other state-of-the-art methods. The code is available at
https://github.com/1024AILab/ReID-SEPG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>红外可见光人员再识别的光谱增强与伪锚点引导</div>
<div class="mono" style="margin-top:8px">深度学习的发展促进了人员再识别(ReID)技术在智能安全中的应用。可见光-红外人员再识别(VI-ReID)旨在跨红外和可见光模态图像匹配行人，实现全天候监控。当前研究依赖于无监督的模态变换以及低效的嵌入约束来弥合红外和可见光图像之间的光谱差异，但限制了其潜在性能。为解决上述方法的局限性，本文提出了一种简单有效的光谱增强与伪锚点引导网络，命名为SEPG-Net。具体而言，我们提出了一种基于频域信息和灰度空间的更均匀的光谱增强方案，避免了低效模态变换通常引起的信息损失。此外，引入了一种伪锚点引导双向聚合(PABA)损失，以弥合局部模态差异，同时更好地保留区分性身份嵌入。在两个公开基准数据集上的实验结果表明，SEPG-Net 的性能优于其他最先进的方法。代码可在 https://github.com/1024AILab/ReID-SEPG 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in visible-infrared person re-identification by proposing SEPG-Net, which combines spectral enhancement and pseudo-anchor guidance. The method enhances spectral consistency in the frequency domain and grayscale space, and introduces a Pseudo Anchor-guided Bidirectional Aggregation loss to improve discriminative identity embeddings. Experiments on public datasets show that SEPG-Net outperforms existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本文提出SEPG-Net来解决可见红外行人重识别的挑战，该方法增强光谱一致性并引入伪锚点引导双向聚合损失以提高区分性身份嵌入。该方法避免了无效模态转换的信息损失，并在两个公开基准数据集上展示了比现有先进方法更好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Extended Cross-Modality United Learning for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-26T09:30:26+00:00 · Latest: 2024-12-26T09:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.19134v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.19134v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展跨模态联合学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征并减少模态间差距。然而，现有方法缺乏跨模态聚类或过度追求聚类级别的关联，这使得难以进行可靠的模态不变特征学习。为解决这一问题，我们提出了一种扩展跨模态联合学习（ECUL）框架，结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。具体而言，我们设计ECUL以自然地整合模态内聚类、模态间聚类和模态间实例选择，建立紧凑而准确的跨模态关联，同时减少引入噪声标签。此外，EMCC通过扩展编码向量捕获和过滤邻域关系，进一步促进聚类算法中模态不变和相机不变知识的学习。最后，TSMem通过分阶段更新记忆为对比学习提供准确且通用的代理点。在SYSU-MM01和RegDB数据集上的广泛实验结果表明，提出的ECUL表现出有希望的性能，并且甚至优于某些监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by learning modality-invariant features from unlabeled cross-modality datasets. The Extended Cross-Modality United Learning (ECUL) framework, which includes Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem), is proposed to address the limitations of existing methods. ECUL integrates intra-modality and inter-modality clustering, and instance selection to establish accurate cross-modality associations. EMCC enhances clustering by capturing and filtering neighborhood relationships, while TSMem updates memory in stages to provide accurate proxy points for contrastive learning. Experiments on SYSU-MM01 and RegDB datasets show that ECUL outperforms certain supervised methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过学习跨模态未标记数据集中的模态不变特征来改进无监督可见-红外行人再识别。提出的扩展跨模态联合学习（ECUL）框架包括扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。EMCC通过扩展编码向量增强聚类，而TSMem分阶段更新记忆以提供对比学习的准确代理点。在SYSU-MM01和RegDB数据集上的实验表明，ECUL的性能优于某些监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Relieving Universal Label Noise for Unsupervised Visible-Infrared Person   Re-Identification by Inferring from Neighbors</div>
<div class="meta-line">Authors: Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin</div>
<div class="meta-line">First: 2024-12-16T04:04:41+00:00 · Latest: 2024-12-16T04:04:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.12220v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.12220v1">PDF</a> · <a href="https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of
great research and practical significance yet remains challenging due to the
absence of annotations. Existing approaches aim to learn modality-invariant
representations in an unsupervised setting. However, these methods often
encounter label noise within and across modalities due to suboptimal clustering
results and considerable modality discrepancies, which impedes effective
training. To address these challenges, we propose a straightforward yet
effective solution for USL-VI-ReID by mitigating universal label noise using
neighbor information. Specifically, we introduce the Neighbor-guided Universal
Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in
both homogeneous and heterogeneous spaces with soft labels derived from
neighboring samples to reduce label noise. Additionally, we present the
Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability
by minimizing the influence of unreliable samples. Extensive experiments on the
RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing
USL-VI-ReID approaches, despite its simplicity. The source code is available
at: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过从邻居推断缓解通用标签噪声以实现无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）具有重要的研究和实际意义，但由于缺乏注释而仍然具有挑战性。现有方法旨在在无监督设置中学习跨模态不变的表示。然而，这些方法经常由于聚类结果不佳和模态差异较大而遇到跨模态的标签噪声问题，这阻碍了有效的训练。为了解决这些挑战，我们提出了一种简单而有效的USL-VI-ReID解决方案，通过邻居信息减轻通用标签噪声。具体而言，我们引入了邻居引导的通用标签校准（N-ULC）模块，该模块用来自邻居样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们提出了邻居引导的动态加权（N-DW）模块，通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，但我们的方法仍优于现有USL-VI-ReID方法。源代码可在：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by proposing a method to mitigate universal label noise. The approach uses a Neighbor-guided Universal Label Calibration (N-ULC) module to infer soft labels from neighboring samples, and a Neighbor-guided Dynamic Weighting (N-DW) module to reduce the impact of unreliable samples. Experiments on RegDB and SYSU-MM01 datasets show that this method outperforms existing approaches despite its simplicity.</div>
<div class="mono" style="margin-top:8px">该论文通过提出一种缓解通用标签噪声的方法来解决无监督可见-红外行人再识别（USL-VI-ReID）的挑战。该方法引入了邻近引导通用标签校准（N-ULC）模块，使用邻近样本的软标签替换硬伪标签，并引入了邻近引导动态加权（N-DW）模块以减少不可靠样本的影响。实验结果表明，该方法在RegDB和SYSU-MM01数据集上的表现优于现有方法，尽管其结构简单。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modality-Camera Invariant Clustering for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yiming Yang, Weipeng Hu, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-11T09:31:03+00:00 · Latest: 2024-12-11T09:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.08231v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.08231v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
offers a more flexible and cost-effective alternative compared to supervised
methods. This field has gained increasing attention due to its promising
potential. Existing methods simply cluster modality-specific samples and employ
strong association techniques to achieve instance-to-cluster or
cluster-to-cluster cross-modality associations. However, they ignore
cross-camera differences, leading to noticeable issues with excessive splitting
of identities. Consequently, this undermines the accuracy and reliability of
cross-modal associations. To address these issues, we propose a novel Dynamic
Modality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID.
Specifically, our DMIC naturally integrates Modality-Camera Invariant Expansion
(MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive
Learning (HMCL) into a unified framework, which eliminates both the
cross-modality and cross-camera discrepancies in clustering. MIE fuses
inter-modal and inter-camera distance coding to bridge the gaps between
modalities and cameras at the clustering level. DNC employs two dynamic search
strategies to refine the network&#x27;s optimization objective, transitioning from
improving discriminability to enhancing cross-modal and cross-camera
generalizability. Moreover, HMCL is designed to optimize instance-level and
cluster-level distributions. Memories for intra-modality and inter-modality
training are updated using randomly selected samples, facilitating real-time
exploration of modality-invariant representations. Extensive experiments have
demonstrated that our DMIC addresses the limitations present in current
clustering approaches and achieve competitive performance, which significantly
reduces the performance gap with supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态模态-摄像机不变聚类的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）相比监督方法提供了更灵活和成本效益更高的替代方案。该领域因其有希望的潜力而越来越受到关注。现有方法简单地聚类模态特定样本，并采用强关联技术实现实例到聚类或聚类到聚类的跨模态关联。然而，它们忽略了跨摄像机差异，导致身份分割过度，从而削弱了跨模态关联的准确性和可靠性。为了解决这些问题，我们提出了一种新颖的动态模态-摄像机不变聚类（DMIC）框架用于USL-VI-ReID。具体而言，我们的DMIC自然地将模态-摄像机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）整合到一个统一框架中，消除了聚类中的跨模态和跨摄像机差异。MIE融合了跨模态和跨摄像机的距离编码，在聚类层面弥合了模态和摄像机之间的差距。DNC采用两种动态搜索策略来细化网络的优化目标，从提高可区分性过渡到增强跨模态和跨摄像机的一般性。此外，HMCL旨在优化实例级和聚类级分布。使用随机选择的样本更新模态内和跨模态训练的记忆，促进实时探索模态不变表示。大量实验表明，我们的DMIC解决了当前聚类方法的局限性，实现了竞争力的性能，显著减少了与监督方法的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a Dynamic Modality-Camera Invariant Clustering (DMIC) framework for unsupervised visible-infrared person re-identification (USL-VI-ReID) to address the issues of excessive identity splitting due to cross-camera differences. DMIC integrates Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC), and Hybrid Modality Contrastive Learning (HMCL) to eliminate both cross-modality and cross-camera discrepancies. Experiments show that DMIC improves cross-modal and cross-camera generalizability and achieves competitive performance, reducing the performance gap with supervised methods.</div>
<div class="mono" style="margin-top:8px">论文提出了一种动态模态-摄像机不变聚类（DMIC）框架，用于解决现有方法中存在的问题，如由于跨摄像机差异导致的身份过度分裂。DMIC 结合了模态-摄像机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL），以消除跨模态和跨摄像机的差异。该框架提高了可区分性和泛化能力，从而实现了更好的跨模态和跨摄像机关联，并且在与监督方法的性能差距上取得了竞争力的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Mix-Modality Person Re-Identification: A New and Practical Paradigm</div>
<div class="meta-line">Authors: Wei Liu, Xin Xu, Hua Chang, Xin Yuan, Zheng Wang</div>
<div class="meta-line">First: 2024-12-06T02:19:57+00:00 · Latest: 2024-12-06T02:19:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.04719v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.04719v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visible-infrared cross-modality person re-identification research has
only focused on exploring the bi-modality mutual retrieval paradigm, and we
propose a new and more practical mix-modality retrieval paradigm. Existing
Visible-Infrared person re-identification (VI-ReID) methods have achieved some
results in the bi-modality mutual retrieval paradigm by learning the
correspondence between visible and infrared modalities. However, significant
performance degradation occurs due to the modality confusion problem when these
methods are applied to the new mix-modality paradigm. Therefore, this paper
proposes a Mix-Modality person re-identification (MM-ReID) task, explores the
influence of modality mixing ratio on performance, and constructs mix-modality
test sets for existing datasets according to the new mix-modality testing
paradigm. To solve the modality confusion problem in MM-ReID, we propose a
Cross-Identity Discrimination Harmonization Loss (CIDHL) adjusting the
distribution of samples in the hyperspherical feature space, pulling the
centers of samples with the same identity closer, and pushing away the centers
of samples with different identities while aggregating samples with the same
modality and the same identity. Furthermore, we propose a Modality Bridge
Similarity Optimization Strategy (MBSOS) to optimize the cross-modality
similarity between the query and queried samples with the help of the similar
bridge sample in the gallery. Extensive experiments demonstrate that compared
to the original performance of existing cross-modality methods on MM-ReID, the
addition of our CIDHL and MBSOS demonstrates a general improvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态人员再识别：一种新的和实用的范式</div>
<div class="mono" style="margin-top:8px">当前可见-红外跨模态人员再识别研究仅专注于探索双模态互检索范式，我们提出了一种新的和更实用的混合模态检索范式。现有的可见-红外人员再识别（VI-ReID）方法通过学习可见和红外模态之间的对应关系，在双模态互检索范式中取得了一些成果。然而，当这些方法应用于新的混合模态范式时，由于模态混淆问题，性能显著下降。因此，本文提出了混合模态人员再识别（MM-ReID）任务，探索模态混合比例对性能的影响，并根据新的混合模态测试范式为现有数据集构建混合模态测试集。为了解决MM-ReID中的模态混淆问题，我们提出了一种跨身份鉴别和谐化损失（CIDHL），调整超球面特征空间中样本的分布，将具有相同身份的样本中心拉近，将具有不同身份的样本中心推开，同时聚合具有相同模态和相同身份的样本。此外，我们提出了一种模态桥梁相似性优化策略（MBSOS），通过帮助查询样本和查询样本之间的相似桥样本优化跨模态相似性。广泛的实验表明，与现有跨模态方法在MM-ReID上的原始性能相比，加入我们的CIDHL和MBSOS显示出普遍的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing visible-infrared person re-identification methods by proposing a new mix-modality retrieval paradigm. It introduces a Mix-Modality person re-identification (MM-ReID) task and explores the impact of modality mixing ratio on performance. To tackle the modality confusion problem, the authors propose a Cross-Identity Discrimination Harmonization Loss (CIDHL) and a Modality Bridge Similarity Optimization Strategy (MBSOS). Experimental results show that these methods improve the performance of existing cross-modality methods in the MM-ReID task.</div>
<div class="mono" style="margin-top:8px">本文提出了一个新的混合模态检索范式，以解决当前可见光-红外人再识别方法的局限性。它引入了跨身份鉴别和谐化损失（CIDHL）和模态桥梁相似性优化策略（MBSOS），以提高混合模态设置下的性能。实验表明，这些方法显著提升了现有跨模态方法在混合模态范式下的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Homogeneous and Heterogeneous Consistent Label Associations   for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2024-02-01T15:33:17+00:00 · Latest: 2024-12-04T03:55:35+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.00672v4">Abs</a> · <a href="http://arxiv.org/pdf/2402.00672v4">PDF</a> · <a href="https://github.com/FranklinLingfeng/code_for_MULT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID)
endeavors to retrieve pedestrian images of the same identity from different
modalities without annotations. While prior work focuses on establishing
cross-modality pseudo-label associations to bridge the modality-gap, they
ignore maintaining the instance-level homogeneous and heterogeneous consistency
between the feature space and the pseudo-label space, resulting in coarse
associations. In response, we introduce a Modality-Unified Label Transfer
(MULT) module that simultaneously accounts for both homogeneous and
heterogeneous fine-grained instance-level structures, yielding high-quality
cross-modality label associations. It models both homogeneous and heterogeneous
affinities, leveraging them to quantify the inconsistency between the
pseudo-label space and the feature space, subsequently minimizing it. The
proposed MULT ensures that the generated pseudo-labels maintain alignment
across modalities while upholding structural consistency within intra-modality.
Additionally, a straightforward plug-and-play Online Cross-memory Label
Refinement (OCLR) module is proposed to further mitigate the side effects of
noisy pseudo-labels while simultaneously aligning different modalities, coupled
with an Alternative Modality-Invariant Representation Learning (AMIRL)
framework. Experiments demonstrate that our proposed method outperforms
existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of
our MULT in comparison to other cross-modality association methods. Code is
available at https://github.com/FranklinLingfeng/code_for_MULT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索一致标签关联的同质性和异质性一致性在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，从不同模态中检索同一身份的行人图像。先前的工作主要集中在建立跨模态的伪标签关联以弥补模态差异，但忽略了在特征空间和伪标签空间之间保持实例级的同质性和异质性一致性，导致关联粗糙。为此，我们引入了一个模态统一标签转移（MULT）模块，该模块同时考虑了同质性和异质性的细粒度实例级结构，从而产生高质量的跨模态标签关联。该模块利用同质性和异质性亲和力来量化伪标签空间与特征空间之间的不一致性，并最小化这种不一致性。所提出的MULT确保生成的伪标签在不同模态之间保持对齐，同时在同模态内保持结构一致性。此外，我们还提出了一种简单的即插即用在线跨记忆标签精炼（OCLR）模块，以进一步减轻嘈杂伪标签的副作用，同时对齐不同模态，结合了一种替代模态不变表示学习（AMIRL）框架。实验表明，我们提出的方法优于现有的USL-VI-ReID方法，突显了我们MULT在与其他跨模态关联方法相比的优势。代码可在https://github.com/FranklinLingfeng/code_for_MULT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by introducing a Modality-Unified Label Transfer (MULT) module that simultaneously considers both homogeneous and heterogeneous fine-grained instance-level structures. The MULT module helps maintain consistency between the feature space and the pseudo-label space, leading to high-quality cross-modality label associations. Additionally, the paper proposes an Online Cross-memory Label Refinement (OCLR) module and an Alternative Modality-Invariant Representation Learning (AMIRL) framework to further refine pseudo-labels and align different modalities. Experimental results show that the proposed method outperforms existing state-of-the-art methods in USL-VI-ReID.</div>
<div class="mono" style="margin-top:8px">该论文通过引入同时考虑同质性和异质性细粒度实例结构的Modality-Unified Label Transfer (MULT) 模块，解决了无监督可见红外行人再识别（USL-VI-ReID）的挑战。MULT模块有助于在特征空间和伪标签空间之间保持一致性，从而生成高质量的跨模态标签关联。此外，论文还提出了Online Cross-memory Label Refinement (OCLR) 模块和Alternative Modality-Invariant Representation Learning (AMIRL) 框架，进一步细化伪标签并使不同模态对齐。实验结果表明，所提出的方法在USL-VI-ReID方面优于现有最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with   Neighbor-Guided Label Refinement</div>
<div class="meta-line">Authors: De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T04:40:30+00:00 · Latest: 2024-11-03T13:50:53+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12711v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12711v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作学习与邻域引导标签精炼的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从跨模态的未标注数据集中学习模态不变特征，这对于视频监控系统中的实际应用至关重要。解决USL-VI-ReID任务的关键在于解决跨模态数据关联问题，以便进行后续的异质联合学习。为了解决这一问题，我们提出了一种双最优传输标签分配（DOTLA）框架，以同时将一种模态生成的标签分配给其对应的模态。提出的DOTLA机制以互为强化和高效的方式解决了跨模态数据关联问题，从而有效减少了某些不足和噪声标签关联的副作用。此外，我们还提出了一种跨模态邻域一致性引导的标签精炼和正则化模块，在假设每个示例的预测或标签分布应与其最近邻相似的前提下，消除不准确的监督信号带来的负面影响。在公开的SYSU-MM01和RegDB数据集上的广泛实验结果表明，所提出的方法具有很高的有效性，平均mAP比现有最先进的方法高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the unsupervised visible-infrared person re-identification (USL-VI-ReID) task by proposing a Dual Optimal Transport Label Assignment (DOTLA) framework. The method aims to solve the cross-modality data association problem and improve heterogeneous joint learning. Experimental results on public datasets SYSU-MM01 and RegDB show that the proposed method outperforms existing state-of-the-art approaches by 7.76% in mean average precision (mAP).</div>
<div class="mono" style="margin-top:8px">论文提出了一个双最优传输标签分配（DOTLA）框架来解决跨模态数据关联问题，通过将一种模态生成的标签分配给另一种模态，增强联合学习过程。此外，还引入了一个跨模态邻居一致性引导的标签精炼模块，以提高标签准确性。在公共数据集SYSU-MM01和RegDB上的实验结果表明，所提出的方法在平均精度（mAP）上比现有最先进的方法高出7.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised   Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T03:27:46+00:00 · Latest: 2024-11-03T13:48:21+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12673v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12673v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to
match pedestrian images of the same identity from different modalities without
annotations. Existing works mainly focus on alleviating the modality gap by
aligning instance-level features of the unlabeled samples. However, the
relationships between cross-modality clusters are not well explored. To this
end, we propose a novel bilateral cluster matching-based learning framework to
reduce the modality gap by matching cross-modality clusters. Specifically, we
design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)
algorithm through optimizing the maximum matching problem in a bipartite graph.
Then, the matched pairwise clusters utilize shared visible and infrared
pseudo-labels during the model training. Under such a supervisory signal, a
Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework
is proposed to align features jointly at a cluster-level. Meanwhile, the
cross-modality Consistency Constraint (CC) is proposed to explicitly reduce the
large modality discrepancy. Extensive experiments on the public SYSU-MM01 and
RegDB datasets demonstrate the effectiveness of the proposed method, surpassing
state-of-the-art approaches by a large margin of 8.76% mAP on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效双边跨模态簇匹配的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，匹配不同模态下同一身份的行人图像。现有工作主要集中在通过对齐未标注样本的实例级特征来缓解模态差异。然而，跨模态簇之间的关系尚未得到充分探索。为此，我们提出了一种新颖的双边簇匹配学习框架，通过匹配跨模态簇来减少模态差异。具体地，我们通过在二分图中优化最大匹配问题设计了一种多对多双边跨模态簇匹配（MBCCM）算法。然后，匹配的成对簇在模型训练过程中利用共享的可见光和红外伪标签。在这样的监督信号下，我们提出了一种模态特定和模态无关的对比学习框架，以在簇级联合对齐特征。同时，我们提出了跨模态一致性约束（CC）以显式地减少大模态差异。在公开的SYSU-MM01和RegDB数据集上的大量实验表明，所提出的方法具有有效性，平均mAP比现有最佳方法高出8.76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses unsupervised visible-infrared person re-identification by proposing a novel bilateral cluster matching framework. The method focuses on matching cross-modality clusters to reduce the modality gap. It introduces a Many-to-many Bilateral Cross-Modality Cluster Matching algorithm and a Modality-Specific and Modality-Agnostic contrastive learning framework. The approach also includes a cross-modality consistency constraint to further align features. Experiments show the method outperforms existing techniques with an average improvement of 8.76% in mAP.</div>
<div class="mono" style="margin-top:8px">论文提出了一种双边簇匹配框架来解决无监督可见红外行人再识别的挑战。该框架包括一种Many-to-many双边跨模态簇匹配（MBCCM）算法来匹配跨模态的簇，以及一种模态特定和模态无关（MSMA）对比学习框架来在簇级别对齐特征。此外，还提出了一种跨模态一致性约束来显式地减少模态差异。在SYSU-MM01和RegDB数据集上的实验表明，该方法显著提升了性能，平均mAP提高了8.76%，超过了现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Adversarial Patches for Physical Attacks on Cross-Modal   Pedestrian Re-Identification</div>
<div class="meta-line">Authors: Yue Su, Hao Li, Maoguo Gong</div>
<div class="meta-line">First: 2024-10-26T06:40:10+00:00 · Latest: 2024-10-26T06:40:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.20097v1">Abs</a> · <a href="http://arxiv.org/pdf/2410.20097v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared pedestrian Re-identification (VI-ReID) aims to match
pedestrian images captured by infrared cameras and visible cameras. However,
VI-ReID, like other traditional cross-modal image matching tasks, poses
significant challenges due to its human-centered nature. This is evidenced by
the shortcomings of existing methods, which struggle to extract common features
across modalities, while losing valuable information when bridging the gap
between them in the implicit feature space, potentially compromising security.
To address this vulnerability, this paper introduces the first physical
adversarial attack against VI-ReID models. Our method, termed Edge-Attack,
specifically tests the models&#x27; ability to leverage deep-level implicit features
by focusing on edge information, the most salient explicit feature
differentiating individuals across modalities. Edge-Attack utilizes a novel
two-step approach. First, a multi-level edge feature extractor is trained in a
self-supervised manner to capture discriminative edge representations for each
individual. Second, a generative model based on Vision Transformer Generative
Adversarial Networks (ViTGAN) is employed to generate adversarial patches
conditioned on the extracted edge features. By applying these patches to
pedestrian clothing, we create realistic, physically-realizable adversarial
samples. This black-box, self-supervised approach ensures the generalizability
of our attack against various VI-ReID models. Extensive experiments on
SYSU-MM01 and RegDB datasets, including real-world deployments, demonstrate the
effectiveness of Edge- Attack in significantly degrading the performance of
state-of-the-art VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成对抗补丁对跨模态行人重识别的物理攻击</div>
<div class="mono" style="margin-top:8px">可见-红外行人重识别（VI-ReID）旨在匹配由红外相机和可见光相机捕获的行人图像。然而，由于其以人为中心的性质，VI-ReID与其他传统的跨模态图像匹配任务一样，面临着显著的挑战。现有的方法在提取跨模态的共同特征方面存在不足，同时在隐式特征空间中弥合模态之间的差距时会丢失有价值的信息，这可能影响安全性。为应对这一漏洞，本文首次提出了针对VI-ReID模型的物理对抗攻击。我们的方法称为Edge-Attack，特别通过关注最能区分不同模态个体的边缘信息，测试模型利用深层隐式特征的能力。Edge-Attack采用了一种新颖的两步方法。首先，通过自监督方式训练多级边缘特征提取器，以捕获每个个体的判别边缘表示。其次，采用基于视觉变换器生成对抗网络（ViTGAN）的生成模型，根据提取的边缘特征生成条件对抗补丁。通过将这些补丁应用于行人的服装，我们创建了现实且物理上可实现的对抗样本。这种黑盒、自监督的方法确保了我们的攻击对各种VI-ReID模型的普适性。在SYSU-MM01和RegDB数据集上的广泛实验，包括实际部署，证明了Edge-Attack在显著降低最先进的VI-ReID方法性能方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of Visible-infrared pedestrian Re-identification (VI-ReID) models by introducing Edge-Attack, a physical adversarial attack. The method uses a two-step process: first, a multi-level edge feature extractor is trained to capture discriminative edge representations, and second, a ViTGAN generates adversarial patches based on these features. These patches are applied to pedestrian clothing to create realistic adversarial samples. Experiments on SYSU-MM01 and RegDB datasets show that Edge-Attack significantly degrades the performance of state-of-the-art VI-ReID methods.</div>
<div class="mono" style="margin-top:8px">该论文通过引入Edge-攻击，一种物理对抗攻击方法，来解决可见-红外行人再识别（VI-ReID）模型的漏洞问题。该方法采用两步过程：首先，训练一个多级边缘特征提取器以捕获区分性边缘表示；其次，使用基于Vision Transformer生成对抗网络（ViTGAN）的生成模型，根据提取的边缘特征生成对抗补丁。将这些补丁应用于行人衣物后，在SYSU-MM01和RegDB数据集上显著降低了最先进的VI-ReID方法的性能，突显了跨模态识别系统中需要采取的稳健安全措施的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">WRIM-Net: Wide-Ranging Information Mining Network for Visible-Infrared   Person Re-Identification</div>
<div class="meta-line">Authors: Yonggan Wu, Ling-Chao Meng, Yuan Zichao, Sixian Chan, Hong-Qiang Wang</div>
<div class="meta-line">First: 2024-08-20T08:06:16+00:00 · Latest: 2024-08-20T08:06:16+00:00</div>
<div class="meta-line">Comments: 18 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2408.10624v1">Abs</a> · <a href="http://arxiv.org/pdf/2408.10624v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For the visible-infrared person re-identification (VI-ReID) task, one of the
primary challenges lies in significant cross-modality discrepancy. Existing
methods struggle to conduct modality-invariant information mining. They often
focus solely on mining singular dimensions like spatial or channel, and
overlook the extraction of specific-modality multi-dimension information. To
fully mine modality-invariant information across a wide range, we introduce the
Wide-Ranging Information Mining Network (WRIM-Net), which mainly comprises a
Multi-dimension Interactive Information Mining (MIIM) module and an
Auxiliary-Information-based Contrastive Learning (AICL) approach. Empowered by
the proposed Global Region Interaction (GRI), MIIM comprehensively mines
non-local spatial and channel information through intra-dimension interaction.
Moreover, Thanks to the low computational complexity design, separate MIIM can
be positioned in shallow layers, enabling the network to better mine
specific-modality multi-dimension information. AICL, by introducing the novel
Cross-Modality Key-Instance Contrastive (CMKIC) loss, effectively guides the
network in extracting modality-invariant information. We conduct extensive
experiments not only on the well-known SYSU-MM01 and RegDB datasets but also on
the latest large-scale cross-modality LLCM dataset. The results demonstrate
WRIM-Net&#x27;s superiority over state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WRIM-Net：可见-红外人体重识别广泛信息挖掘网络</div>
<div class="mono" style="margin-top:8px">对于可见-红外人体重识别（VI-ReID）任务，主要挑战在于显著的跨模态差异。现有方法难以进行模态不变的信息挖掘。它们通常仅专注于挖掘单一维度的信息，如空间或通道，而忽视了特定模态多维度信息的提取。为了全面挖掘广泛范围内的模态不变信息，我们引入了广泛信息挖掘网络（WRIM-Net），主要由多维度交互信息挖掘（MIIM）模块和基于辅助信息的对比学习（AICL）方法组成。借助提出的全局区域交互（GRI），MIIM通过内在维度交互全面挖掘非局部的空间和通道信息。此外，由于低计算复杂度设计，单独的MIIM可以放置在浅层，使网络更好地挖掘特定模态多维度信息。AICL通过引入新颖的跨模态关键实例对比损失（CMKIC损失），有效引导网络提取模态不变信息。我们在SYSU-MM01和RegDB等知名数据集以及最新的大规模跨模态LLCM数据集上进行了广泛的实验。结果表明WRIM-Net优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of cross-modality discrepancy in visible-infrared person re-identification by introducing WRIM-Net, which includes a Multi-dimension Interactive Information Mining module and an Auxiliary-Information-based Contrastive Learning approach. The method uses Global Region Interaction to comprehensively mine spatial and channel information and employs a novel Cross-Modality Key-Instance Contrastive loss to guide the extraction of modality-invariant information. Experiments on SYSU-MM01, RegDB, and LLCM datasets show WRIM-Net outperforms existing methods.</div>
<div class="mono" style="margin-top:8px">研究通过引入WRIM-Net解决可见红外人再识别中的跨模态差异问题，WRIM-Net包含多维度交互信息挖掘模块和辅助信息基于对比学习方法。MIIM模块通过内维度交互挖掘非局部的空间和通道信息，AICL使用新型CMKIC损失引导提取跨模态不变信息。实验结果在SYSU-MM01、RegDB和LLCM数据集上表明WRIM-Net优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Memory Matching for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-01-12T01:24:04+00:00 · Latest: 2024-07-29T09:40:11+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2401.06825v2">Abs</a> · <a href="http://arxiv.org/pdf/2401.06825v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a
promising yet challenging retrieval task. The key challenges in USL-VI-ReID are
to effectively generate pseudo-labels and establish pseudo-label
correspondences across modalities without relying on any prior annotations.
Recently, clustered pseudo-label methods have gained more attention in
USL-VI-ReID. However, previous methods fell short of fully exploiting the
individual nuances, as they simply utilized a single memory that represented an
identity to establish cross-modality correspondences, resulting in ambiguous
cross-modality correspondences. To address the problem, we propose a
Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a
Cross-Modality Clustering (CMC) module to generate the pseudo-labels through
clustering together both two modality samples. To associate cross-modality
clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM)
module, ensuring that optimization explicitly focuses on the nuances of
individual perspectives and establishes reliable cross-modality
correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module
to narrow the modality gap while mitigating the effect of noise pseudo-labels
through a soft many-to-many alignment strategy. Extensive experiments on the
public SYSU-MM01 and RegDB datasets demonstrate the reliability of the
established cross-modality correspondences and the effectiveness of our MMM.
The source codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多记忆匹配在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）是一项有前景但极具挑战性的检索任务。USL-VI-ReID 的关键挑战在于有效生成伪标签并跨模态建立伪标签对应关系，而不依赖任何先验注释。最近，聚类伪标签方法在 USL-VI-ReID 中引起了更多关注。然而，先前的方法未能充分利用个体差异，因为它们仅利用单一记忆来表示身份以建立跨模态对应关系，导致跨模态对应关系模糊。为解决这一问题，我们提出了一种 USL-VI-ReID 的多记忆匹配（MMM）框架。我们首先设计了一个跨模态聚类（CMC）模块，通过聚类两种模态样本来生成伪标签。为了关联跨模态聚类伪标签，我们设计了一个多记忆学习和匹配（MMLM）模块，确保优化明确关注个体视角的差异并建立可靠的跨模态对应关系。最后，我们设计了一个软聚类级对齐（SCA）模块，在通过软多对多对齐策略减轻噪声伪标签影响的同时，缩小模态差距。在公开的 SYSU-MM01 和 RegDB 数据集上的广泛实验表明，建立的跨模态对应关系的可靠性以及我们 MMM 的有效性。源代码将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Multi-Memory Matching (MMM) framework. It introduces a Cross-Modality Clustering (CMC) module to generate pseudo-labels and a Multi-Memory Learning and Matching (MMLM) module to establish reliable cross-modality correspondences. Additionally, a Soft Cluster-level Alignment (SCA) module is designed to reduce the modality gap and mitigate noise. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the proposed method in generating reliable cross-modality correspondences.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多记忆匹配（MMM）框架来解决无监督可见红外行人重识别的挑战。该框架包含跨模态聚类（CMC）模块生成伪标签，多记忆学习和匹配（MMLM）模块建立可靠的跨模态对应关系，以及软簇级对齐（SCA）模块减少模态差距并减轻噪声影响。实验结果表明，该方法在SYSU-MM01和RegDB数据集上能够生成可靠的跨模态对应关系。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Identity-Guided Attention Network for Visible-Infrared Person   Re-identification</div>
<div class="meta-line">Authors: Peng Gao, Yujian Lee, Hui Zhang, Xubo Liu, Yiyang Hu, Guquan Jing</div>
<div class="meta-line">First: 2024-05-21T12:04:56+00:00 · Latest: 2024-07-22T09:23:26+00:00</div>
<div class="meta-line">Comments: I need to further debug my code to improve accuracy</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2405.12713v2">Abs</a> · <a href="http://arxiv.org/pdf/2405.12713v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) aims to match people with
the same identity between visible and infrared modalities. VI-ReID is a
challenging task due to the large differences in individual appearance under
different modalities. Existing methods generally try to bridge the cross-modal
differences at image or feature level, which lacks exploring the discriminative
embeddings. Effectively minimizing these cross-modal discrepancies relies on
obtaining representations that are guided by identity and consistent across
modalities, while also filtering out representations that are irrelevant to
identity. To address these challenges, we introduce a dynamic identity-guided
attention network (DIAN) to mine identity-guided and modality-consistent
embeddings, facilitating effective bridging the gap between different
modalities. Specifically, in DIAN, to pursue a semantically richer
representation, we first use orthogonal projection to fuse the features from
two connected coarse and fine layers. Furthermore, we first use dynamic
convolution kernels to mine identity-guided and modality-consistent
representations. More notably, a cross embedding balancing loss is introduced
to effectively bridge cross-modal discrepancies by above embeddings.
Experimental results on SYSU-MM01 and RegDB datasets show that DIAN achieves
state-of-the-art performance. Specifically, for indoor search on SYSU-MM01, our
method achieves 86.28% rank-1 accuracy and 87.41% mAP, respectively. Our code
will be available soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可见-红外人体重识别的动态身份引导注意力网络</div>
<div class="mono" style="margin-top:8px">可见-红外人体重识别（VI-ReID）旨在将不同模态下的同一身份的人匹配起来。由于不同模态下个体外观的巨大差异，VI-ReID 是一个具有挑战性的任务。现有方法通常试图在图像或特征级别上弥合跨模态差异，但缺乏探索具有区分性的嵌入。有效缩小这些跨模态差异依赖于获得由身份引导且跨模态一致的表示，同时过滤掉与身份无关的表示。为了解决这些挑战，我们引入了一种动态身份引导注意力网络（DIAN），以挖掘身份引导且模态一致的嵌入，促进不同模态之间的有效连接。具体而言，在 DIAN 中，为了追求语义更丰富的表示，我们首先使用正交投影将两个相连的粗层和细层的特征融合。此外，我们首先使用动态卷积核挖掘身份引导且模态一致的表示。更值得注意的是，引入了一种跨嵌入平衡损失，通过上述嵌入有效弥合跨模态差异。在 SYSU-MM01 和 RegDB 数据集上的实验结果表明，DIAN 达到了最先进的性能。具体而言，对于 SYSU-MM01 的室内搜索，我们的方法分别实现了 86.28% 的 rank-1 准确率和 87.41% 的 mAP。我们的代码将很快开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DIAN, a dynamic identity-guided attention network for visible-infrared person re-identification. It addresses the challenge of large appearance differences between modalities by using orthogonal projection and dynamic convolution kernels to mine identity-guided and modality-consistent embeddings. The cross embedding balancing loss further minimizes cross-modal discrepancies. DIAN achieves state-of-the-art performance with 86.28% rank-1 accuracy and 87.41% mAP on the SYSU-MM01 dataset for indoor search.</div>
<div class="mono" style="margin-top:8px">论文提出了DIAN，一种用于可见光-红外人像重识别的动态身份引导注意力网络。该方法通过关注身份导向和模态一致性嵌入来解决不同模态间显著外观差异的挑战。方法使用正交投影和动态卷积核融合特征，并引入跨嵌入平衡损失来最小化跨模态差异。实验结果表明，DIAN在SYSU-MM01和RegDB数据集上优于现有方法，室内搜索任务中达到86.28%的rank-1准确率和87.41%的mAP。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="http://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a
challenging retrieval task that aims to retrieve cross-modality pedestrian
images without using any label information. In this task, the large
cross-modality variance makes it difficult to generate reliable cross-modality
labels, and the lack of annotations also provides additional difficulties for
learning modality-invariant features. In this paper, we first deduce an
optimization objective for unsupervised VI-ReID based on the mutual information
between the model&#x27;s cross-modality input and output. With equivalent
derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy
minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable
cross-modality matching) are obtained. Under their guidance, we design a loop
iterative training strategy alternating between model training and
cross-modality matching. In the matching stage, a uniform prior guided optimal
transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched
visible and infrared prototypes. In the training stage, we utilize this
matching information to introduce prototype-based contrastive learning for
minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive
experimental results on benchmarks demonstrate the effectiveness of our method,
e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any
annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人再识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人再识别（USVI-ReID）是一个具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出一个无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一个交替进行模型训练和跨模态匹配的循环训练策略。在匹配阶段，我们提出了一种均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化类内和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如在SYSU-MM01和RegDB上分别达到了60.6%和90.3%的Rank-1精度，且没有任何注释。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: Sharpness, Fairness, and Fitness, and proposes a loop iterative training strategy. The method uses a uniform prior guided optimal transport assignment to select matched prototypes and introduces prototype-based contrastive learning to minimize entropy. The results show significant improvements, achieving 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div>
<div class="mono" style="margin-top:8px">本文通过在跨模态输入和输出之间使用互信息来制定无监督可见红外行人再识别的优化目标，引入了三个学习原则：“Sharpness”用于最小化熵，“Fairness”用于均匀标签分布，“Fitness”用于可靠的跨模态匹配。方法采用循环迭代训练策略和均匀先验引导的最优传输分配来增强跨模态匹配，并利用这些信息进行基于原型的对比学习以最小化内模态和跨模态熵。实验结果表明显著改进，SYSU-MM01和RegDB基准上的Rank-1准确率分别为60.6%和90.3%，且无需标注。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Visible-Infrared Person Re-identification with Modality- and   Instance-aware Visual Prompt Learning</div>
<div class="meta-line">Authors: Ruiqi Wu, Bingliang Jiao, Wenxuan Wang, Meng Liu, Peng Wang</div>
<div class="meta-line">Venue: ICMR&#x27;24: Proceedings of the 2024 International Conference on
  Multimedia Retrieval (2024) 579 - 588</div>
<div class="meta-line">First: 2024-06-18T06:39:03+00:00 · Latest: 2024-06-18T06:39:03+00:00</div>
<div class="meta-line">Comments: Accepyed by ACM International Conference on Multimedia Retrieval
  (ICMR&#x27;24)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.12316v1">Abs</a> · <a href="http://arxiv.org/pdf/2406.12316v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Visible-Infrared Person Re-identification (VI ReID) aims to match visible
and infrared images of the same pedestrians across non-overlapped camera views.
These two input modalities contain both invariant information, such as shape,
and modality-specific details, such as color. An ideal model should utilize
valuable information from both modalities during training for enhanced
representational capability. However, the gap caused by modality-specific
information poses substantial challenges for the VI ReID model to handle
distinct modality inputs simultaneously. To address this, we introduce the
Modality-aware and Instance-aware Visual Prompts (MIP) network in our work,
designed to effectively utilize both invariant and specific information for
identification. Specifically, our MIP model is built on the transformer
architecture. In this model, we have designed a series of modality-specific
prompts, which could enable our model to adapt to and make use of the specific
information inherent in different modality inputs, thereby reducing the
interference caused by the modality gap and achieving better identification.
Besides, we also employ each pedestrian feature to construct a group of
instance-specific prompts. These customized prompts are responsible for guiding
our model to adapt to each pedestrian instance dynamically, thereby capturing
identity-level discriminative clues for identification. Through extensive
experiments on SYSU-MM01 and RegDB datasets, the effectiveness of both our
designed modules is evaluated. Additionally, our proposed MIP performs better
than most state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用模态和实例感知视觉提示学习增强可见光-红外行人重识别</div>
<div class="mono" style="margin-top:8px">可见光-红外行人重识别（VI ReID）旨在跨非重叠摄像头视角匹配同一行人的可见光和红外图像。这两种输入模态包含不变信息，如形状，以及模态特定细节，如颜色。理想的模型应该在训练过程中利用两种模态的有价值信息，以增强表示能力。然而，由模态特定信息引起的差距给VI ReID模型同时处理不同模态输入带来了重大挑战。为解决这一问题，我们在工作中引入了模态感知和实例感知视觉提示（MIP）网络，旨在有效利用不变和特定信息进行识别。具体而言，我们的MIP模型基于Transformer架构。在这个模型中，我们设计了一系列模态特定提示，使我们的模型能够适应并利用不同模态输入中固有的特定信息，从而减少模态差距造成的干扰并实现更好的识别。此外，我们还利用每个行人的特征构建了一组实例特定提示。这些定制提示负责引导我们的模型动态适应每个行人类别，从而捕捉识别所需的身份级区分线索。通过在SYSU-MM01和RegDB数据集上的广泛实验，评估了我们设计模块的有效性。此外，我们提出的MIP方法优于大多数最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the Visible-Infrared Person Re-identification (VI ReID) challenge by proposing the Modality-aware and Instance-aware Visual Prompts (MIP) network. This network uses a transformer architecture to effectively utilize both invariant and modality-specific information. Key findings show that MIP outperforms most state-of-the-art methods on SYSU-MM01 and RegDB datasets, demonstrating its effectiveness in handling the modality gap and improving identification accuracy.</div>
<div class="mono" style="margin-top:8px">论文通过引入Modality-aware和Instance-aware Visual Prompts (MIP)网络解决了可见光-红外行人重识别（VI ReID）问题。该模型使用了变压器架构，有效地利用了不变信息和模态特定信息，减少了模态差异带来的干扰。在SYSU-MM01和RegDB数据集上的实验结果表明，MIP在VI ReID任务中优于大多数最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality   Learning</div>
<div class="meta-line">Authors: Zhihao Qian, Yutian Lin, Bo Du</div>
<div class="meta-line">First: 2023-02-16T10:56:00+00:00 · Latest: 2024-04-30T09:51:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2302.08212v2">Abs</a> · <a href="http://arxiv.org/pdf/2302.08212v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) aims to retrieve images
of the same pedestrian from different modalities, where the challenges lie in
the significant modality discrepancy. To alleviate the modality gap, recent
methods generate intermediate images by GANs, grayscaling, or mixup strategies.
However, these methods could introduce extra data distribution, and the
semantic correspondence between the two modalities is not well learned. In this
paper, we propose a Patch-Mixed Cross-Modality framework (PMCM), where two
images of the same person from two modalities are split into patches and
stitched into a new one for model learning. A part-alignment loss is introduced
to regularize representation learning, and a patch-mixed modality learning loss
is proposed to align between the modalities. In this way, the model learns to
recognize a person through patches of different styles, thereby the modality
semantic correspondence can be inferred. In addition, with the flexible image
generation strategy, the patch-mixed images freely adjust the ratio of
different modality patches, which could further alleviate the modality
imbalance problem. On two VI-ReID datasets, we report new state-of-the-art
performance with the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可见-红外行人重识别通过块混合跨模态学习</div>
<div class="mono" style="margin-top:8px">可见-红外行人重识别（VI-ReID）旨在从不同模态中检索同一行人的图像，其中挑战在于显著的模态差异。为缓解模态差距，最近的方法通过生成中间图像、使用生成对抗网络（GAN）、灰度化或混合策略来生成中间图像。然而，这些方法可能会引入额外的数据分布，且两个模态之间的语义对应关系没有很好地学习。在本文中，我们提出了一种块混合跨模态框架（PMCM），其中来自两个模态的同一个人的两张图像被分割成块并拼接成一个新的图像以供模型学习。引入了一种部分对齐损失来正则化表示学习，并提出了一种块混合模态学习损失以在模态之间进行对齐。这样，模型学会通过不同风格的块来识别行人，从而可以推断出模态语义对应关系。此外，通过灵活的图像生成策略，块混合图像可以自由调整不同模态块的比例，从而进一步缓解模态不平衡问题。在两个VI-ReID数据集上，我们使用所提出的方法报告了新的最佳性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of visible-infrared person re-identification by proposing a Patch-Mixed Cross-Modality framework (PMCM). The method splits images into patches from both visible and infrared modalities, then stitches them to learn cross-modality representations. Key findings include improved performance on two datasets and a new state-of-the-art result, attributed to the part-alignment loss and patch-mixed modality learning loss that enhance semantic correspondence and address modality imbalance.</div>
<div class="mono" style="margin-top:8px">本文提出了一种Patch-Mixed Cross-Modality框架（PMCM），通过将两种模态的图像分割成块并拼接来解决可见光-红外人再识别的挑战。该方法引入了部分对齐损失和块混合模态学习损失，以提高模型在不同模态下识别人员的能力。在两个数据集上的实验结果表明，所提出的方法达到了新的最佳性能。</div>
</details>
</div>
<div class="card">
<div class="title">Parameter Hierarchical Optimization for Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Zeng YU, Yunxiao Shi</div>
<div class="meta-line">First: 2024-04-11T17:27:39+00:00 · Latest: 2024-04-11T17:27:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2404.07930v1">Abs</a> · <a href="http://arxiv.org/pdf/2404.07930v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-reID) aims at matching
cross-modality pedestrian images captured by disjoint visible or infrared
cameras. Existing methods alleviate the cross-modality discrepancies via
designing different kinds of network architectures. Different from available
methods, in this paper, we propose a novel parameter optimizing paradigm,
parameter hierarchical optimization (PHO) method, for the task of VI-ReID. It
allows part of parameters to be directly optimized without any training, which
narrows the search space of parameters and makes the whole network more easier
to be trained. Specifically, we first divide the parameters into different
types, and then introduce a self-adaptive alignment strategy (SAS) to
automatically align the visible and infrared images through transformation.
Considering that features in different dimension have varying importance, we
develop an auto-weighted alignment learning (AAL) module that can automatically
weight features according to their importance. Importantly, in the alignment
process of SAS and AAL, all the parameters are immediately optimized with
optimization principles rather than training the whole network, which yields a
better parameter training manner. Furthermore, we establish the cross-modality
consistent learning (CCL) loss to extract discriminative person representations
with translation consistency. We provide both theoretical justification and
empirical evidence that our proposed PHO method outperform existing VI-reID
approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可见-红外行人再识别的参数分层优化</div>
<div class="mono" style="margin-top:8px">可见-红外行人再识别（VI-reID）旨在匹配由分离的可见光或红外摄像机捕获的跨模态行人图像。现有方法通过设计不同类型的网络架构来缓解跨模态差异。与现有方法不同，本文提出了一种新颖的参数优化范式——参数分层优化（PHO）方法，用于VI-ReID任务。它允许部分参数无需任何训练即可直接优化，从而缩小参数搜索空间，使整个网络更容易训练。具体来说，我们首先将参数分为不同类型，然后引入一种自适应对齐策略（SAS），通过变换自动对齐可见光和红外图像。考虑到不同维度的特征具有不同的重要性，我们开发了一种自动加权对齐学习（AAL）模块，可以根据其重要性自动加权特征。重要的是，在SAS和AAL的对齐过程中，所有参数都立即根据优化原则进行优化，而不是训练整个网络，从而获得更好的参数训练方式。此外，我们建立了跨模态一致学习（CCL）损失，以通过翻译一致性提取具有区分性的行人表示。我们提供了理论依据和实验证据，证明我们提出的PHO方法优于现有的VI-reID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of visible-infrared person re-identification (VI-reID) by proposing a novel parameter hierarchical optimization (PHO) method. This method divides parameters into different types and introduces a self-adaptive alignment strategy (SAS) and an auto-weighted alignment learning (AAL) module to align and weight features, respectively. The PHO method optimizes parameters directly without training the entire network, which simplifies the training process. The authors demonstrate that their approach outperforms existing VI-reID methods through both theoretical justification and empirical evidence.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新型参数层次优化（PHO）方法，以解决可见光-红外人像再识别的挑战。不同于现有依赖复杂网络架构的方法，PHO 允许部分参数无需训练即可直接优化，从而缩小参数搜索空间并简化训练过程。该方法包括自适应对齐策略（SAS）和自动加权对齐学习（AAL）模块，能够自动对齐和加权特征以提高跨模态一致性。作者通过理论分析和实验验证，证明了PHO方法优于现有可见光-红外人像再识别方法。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Discriminative Knowledge Learning for Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Kaijie Ren, Lei Zhang</div>
<div class="meta-line">Venue: CVPR 2024</div>
<div class="meta-line">First: 2024-03-18T12:12:45+00:00 · Latest: 2024-03-26T13:21:52+00:00</div>
<div class="meta-line">Comments: CVPR 2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2403.11708v3">Abs</a> · <a href="http://arxiv.org/pdf/2403.11708v3">PDF</a> · <a href="https://github.com/1KK077/IDKL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-Infrared Person Re-identification (VI-ReID) is a challenging
cross-modal pedestrian retrieval task, due to significant intra-class
variations and cross-modal discrepancies among different cameras. Existing
works mainly focus on embedding images of different modalities into a unified
space to mine modality-shared features. They only seek distinctive information
within these shared features, while ignoring the identity-aware useful
information that is implicit in the modality-specific features. To address this
issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)
network to uncover and leverage the implicit discriminative information
contained within the modality-specific. First, we extract modality-specific and
modality-shared features using a novel dual-stream network. Then, the
modality-specific features undergo purification to reduce their modality style
discrepancies while preserving identity-aware discriminative knowledge.
Subsequently, this kind of implicit knowledge is distilled into the
modality-shared feature to enhance its distinctiveness. Finally, an alignment
loss is proposed to minimize modality discrepancy on enhanced modality-shared
features. Extensive experiments on multiple public datasets demonstrate the
superiority of IDKL network over the state-of-the-art methods. Code is
available at https://github.com/1KK077/IDKL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可见-红外行人再识别中的隐式判别性知识学习</div>
<div class="mono" style="margin-top:8px">可见-红外行人再识别（VI-ReID）是一项具有挑战性的跨模态行人检索任务，由于不同摄像头间存在显著的类内变化和跨模态差异。现有工作主要集中在将不同模态的图像嵌入到统一空间中以挖掘模态共享特征，仅寻求这些共享特征中的显著信息，而忽略了隐含在模态特定特征中的身份感知有用信息。为解决这一问题，我们提出了一种新颖的隐式判别性知识学习（IDKL）网络，以揭示并利用模态特定特征中隐含的判别性信息。首先，我们使用一种新颖的双流网络提取模态特定和模态共享特征。然后，模态特定特征经过净化以减少其模态风格差异，同时保留身份感知的判别性知识。随后，这种隐性知识被提炼到模态共享特征中以增强其显著性。最后，提出了一种对齐损失以最小化增强模态共享特征的模态差异。在多个公开数据集上的广泛实验表明，IDKL网络优于现有方法。代码可在https://github.com/1KK077/IDKL获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve Visible-Infrared Person Re-identification (VI-ReID) by addressing the challenges of intra-class variations and cross-modal discrepancies. The proposed IDKL network uses a dual-stream approach to extract both modality-specific and shared features. It then purifies the modality-specific features to preserve discriminative identity information while reducing modality style discrepancies. This purified information is distilled into the shared features to enhance their distinctiveness. An alignment loss is introduced to minimize modality discrepancies. Experiments show that IDKL outperforms existing methods on multiple public datasets.</div>
<div class="mono" style="margin-top:8px">论文提出了一种新颖的隐式鉴别知识学习（IDKL）网络，以解决可见光-红外人像再识别（VI-ReID）的挑战。该网络使用双流架构提取模态特定和模态共享特征，然后净化模态特定特征以保留身份相关的鉴别性知识，并将这种知识转移到模态共享特征中以增强其独特性。引入了一种对齐损失来最小化模态间的差异。在多个公开数据集上的实验表明，IDKL在网络性能上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">YYDS: Visible-Infrared Person Re-Identification with Coarse Descriptions</div>
<div class="meta-line">Authors: Yunhao Du, Zhicheng Zhao, Fei Su</div>
<div class="meta-line">First: 2024-03-07T03:26:02+00:00 · Latest: 2024-03-07T03:26:02+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2403.04183v1">Abs</a> · <a href="http://arxiv.org/pdf/2403.04183v1">PDF</a> · <a href="https://github.com/dyhBUPT/YYDS">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) is challenging due to
considerable cross-modality discrepancies. Existing works mainly focus on
learning modality-invariant features while suppressing modality-specific ones.
However, retrieving visible images only depends on infrared samples is an
extreme problem because of the absence of color information. To this end, we
present the Refer-VI-ReID settings, which aims to match target visible images
from both infrared images and coarse language descriptions (e.g., &quot;a man with
red top and black pants&quot;) to complement the missing color information. To
address this task, we design a Y-Y-shape decomposition structure, dubbed YYDS,
to decompose and aggregate texture and color features of targets. Specifically,
the text-IoU regularization strategy is firstly presented to facilitate the
decomposition training, and a joint relation module is then proposed to infer
the aggregation. Furthermore, the cross-modal version of k-reciprocal
re-ranking algorithm is investigated, named CMKR, in which three neighbor
search strategies and one local query expansion method are explored to
alleviate the modality bias problem of the near neighbors. We conduct
experiments on SYSU-MM01, RegDB and LLCM datasets with our manually annotated
descriptions. Both YYDS and CMKR achieve remarkable improvements over SOTA
methods on all three datasets. Codes are available at
https://github.com/dyhBUPT/YYDS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>YYDS：可见-红外人体重识别与粗描述</div>
<div class="mono" style="margin-top:8px">可见-红外人体重识别（VI-ReID）由于跨模态差异显著而具有挑战性。现有工作主要集中在学习模态不变特征，抑制模态特定特征。然而，仅依靠红外样本检索可见图像是一个极端问题，因为缺乏颜色信息。为此，我们提出了Refer-VI-ReID设置，旨在通过结合红外图像和粗语言描述（例如，“一个穿红色上衣和黑色裤子的男人”）来匹配目标可见图像，以补充缺失的颜色信息。为了解决这一任务，我们设计了一种Y-Y形分解结构，称为YYDS，以分解和聚合目标的纹理和颜色特征。具体而言，首次提出了文本-IoU正则化策略以促进分解训练，然后提出了一种联合关系模块以推断聚合。此外，我们研究了跨模态版本的k-互逆重排序算法，称为CMKR，在其中探索了三种邻近搜索策略和一种局部查询扩展方法以缓解近邻的模态偏差问题。我们在SYSU-MM01、RegDB和LLCM数据集上使用我们手动标注的描述进行了实验。YYDS和CMKR在所有三个数据集上均显著优于当前最佳方法。代码可在https://github.com/dyhBUPT/YYDS获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of visible-infrared person re-identification by proposing a Refer-VI-ReID setting that incorporates coarse language descriptions to complement the lack of color information in infrared images. The authors introduce a Y-Y-shape decomposition structure, named YYDS, to decompose and aggregate texture and color features. They also develop a text-IoU regularization strategy and a joint relation module for feature aggregation. Additionally, a cross-modal version of k-reciprocal re-ranking algorithm, CMKR, is introduced to reduce modality bias. Experimental results on SYSU-MM01, RegDB, and LLCM datasets show significant improvements over state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">论文通过引入Refer-VI-ReID设置，结合粗粒度语言描述来补充红外图像中缺失的颜色信息，以解决可见光-红外人像再识别的挑战。作者提出了一种Y-Y形分解结构YYDS，用于分解和聚合纹理和颜色特征。还开发了文本-IoU正则化策略和联合关系模块进行特征聚合。此外，提出了一种跨模态的k-最近邻重排序算法版本CMKR，以减轻近邻的模态偏差问题。在SYSU-MM01、RegDB和LLCM数据集上的实验结果表明，YYDS和CMKR均显著优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Frequency Domain Modality-invariant Feature Learning for   Visible-infrared Person Re-Identification</div>
<div class="meta-line">Authors: Yulin Li, Tianzhu Zhang, Yongdong Zhang</div>
<div class="meta-line">First: 2024-01-03T17:11:27+00:00 · Latest: 2024-01-04T03:23:04+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2401.01839v2">Abs</a> · <a href="http://arxiv.org/pdf/2401.01839v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可见-红外人体重识别的频域模态不变特征学习</div>
<div class="mono" style="margin-top:8px">可见-红外人体重识别（VI-ReID）由于可见光和红外图像之间存在显著的跨模态差异而具有挑战性。虽然现有方法集中在设计复杂的网络架构或使用度量学习约束来学习模态不变特征，但它们往往忽略了导致模态差异问题的具体图像成分。在本文中，我们首先揭示可见光和红外图像在幅度成分上的差异是主要因素，进一步提出了一种新颖的频域模态不变特征学习框架（FDMNet），从频域的角度减少模态差异。我们的框架引入了两个新颖模块，即实例自适应幅度滤波器（IAF）模块和短语保留归一化（PPNorm）模块，以增强模态不变的幅度成分并抑制图像和特征层面的模态特定成分。在SYSU-MM01和RegDB两个标准基准上的广泛实验结果表明，我们的FDMNet在与最先进的方法相比时表现出更优的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of visible-infrared person re-identification by focusing on the amplitude difference between visible and infrared images. It proposes a Frequency Domain modality-invariant feature learning framework (FDMNet) that includes an Instance-Adaptive Amplitude Filter (IAF) module and a Phrase-Preserving Normalization (PPNorm) module to enhance the modality-invariant amplitude component and suppress the modality-specific component. Experiments on SYSU-MM01 and RegDB benchmarks show that FDMNet outperforms existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">论文针对可见光-红外人像再识别中的模态差异问题，关注可见光和红外图像的幅度差异。提出了一种频域模态不变特征学习框架（FDMNet），引入了实例自适应幅度滤波器（IAF）和短语保持归一化（PPNorm）模块，以增强不变的幅度成分并抑制特定成分。在SYSU-MM01和RegDB标准基准上的实验表明，FDMNet在该任务中优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Transferring Modality-Aware Pedestrian Attentive Learning for   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yuwei Guo, Wenhao Zhang, Licheng Jiao, Shuang Wang, Shuo Wang, Fang Liu</div>
<div class="meta-line">First: 2023-12-12T07:15:17+00:00 · Latest: 2023-12-19T02:46:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2312.07021v2">Abs</a> · <a href="http://arxiv.org/pdf/2312.07021v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared person re-identification (VI-ReID) aims to search the same
pedestrian of interest across visible and infrared modalities. Existing models
mainly focus on compensating for modality-specific information to reduce
modality variation. However, these methods often lead to a higher computational
overhead and may introduce interfering information when generating the
corresponding images or features. To address this issue, it is critical to
leverage pedestrian-attentive features and learn modality-complete and
-consistent representation. In this paper, a novel Transferring Modality-Aware
Pedestrian Attentive Learning (TMPA) model is proposed, focusing on the
pedestrian regions to efficiently compensate for missing modality-specific
features. Specifically, we propose a region-based data augmentation module
PedMix to enhance pedestrian region coherence by mixing the corresponding
regions from different modalities. A lightweight hybrid compensation module,
i.e., the Modality Feature Transfer (MFT), is devised to integrate cross
attention and convolution networks to fully explore the discriminative
modality-complete features with minimal computational overhead. Extensive
experiments conducted on the benchmark SYSU-MM01 and RegDB datasets
demonstrated the effectiveness of our proposed TMPA model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模态感知的行人注意力学习在可见光-红外行人再识别中的迁移</div>
<div class="mono" style="margin-top:8px">可见光-红外行人再识别（VI-ReID）旨在跨可见光和红外模态搜索同一行人。现有模型主要集中在补偿模态特定信息以减少模态差异。然而，这些方法往往导致更高的计算开销，并可能在生成相应图像或特征时引入干扰信息。为解决这一问题，关键在于利用行人注意力特征并学习模态完整且一致的表示。本文提出了一种新颖的基于模态感知的行人注意力学习（TMPA）模型，专注于行人区域以高效地补偿缺失的模态特定特征。具体地，我们提出了一种基于区域的数据增强模块PedMix，通过从不同模态混合相应的区域来增强行人区域的一致性。设计了一种轻量级的混合补偿模块，即模态特征转移（MFT），结合跨注意力和卷积网络，以最小的计算开销全面探索判别性的模态完整特征。在基准数据集SYSU-MM01和RegDB上的广泛实验表明了我们提出的TMPA模型的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of visible-infrared person re-identification by proposing a novel Transferring Modality-Aware Pedestrian Attentive Learning (TMPA) model. The model focuses on pedestrian regions to compensate for missing modality-specific features and includes a PedMix module for enhancing coherence and a lightweight MFT module for integrating cross-modal features. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the TMPA model in reducing modality variation without increasing computational overhead.</div>
<div class="mono" style="margin-top:8px">论文旨在通过解决模态差异问题来提高可见光-红外行人再识别的性能。提出了一个TMPA模型，专注于行人区域来补偿缺失的模态特定特征。该模型包括一个PedMix模块以增强不同模态之间的连贯性，以及一个MFT模块以结合跨注意力和卷积网络来探索具有最小计算开销的判别性特征。在SYSU-MM01和RegDB数据集上的实验表明了所提出方法的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
