<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-27 03:22</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251027_0322</div>
    <div class="row"><div class="card">
<div class="title">8-Calves Image dataset</div>
<div class="meta-line">Authors: Xuyang Fang, Sion Hannuna, Neill Campbell, Edwin Simpson</div>
<div class="meta-line">First: 2025-03-17T23:47:52+00:00 · Latest: 2025-10-22T22:10:21+00:00</div>
<div class="meta-line">Comments: 16 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.13777v3">Abs</a> · <a href="http://arxiv.org/pdf/2503.13777v3">PDF</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated livestock monitoring is crucial for precision farming, but robust
computer vision models are hindered by a lack of datasets reflecting real-world
group challenges. We introduce the 8-Calves dataset, a challenging benchmark
for multi-animal detection, tracking, and identification. It features a
one-hour video of eight Holstein Friesian calves in a barn, with frequent
occlusions, motion blur, and diverse poses. A semi-automated pipeline using a
fine-tuned YOLOv8 detector and ByteTrack, followed by manual correction,
provides over 537,000 bounding boxes with temporal identity labels.
  We benchmark 28 object detectors, showing near-perfect performance on a
lenient IoU threshold (mAP50: 95.2-98.9%) but significant divergence on
stricter metrics (mAP50:95: 56.5-66.4%), highlighting fine-grained localization
challenges. Our identification benchmark across 23 models reveals a trade-off:
scaling model size improves classification accuracy but compromises retrieval.
Smaller architectures like ConvNextV2 Nano achieve the best balance (73.35%
accuracy, 50.82% Top-1 KNN). Pre-training focused on semantic learning (e.g.,
BEiT) yielded superior transferability. For tracking, leading methods achieve
high detection accuracy (MOTA &gt; 0.92) but struggle with identity preservation
(IDF1 $\approx$ 0.27), underscoring a key challenge in occlusion-heavy
scenarios.
  The 8-Calves dataset bridges a gap by providing temporal richness and
realistic challenges, serving as a resource for advancing agricultural vision
models. The dataset and code are available at
https://huggingface.co/datasets/tonyFang04/8-calves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>8头小牛图像数据集</div>
<div class="mono" style="margin-top:8px">精准农业中自动牲畜监控至关重要，但稳健的计算机视觉模型受限于缺乏反映现实世界群组挑战的数据集。我们介绍了8-Calves数据集，这是一个具有挑战性的多动物检测、跟踪和识别基准。该数据集包含一个农场中八头荷斯坦-弗里斯兰小牛的一小时视频，其中包含频繁的遮挡、运动模糊和多样的姿态。使用微调后的YOLOv8检测器和ByteTrack的半自动化管道，随后进行手动校正，提供了超过537,000个带有时间身份标签的边界框。我们对28种对象检测器进行了基准测试，显示在宽松的IoU阈值下（mAP50：95.2-98.9%）接近完美性能，但在更严格的指标下（mAP50:95：56.5-66.4%）存在显著差异，突显了精细定位的挑战。在23个模型的识别基准测试中揭示了权衡：模型规模的扩大提高了分类准确性，但牺牲了检索性能。ConvNextV2 Nano等较小的架构实现了最佳平衡（准确率73.35%，Top-1 KNN 50.82%）。专注于语义学习的预训练（例如，BEiT）表现出更优的迁移性能。在跟踪方面，领先方法实现了高检测准确性（MOTA &gt; 0.92），但在身份保留方面遇到困难（IDF1 ≈ 0.27），突显了遮挡密集场景中的关键挑战。8-Calves数据集通过提供时间丰富性和现实挑战，填补了空白，为推进农业视觉模型提供了资源。数据集和代码可在https://huggingface.co/datasets/tonyFang04/8-calves/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The 8-Calves dataset was created to address the lack of real-world challenges in livestock monitoring datasets. It includes a one-hour video of eight calves with frequent occlusions and motion blur. The dataset was used to benchmark 28 object detectors, showing near-perfect performance on lenient metrics but significant challenges on stricter ones. For identification, smaller architectures like ConvNextV2 Nano performed best, and pre-training focused on semantic learning improved transferability. Tracking methods achieved high detection accuracy but struggled with identity preservation in occlusion-heavy scenarios.</div>
<div class="mono" style="margin-top:8px">8-Calves数据集旨在解决 livestock 监控数据集中缺乏真实挑战的问题。该数据集包含一段八头小牛的一小时视频，其中包含频繁的遮挡和运动模糊。该数据集用于评估28种目标检测器，结果显示在宽松的指标上表现近乎完美，但在严格指标上存在显著挑战。在识别方面，ConvNextV2 Nano等较小的架构表现最佳，而专注于语义学习的预训练提高了迁移性。跟踪方法在检测准确性上表现良好，但在遮挡密集场景中难以保持身份识别。</div>
</details>
</div>
<div class="card">
<div class="title">Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical   Mitosis Classification</div>
<div class="meta-line">Authors: Mieko Ochi, Bae Yuan</div>
<div class="meta-line">First: 2025-08-29T03:24:57+00:00 · Latest: 2025-09-18T10:00:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.02591v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.02591v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mitotic figures are classified into typical and atypical variants, with
atypical counts correlating strongly with tumor aggressiveness. Accurate
differentiation is therefore essential for patient prognostication and resource
allocation, yet remains challenging even for expert pathologists. Here, we
leveraged Pathology Foundation Models (PFMs) pre-trained on large
histopathology datasets and applied parameter-efficient fine-tuning via
low-rank adaptation. In addition, we incorporated ConvNeXt V2, a
state-of-the-art convolutional neural network architecture, to complement PFMs.
During training, we employed a fisheye transform to emphasize mitoses and
Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled
multiple PFMs to integrate complementary morphological insights, achieving
competitive balanced accuracy on the Preliminary Evaluation Phase dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIDOG 2025 轨道2：非典型分裂分类的病理基础模型集成</div>
<div class="mono" style="margin-top:8px">分裂象被分类为典型和非典型变体，非典型计数与肿瘤侵袭性密切相关。因此，准确区分对于患者的预后评估和资源分配至关重要，即使是专家病理学家也面临挑战。我们利用在大规模组织病理学数据集上预训练的病理基础模型（PFMs），并通过低秩适应进行参数高效的微调。此外，我们引入了最先进的卷积神经网络架构ConvNeXt V2来补充PFMs。在训练过程中，我们使用鱼眼变换强调分裂象，并使用ImageNet目标图像进行频域适应。最后，我们集成多个PFMs以整合互补的形态学见解，在初步评估阶段数据集上实现了竞争力的平衡准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to accurately classify mitotic figures into typical and atypical variants, which is crucial for patient prognosis. The authors used Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and fine-tuned them with low-rank adaptation. They also incorporated ConvNeXt V2 and used a fisheye transform and Fourier Domain Adaptation for training. The ensemble of PFMs achieved competitive balanced accuracy on the Preliminary Evaluation Phase dataset.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提高非典型分裂细胞分类的准确性，从而改善患者的预后和资源分配。方法包括使用在大规模组织病理学数据集上预训练的Pathology Foundation Models (PFMs)，通过低秩适应进行微调，并引入ConvNeXt V2。此外，在训练过程中使用了鱼眼变换和Fourier域适应。最后，通过集成多个PFMs来整合互补的形态学见解，从而在初步评估阶段的数据集上获得了竞争力的平衡准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2025-09-15T05:10:43+00:00 · Latest: 2025-09-15T05:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.11587v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.11587v1">PDF</a> · <a href="https://github.com/haonanshi0125/HIL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外行人重识别的分层身份学习</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在通过减少模态差异并最小化昂贵的手动注释依赖性，从跨模态的无标签行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来处理USVI-ReID，通过单个聚类中心表示一个人。然而，它们主要关注每个聚类内的图像共性，而忽视了它们之间的细微差异。为了解决这一局限性，我们提出了一种分层身份学习（HIL）框架。由于每个聚类可能包含多个反映图像间细微差异的较小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习（MCCL）以细化表示，增强同模态聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输（BRST）机制，通过双向匹配伪标签来建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法优于现有方法。源代码可在：https://github.com/haonanshi0125/HIL 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of existing methods that focus on commonalities within clusters. The Hierarchical Identity Learning (HIL) framework is proposed, which generates multiple memories for each cluster through secondary clustering and uses Multi-Center Contrastive Learning (MCCL) to enhance intra-modal clustering and reduce cross-modal discrepancies. The Bidirectional Reverse Selection Transmission (BRST) mechanism further improves cross-modal matching. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing approaches.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有方法忽视细粒度差异的问题，提高无监督可见-红外行人重识别的性能。提出了层次身份学习（HIL）框架，为每个簇生成多个记忆，并使用多中心对比学习（MCCL）来细化表示并减少跨模态差异。双向反向选择传输（BRST）机制进一步提高了跨模态匹配质量。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for   Gastric Tissue Classification</div>
<div class="meta-line">Authors: Mustafa Yurdakul, Sakir Tasdemir</div>
<div class="meta-line">First: 2025-09-11T08:24:50+00:00 · Latest: 2025-09-11T08:24:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.09242v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.09242v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoAtNeX:一种基于注意力增强的ConvNeXtV2-Transformer混合模型用于胃组织分类</div>
<div class="mono" style="margin-top:8px">背景与目的早期诊断胃病对于预防致命结果至关重要。尽管组织病理学检查仍然是诊断的金标准，但其完全由手工操作，使得评估劳动密集且容易受到病理学家之间变异的影响。关键发现可能被遗漏，缺乏标准化程序降低了一致性。这些限制突显了需要自动化、可靠且高效的胃组织分析方法。方法在本研究中，提出了一种名为CoAtNeXt的新型混合模型，用于胃组织图像分类。该模型基于CoAtNet架构，通过用增强的ConvNeXtV2块替换其MBConv层来构建。此外，还集成了卷积块注意力模块（CBAM），以通过通道和空间注意力机制提高局部特征提取。该架构被扩展以在计算效率和分类性能之间取得平衡。CoAtNeXt在两个公开可用的数据集HMU-GC-HE-30K（用于八类分类）和GasHisSDB（用于二类分类）上进行了评估，并与10种卷积神经网络（CNN）和10种视觉变换器（ViT）模型进行了比较。结果CoAtNeXt在HMU-GC-HE-30K上达到了96.47%的准确率、96.60%的精确率、96.47%的召回率、96.45%的F1分数和99.89%的AUC。在GasHisSDB上，它达到了98.29%的准确率、98.07%的精确率、98.41%的召回率、98.23%的F1分数和99.90%的AUC。它在所有测试的CNN和ViT模型中表现最佳，并超越了文献中的先前研究。结论实验结果表明，CoAtNeXt是一种稳健的架构，适用于胃组织图像的组织病理学分类，提供二类和多类的性能。它突显了其通过提高诊断准确性和减轻工作量来辅助病理学家的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study proposes CoAtNeXt, a hybrid model combining CoAtNet and enhanced ConvNeXtV2 blocks with CBAM for gastric tissue classification. Evaluated on HMU-GC-HE-30K and GasHisSDB datasets, CoAtNeXt achieved high accuracy, precision, recall, F1 score, and AUC, outperforming 20 comparison models and previous studies.</div>
<div class="mono" style="margin-top:8px">研究旨在通过胃组织图像分类实现胃病的早期诊断，提出了一种结合CoAtNet和ConvNeXtV2并集成CBAM以增强特征提取的CoAtNeXt模型。该模型在两个数据集上的评估结果表明，其在准确率、精确率、召回率、F1分数和AUC值等方面均优于10种CNN和10种ViT模型。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Classification of Normal and Atypical Mitotic Figures Using   ConvNeXt V2: MIDOG 2025 Track 2</div>
<div class="meta-line">Authors: Yosuke Yamagishi, Shouhei Hanaoka</div>
<div class="meta-line">First: 2025-08-26T09:11:12+00:00 · Latest: 2025-08-26T09:11:12+00:00</div>
<div class="meta-line">Comments: MIDOG 2025 solution</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.18831v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.18831v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents our solution for the MIDOG 2025 Challenge Track 2, which
focuses on binary classification of normal mitotic figures (NMFs) versus
atypical mitotic figures (AMFs) in histopathological images. Our approach
leverages a ConvNeXt V2 base model with center cropping preprocessing and
5-fold cross-validation ensemble strategy. The method addresses key challenges
including severe class imbalance, high morphological variability, and domain
heterogeneity across different tumor types, species, and scanners. Through
strategic preprocessing with 60% center cropping and mixed precision training,
our model achieved robust performance on the diverse MIDOG 2025 dataset. The
solution demonstrates the effectiveness of modern convolutional architectures
for mitotic figure subtyping while maintaining computational efficiency through
careful architectural choices and training optimizations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用ConvNeXt V2自动分类正常和异常分裂象：MIDOG 2025赛道2</div>
<div class="mono" style="margin-top:8px">本文介绍了我们对MIDOG 2025挑战赛赛道2的解决方案，该挑战专注于在组织病理学图像中对正常分裂象（NMFs）与异常分裂象（AMFs）进行二分类。我们的方法利用了ConvNeXt V2基础模型，采用中心裁剪预处理和5折交叉验证集成策略。该方法解决了包括严重类别不平衡、高形态学变异性以及不同肿瘤类型、物种和扫描器之间的领域异质性在内的关键挑战。通过使用60%中心裁剪和混合精度训练的战略性预处理，我们的模型在多样的MIDOG 2025数据集上实现了稳健的性能。该解决方案展示了现代卷积架构在分裂象亚型分类中的有效性，同时通过精心选择的架构和训练优化保持了计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a solution for the MIDOG 2025 Challenge Track 2, focusing on classifying normal and atypical mitotic figures using a ConvNeXt V2 model with center cropping preprocessing and 5-fold cross-validation. The method addresses challenges such as class imbalance and morphological variability, achieving robust performance on the dataset. Key findings include the effectiveness of ConvNeXt V2 and the benefits of 60% center cropping and mixed precision training.</div>
<div class="mono" style="margin-top:8px">本文提出了一个针对MIDOG 2025挑战赛第二赛道的解决方案，使用ConvNeXt V2模型结合中心裁剪预处理和5折交叉验证方法来区分正常分裂图和异常分裂图。该方法通过采用战略性的预处理和混合精度训练来应对类别不平衡和形态学变异性等挑战，实现了在数据集上的稳健性能。该解决方案突显了现代卷积架构在分裂图亚型分类中的有效性，同时保持了计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Multi-label Classification of Eleven Retinal Diseases: A   Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic   Dataset</div>
<div class="meta-line">Authors: Jerry Cao-Xue, Tien Comlekoglu, Keyi Xue, Guanliang Wang, Jiang Li, Gordon Laurie</div>
<div class="meta-line">First: 2025-08-21T22:09:53+00:00 · Latest: 2025-08-21T22:09:53+00:00</div>
<div class="meta-line">Comments: 25 pages, 6 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15986v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15986v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于现代架构的大规模合成数据集上十一种视网膜疾病自动多标签分类：基准测试与元集成</div>
<div class="mono" style="margin-top:8px">由于患者隐私问题和高昂的成本，开发用于视网膜疾病分类的多标签深度学习模型往往受到大型专家注释临床数据集稀缺的阻碍。最近发布的SynFundus-1M，一个高保真度的合成数据集，包含超过一百万张眼底图像，为克服这些障碍提供了新的机会。为了为这一新资源建立基础性能基准，我们开发了一个端到端的深度学习管道，使用五折多标签分层交叉验证策略，训练六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型）来分类十一种视网膜疾病。我们进一步开发了一个元集成模型，通过堆叠五折外折预测和XGBoost分类器。最终集成模型在内部验证集上表现最佳，宏平均受试者操作特征曲线下面积（AUC）为0.9973。关键的是，模型在三个不同真实世界临床数据集上表现出强大的泛化能力，DR数据集上的AUC为0.7972，AIROGS青光眼数据集上的AUC为0.9126，多标签RFMiD数据集上的宏AUC为0.8800。这项工作为未来在大规模合成数据集上的研究提供了稳健的基准，并证明了仅在合成数据上训练的模型能够准确分类多种病理并有效泛化到真实临床图像，为加速眼科全面AI系统的开发提供了可行途径。</div>
</details>
</div>
<div class="card">
<div class="title">RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in   Streetscape Images from Open Government Metadata</div>
<div class="meta-line">Authors: John S. O&#x27;Meara, Jared Hwang, Zeyu Wang, Michael Saugstad, Jon E. Froehlich</div>
<div class="meta-line">Venue: ICCV</div>
<div class="meta-line">First: 2025-08-13T01:22:48+00:00 · Latest: 2025-08-13T01:22:48+00:00</div>
<div class="meta-line">Comments: Accepted to the ICCV&#x27;25 Workshop on Vision Foundation Models and
  Generative AI for Accessibility: Challenges and Opportunities</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.09415v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.09415v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RampNet：街道景观图像中基于开放政府元数据自举坡道检测的两阶段管道</div>
<div class="mono" style="margin-top:8px">坡道对于城市无障碍通行至关重要，但在图像中稳健地检测它们仍然是一个开放问题，因为缺乏大规模、高质量的数据集。尽管先前的工作尝试通过众包或手动标注数据来提高数据可用性，但这些努力往往在质量和规模上都存在不足。在本文中，我们介绍并评估了一个名为RampNet的两阶段管道，以扩大坡道检测数据集并提高模型性能。在第一阶段，我们通过自动将政府提供的坡道位置数据翻译为全景图像的像素坐标，生成了超过210,000个标注的Google街景（GSV）全景图像数据集。在第二阶段，我们从生成的数据集训练了一个坡道检测模型（修改后的ConvNeXt V2），实现了最先进的性能。为了评估我们管道的两个阶段，我们将其与手动标注的全景图像进行了比较。我们生成的数据集达到了94.0%的精确率和92.5%的召回率，我们的检测模型达到了0.9236的AP，远超先前的工作。我们的工作贡献了第一个大规模、高质量的坡道检测数据集、基准和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RampNet is a two-stage pipeline designed to enhance curb ramp detection in street images. In Stage 1, the authors auto-translate government-provided curb ramp location data into pixel coordinates in panoramic images, generating a dataset of over 210,000 annotated panoramas. In Stage 2, they train a modified ConvNeXt V2 model on this dataset, achieving state-of-the-art performance with 0.9236 AP. The generated dataset and model significantly outperform previous work, with 94.0% precision and 92.5% recall, and contribute the first large-scale, high-quality curb ramp detection dataset and benchmark.</div>
<div class="mono" style="margin-top:8px">RampNet 是一个两阶段管道，旨在增强街道图像中的坡道检测。第一阶段通过将政府提供的坡道位置数据自动翻译成全景图像中的像素坐标，生成超过 210,000 张标注图像的数据库。第二阶段在生成的数据库上训练一个修改后的 ConvNeXt V2 模型，达到了 94.0% 的精确率、92.5% 的召回率和 0.9236 的 AP，显著超越了先前的工作。这项工作提供了第一个大规模、高质量的坡道检测数据集、基准和模型。</div>
</details>
</div>
<div class="card">
<div class="title">WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop   Prediction in Integrated Circuit Design</div>
<div class="meta-line">Authors: Youngmin Seo, Yunhyeong Kwon, Younghun Park, HwiRyong Kim, Seungho Eum, Jinha Kim, Taigon Song, Juho Kim, Unsang Park</div>
<div class="meta-line">First: 2025-07-25T12:07:16+00:00 · Latest: 2025-07-25T12:07:16+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.19197v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.19197v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate spatial prediction of power integrity issues, such as IR drop, is
critical for reliable VLSI design. However, traditional simulation-based
solvers are computationally expensive and difficult to scale. We address this
challenge by reformulating IR drop estimation as a pixel-wise regression task
on heterogeneous multi-channel physical maps derived from circuit layouts.
Prior learning-based methods treat all input layers (e.g., metal, via, and
current maps) equally, ignoring their varying importance to prediction
accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention
(WACA) mechanism, which recursively enhances weak feature channels while
suppressing over-dominant ones through a two-stage gating strategy. Integrated
into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and
balanced feature representation. On the public ICCAD-2023 benchmark, our method
outperforms the ICCAD-2023 contest winner by reducing mean absolute error by
61.1% and improving F1-score by 71.0%. These results demonstrate that
channel-wise heterogeneity is a key inductive bias in physical layout analysis
for VLSI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WACA-UNet: 弱性意识通道注意机制在集成电路设计中静态IR降预测</div>
<div class="mono" style="margin-top:8px">准确的空间预测电力完整性问题，如IR降，对于可靠的VLSI设计至关重要。然而，传统的基于仿真的求解器计算成本高昂且难以扩展。我们通过将IR降估计重新表述为基于异构多通道物理图的像素级回归任务来应对这一挑战。先前的学习方法将所有输入层（例如，金属、通孔和电流图）视为同等重要，忽略了它们对预测准确性的不同重要性。为了解决这一问题，我们提出了一种新颖的弱性意识通道注意（WACA）机制，该机制通过两阶段门控策略递归增强弱特征通道并抑制主导通道。将该机制集成到基于ConvNeXtV2的注意力U-Net中，我们的方法能够实现自适应和平衡的特征表示。在公开的ICCAD-2023基准测试中，我们的方法通过降低平均绝对误差61.1%和提高F1分数71.0%击败了ICCAD-2023竞赛的获胜者。这些结果表明，在VLSI的物理布局分析中，通道级异质性是关键的归纳偏置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of accurately predicting IR drop in VLSI design through a pixel-wise regression task on multi-channel physical maps. It introduces a Weakness-Aware Channel Attention (WACA) mechanism that enhances weak feature channels and suppresses dominant ones, integrated into a ConvNeXtV2-based attention U-Net. The method significantly outperforms the ICCAD-2023 contest winner, reducing mean absolute error by 61.1% and improving F1-score by 71.0%.</div>
<div class="mono" style="margin-top:8px">论文通过在多通道物理图上进行像素级回归任务，解决VLSI设计中的IR降准确预测问题。提出WACA-UNet，结合弱特征感知通道注意机制，增强弱特征通道并抑制主导通道，提高模型的适应性和平衡性。该方法在ICCAD-2023竞赛中胜出，显著降低了均方绝对误差并提高了F1分数。</div>
</details>
</div>
<div class="card">
<div class="title">Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal   Imaging Exams</div>
<div class="meta-line">Authors: Leonor Fernandes, Tiago Gonçalves, João Matos, Luis Filipe Nakayama, Jaime S. Cardoso</div>
<div class="meta-line">First: 2025-07-13T14:11:41+00:00 · Latest: 2025-07-13T14:11:41+00:00</div>
<div class="meta-line">Comments: 10 pages. Under review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.09640v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.09640v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>眼科视网膜成像检查中短路和解缠的分离与评估</div>
<div class="mono" style="margin-top:8px">糖尿病视网膜病变（DR）是工作年龄段成人视力丧失的主要原因。虽然筛查可以降低失明的风险，但传统成像往往成本高且难以获得。人工智能（AI）算法提供了一种可扩展的诊断解决方案，但公平性和泛化性的问题仍然存在。本研究评估了图像训练模型在DR预测中的公平性和性能，以及解缠作为偏见缓解技术的影响，使用了多BRSET眼底数据集。三种模型，ConvNeXt V2、DINOv2和Swin V2，均在黄斑图像上训练以预测DR和敏感属性（如年龄和性别/性别），并评估了公平性，应用了解缠以减少偏见。所有模型在诊断DR方面均表现出高预测性能（最高达94% AUCROC），并且能够合理预测年龄和性别/性别（分别为91%和77% AUCROC）。公平性评估表明，如DINOv2在年龄组之间存在10%的AUCROC差距。从DR预测中分离敏感属性的效果因所选模型而异。解缠提高了DINOv2的性能（AUCROC提升2%），但在ConvNeXt V2和Swin V2中导致性能下降（分别下降7%和3%）。这些发现突显了在眼底成像中分离细粒度特征的复杂性，并强调了医疗成像AI中的公平性的重要性，以确保公平可靠的医疗保健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study evaluates the fairness and performance of AI models in predicting diabetic retinopathy (DR) using fundus images, with a focus on disentanglement as a bias mitigation technique. Three models—ConvNeXt V2, DINOv2, and Swin V2—were trained to predict DR and sensitive attributes like age and gender. While all models showed high performance in DR prediction and age/gender prediction, disentanglement had mixed results, improving DINOv2 but reducing performance in ConvNeXt V2 and Swin V2. The study highlights the challenges in disentangling fine-grained features and the importance of fairness in medical imaging AI.</div>
<div class="mono" style="margin-top:8px">该研究评估了AI模型在使用眼底图像预测糖尿病视网膜病变（DR）方面的公平性和性能，并重点关注解缠作为偏见缓解技术。三种模型（ConvNeXt V2、DINOv2和Swin V2）被训练来预测DR和年龄、性别等敏感属性。尽管所有模型在DR预测和年龄/性别预测方面都表现出高水平的性能，但解缠效果参差不齐，提高了DINOv2的性能，但在ConvNeXt V2和Swin V2中则降低了性能。研究强调了解缠细粒度特征的挑战，并强调了公平的医疗成像AI对于确保公平可靠的医疗服务的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Event Detection: Current Approaches and Defining the New   Playground through LLMs and VLMs</div>
<div class="meta-line">Authors: Abhishek Dey, Aabha Bothera, Samhita Sarikonda, Rishav Aryan, Sanjay Kumar Podishetty, Akshay Havalgi, Gaurav Singh, Saurabh Srivastava</div>
<div class="meta-line">First: 2025-05-16T04:07:21+00:00 · Latest: 2025-05-16T04:07:21+00:00</div>
<div class="meta-line">Comments: Accepted at NLDB 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.10836v1">Abs</a> · <a href="http://arxiv.org/pdf/2505.10836v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we study the challenges of detecting events on social media,
where traditional unimodal systems struggle due to the rapid and multimodal
nature of data dissemination. We employ a range of models, including unimodal
ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced
generative models like GPT-4o, and LLaVA. Additionally, we also study the
effect of providing multimodal generative models (such as GPT-4o) with a single
modality to assess their efficacy. Our results indicate that while multimodal
approaches notably outperform unimodal counterparts, generative approaches
despite having a large number of parameters, lag behind supervised methods in
precision. Furthermore, we also found that they lag behind instruction-tuned
models because of their inability to generate event classes correctly. During
our error analysis, we discovered that common social media issues such as leet
speak, text elongation, etc. are effectively handled by generative approaches
but are hard to tackle using supervised approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态事件检测：当前方法与通过LLMs和VLMs定义的新领域</div>
<div class="mono" style="margin-top:8px">在本文中，我们研究了在社交媒体上检测事件的挑战，由于数据传播的快速和多模态性，传统的单模态系统难以应对。我们使用了包括单模态ModernBERT和ConvNeXt-V2、多模态融合技术和先进的生成模型如GPT-4o和LLaVA等一系列模型。此外，我们还研究了向单模态提供多模态生成模型（如GPT-4o）的效果，以评估其有效性。结果显示，尽管多模态方法显著优于单模态方法，但生成方法尽管参数量庞大，但在精确度上仍落后于监督方法。此外，我们还发现，由于无法正确生成事件类别，它们落后于指令调优模型。在我们的错误分析中，我们发现常见的社交媒体问题如leetspeak、文本拉长等，生成方法能够有效处理，但难以通过监督方法解决。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of detecting events on social media using traditional unimodal systems, which struggle with the rapid and multimodal nature of data. It employs various models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o and LLaVA. The study finds that multimodal approaches outperform unimodal ones, but generative models, despite their large parameter count, fall short in precision and fail to generate event classes correctly, which instruction-tuned models can handle more effectively. The research also notes that generative models are better at handling common social media issues like leet speak and text elongation compared to supervised models.</div>
<div class="mono" style="margin-top:8px">本文研究了在社交媒体上检测事件的挑战，传统单模态系统难以应对数据的快速和多模态特性。研究使用了包括ModernBERT、ConvNeXt-V2、多模态融合技术和生成模型如GPT-4o和LLaVA等多种模型。研究发现，多模态方法显著优于单模态方法，但生成模型尽管参数量大，但在精度上落后于监督方法。此外，生成模型在生成正确事件类别方面不如指令调优模型有效。错误分析表明，生成模型在处理常见的社交媒体问题方面优于监督模型。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251026_0325.html">20251026_0325</a>
<a href="archive/20251025_0327.html">20251025_0327</a>
<a href="archive/20251024_0327.html">20251024_0327</a>
<a href="archive/20251023_0327.html">20251023_0327</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0329.html">20251021_0329</a>
<a href="archive/20251020_0318.html">20251020_0318</a>
<a href="archive/20251019_0327.html">20251019_0327</a>
<a href="archive/20251018_0326.html">20251018_0326</a>
<a href="archive/20251017_0325.html">20251017_0325</a>
<a href="archive/20251016_0321.html">20251016_0321</a>
<a href="archive/20251015_0327.html">20251015_0327</a>
<a href="archive/20251014_0326.html">20251014_0326</a>
<a href="archive/20251012_0325.html">20251012_0325</a>
<a href="archive/20251011_0327.html">20251011_0327</a>
<a href="archive/20251010_0328.html">20251010_0328</a>
<a href="archive/20251009_0319.html">20251009_0319</a>
<a href="archive/20251008_0344.html">20251008_0344</a>
<a href="archive/20251007_0345.html">20251007_0345</a>
<a href="archive/20251006_0350.html">20251006_0350</a>
<a href="archive/20251005_0349.html">20251005_0349</a>
<a href="archive/20251004_0351.html">20251004_0351</a>
<a href="archive/20251003_0351.html">20251003_0351</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0340.html">20251001_0340</a>
<a href="archive/20250930_0357.html">20250930_0357</a>
<a href="archive/20250929_0354.html">20250929_0354</a>
<a href="archive/20250928_1405.html">20250928_1405</a>
<a href="archive/20250928_0338.html">20250928_0338</a>
<a href="archive/20250927_2233.html">20250927_2233</a>
<a href="archive/20250925_0328.html">20250925_0328</a>
<a href="archive/20250924_0337.html">20250924_0337</a>
<a href="archive/20250923_0336.html">20250923_0336</a>
<a href="archive/20250922_0334.html">20250922_0334</a>
<a href="archive/20250921_0333.html">20250921_0333</a>
<a href="archive/20250920_0334.html">20250920_0334</a>
<a href="archive/20250919_1904.html">20250919_1904</a>
<a href="archive/20250919_1023.html">20250919_1023</a>
<a href="archive/20250919_0332.html">20250919_0332</a>
<a href="archive/20250918_2047.html">20250918_2047</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250918_0326.html">20250918_0326</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250917_0326.html">20250917_0326</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250916_0328.html">20250916_0328</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
