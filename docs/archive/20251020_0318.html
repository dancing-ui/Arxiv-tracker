<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-20 03:18</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251020_0318</div>
    <div class="row"><div class="card">
<div class="title">Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical   Mitosis Classification</div>
<div class="meta-line">Authors: Mieko Ochi, Bae Yuan</div>
<div class="meta-line">First: 2025-08-29T03:24:57+00:00 · Latest: 2025-09-18T10:00:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.02591v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.02591v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mitotic figures are classified into typical and atypical variants, with
atypical counts correlating strongly with tumor aggressiveness. Accurate
differentiation is therefore essential for patient prognostication and resource
allocation, yet remains challenging even for expert pathologists. Here, we
leveraged Pathology Foundation Models (PFMs) pre-trained on large
histopathology datasets and applied parameter-efficient fine-tuning via
low-rank adaptation. In addition, we incorporated ConvNeXt V2, a
state-of-the-art convolutional neural network architecture, to complement PFMs.
During training, we employed a fisheye transform to emphasize mitoses and
Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled
multiple PFMs to integrate complementary morphological insights, achieving
competitive balanced accuracy on the Preliminary Evaluation Phase dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIDOG 2025 轨道2：非典型分裂分类的病理基础模型集成</div>
<div class="mono" style="margin-top:8px">分裂像被分类为典型和非典型变体，非典型计数与肿瘤侵袭性密切相关。因此，准确区分对于患者的预后评估和资源分配至关重要，即使是专家病理学家也面临挑战。我们利用在大量组织病理学数据上预训练的病理基础模型（PFMs），并通过低秩适应进行参数高效的微调。此外，我们引入了最先进的卷积神经网络架构ConvNeXt V2来补充PFMs。在训练过程中，我们使用鱼眼变换强调分裂像，并使用ImageNet目标图像进行频域适应。最后，我们集成多个PFMs以整合互补的形态学见解，在初步评估阶段数据集上实现了竞争力的平衡准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2025-09-15T05:10:43+00:00 · Latest: 2025-09-15T05:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.11587v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.11587v1">PDF</a> · <a href="https://github.com/haonanshi0125/HIL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外行人重识别的分层身份学习</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在通过减少模态差异并最小化昂贵的手动注释依赖性，从跨模态的无标签行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来解决USVI-ReID问题，通过单个聚类中心表示一个人。然而，它们主要关注每个聚类内的图像共性，而忽视了它们之间的细微差异。为了解决这一局限性，我们提出了一种分层身份学习（HIL）框架。由于每个聚类可能包含多个反映图像间细微差异的较小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习（MCCL）以细化表示，增强同模态聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输（BRST）机制，通过双向匹配伪标签来建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法优于现有方法。源代码可在：https://github.com/haonanshi0125/HIL 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of existing cluster-based methods. It introduces a Hierarchical Identity Learning (HIL) framework that generates multiple memories for each cluster and uses Multi-Center Contrastive Learning (MCCL) to enhance intra-modal clustering and minimize cross-modal discrepancies. The Bidirectional Reverse Selection Transmission (BRST) mechanism further improves cross-modal matching. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing approaches.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有聚类方法的局限性，提高无监督可见-红外行人再识别的效果。提出的层次身份学习（HIL）框架为每个聚类生成多个记忆，并使用多中心对比学习（MCCL）增强同模态聚类并最小化跨模态差异。此外，引入了双向反向选择传输（BRST）机制，以建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for   Gastric Tissue Classification</div>
<div class="meta-line">Authors: Mustafa Yurdakul, Sakir Tasdemir</div>
<div class="meta-line">First: 2025-09-11T08:24:50+00:00 · Latest: 2025-09-11T08:24:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.09242v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.09242v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoAtNeX:一种基于注意力增强的ConvNeXtV2-Transformer混合模型用于胃组织分类</div>
<div class="mono" style="margin-top:8px">背景与目的早期诊断胃病对于预防致命结果至关重要。尽管组织病理学检查仍然是诊断的金标准，但其完全由手工操作，使得评估劳动密集且容易受到病理学家之间变异的影响。关键发现可能被遗漏，缺乏标准化程序降低了一致性。这些限制突显了需要自动化、可靠且高效的胃组织分析方法。方法在本研究中，提出了一种名为CoAtNeXt的新型混合模型，用于胃组织图像分类。该模型基于CoAtNet架构，通过用增强的ConvNeXtV2块替换其MBConv层来构建。此外，还集成了卷积块注意力模块（CBAM），以通过通道和空间注意力机制提高局部特征提取。该架构被扩展以在计算效率和分类性能之间取得平衡。CoAtNeXt在两个公开可用的数据集HMU-GC-HE-30K（用于八类分类）和GasHisSDB（用于二类分类）上进行了评估，并与10种卷积神经网络（CNN）和10种视觉变换器（ViT）模型进行了比较。结果CoAtNeXt在HMU-GC-HE-30K上达到了96.47%的准确率、96.60%的精确率、96.47%的召回率、96.45%的F1分数和99.89%的AUC。在GasHisSDB上，它达到了98.29%的准确率、98.07%的精确率、98.41%的召回率、98.23%的F1分数和99.90%的AUC。它在所有测试的CNN和ViT模型中表现最佳，并超越了文献中的先前研究。结论实验结果表明，CoAtNeXt是一种稳健的架构，适用于胃组织图像的组织病理学分类，提供二类和多类的性能。它突显了其通过提高诊断准确性和减轻工作量来辅助病理学家的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aimed to develop an automated method for the early diagnosis of gastric diseases through the classification of gastric tissue images. CoAtNeXt, a hybrid model combining the CoAtNet architecture with enhanced ConvNeXtV2 blocks and CBAM for improved feature extraction, was proposed. On two datasets, HMU-GC-HE-30K and GasHisSDB, CoAtNeXt achieved high accuracy, precision, recall, F1 score, and AUC, outperforming all tested CNN and ViT models.</div>
<div class="mono" style="margin-top:8px">研究旨在通过胃组织图像分类实现胃病的早期诊断，提出了一种结合增强ConvNeXtV2块和CBAM的CoAtNeXt模型以提升特征提取能力。该模型在两个数据集上的评估结果表明，其在准确率、精确率、召回率、F1分数和AUC方面均优于10种CNN模型和10种ViT模型。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Classification of Normal and Atypical Mitotic Figures Using   ConvNeXt V2: MIDOG 2025 Track 2</div>
<div class="meta-line">Authors: Yosuke Yamagishi, Shouhei Hanaoka</div>
<div class="meta-line">First: 2025-08-26T09:11:12+00:00 · Latest: 2025-08-26T09:11:12+00:00</div>
<div class="meta-line">Comments: MIDOG 2025 solution</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.18831v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.18831v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents our solution for the MIDOG 2025 Challenge Track 2, which
focuses on binary classification of normal mitotic figures (NMFs) versus
atypical mitotic figures (AMFs) in histopathological images. Our approach
leverages a ConvNeXt V2 base model with center cropping preprocessing and
5-fold cross-validation ensemble strategy. The method addresses key challenges
including severe class imbalance, high morphological variability, and domain
heterogeneity across different tumor types, species, and scanners. Through
strategic preprocessing with 60% center cropping and mixed precision training,
our model achieved robust performance on the diverse MIDOG 2025 dataset. The
solution demonstrates the effectiveness of modern convolutional architectures
for mitotic figure subtyping while maintaining computational efficiency through
careful architectural choices and training optimizations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用ConvNeXt V2自动分类正常和异常分裂象：MIDOG 2025赛道2</div>
<div class="mono" style="margin-top:8px">本文介绍了我们对MIDOG 2025挑战赛道2的解决方案，该挑战专注于在组织病理学图像中对正常分裂象（NMFs）与异常分裂象（AMFs）进行二分类。我们的方法利用了ConvNeXt V2基础模型，采用中心裁剪预处理和5折交叉验证集成策略。该方法解决了包括严重类别不平衡、高形态学变异性以及不同肿瘤类型、物种和扫描器之间的领域异质性在内的关键挑战。通过使用60%中心裁剪和混合精度训练的战略预处理，我们的模型在多样的MIDOG 2025数据集上实现了稳健的性能。该解决方案展示了现代卷积架构在分裂象亚型分类中的有效性，同时通过精心选择的架构和训练优化保持了计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a solution for the MIDOG 2025 Challenge Track 2, focusing on classifying normal mitotic figures versus atypical mitotic figures using a ConvNeXt V2 model with center cropping preprocessing and 5-fold cross-validation. The method tackles challenges such as class imbalance and morphological variability by employing strategic preprocessing and mixed precision training, achieving robust performance on the MIDOG 2025 dataset. The solution highlights the effectiveness of modern convolutional architectures for mitotic figure subtyping while ensuring computational efficiency through careful architectural choices and training optimizations.</div>
<div class="mono" style="margin-top:8px">该论文使用带有中心裁剪预处理和5折交叉验证的ConvNeXt V2模型来解决在组织病理学图像中分类正常和异常分裂图的挑战。方法通过采用策略性预处理和混合精度训练来应对类别不平衡和形态学变异性等问题。模型在MIDOG 2025数据集上表现出色，展示了现代卷积架构在该任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Multi-label Classification of Eleven Retinal Diseases: A   Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic   Dataset</div>
<div class="meta-line">Authors: Jerry Cao-Xue, Tien Comlekoglu, Keyi Xue, Guanliang Wang, Jiang Li, Gordon Laurie</div>
<div class="meta-line">First: 2025-08-21T22:09:53+00:00 · Latest: 2025-08-21T22:09:53+00:00</div>
<div class="meta-line">Comments: 25 pages, 6 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15986v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15986v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于现代架构的大规模合成数据集上十一种视网膜疾病自动多标签分类：基准测试与元集成</div>
<div class="mono" style="margin-top:8px">由于患者隐私问题和高昂的成本，开发用于视网膜疾病分类的多标签深度学习模型往往受到大型专家注释临床数据集稀缺的阻碍。最近发布的SynFundus-1M，一个高保真度的合成数据集，包含超过一百万张眼底图像，为克服这些障碍提供了新的机会。为了为这一新资源建立基础性能基准，我们开发了一个端到端的深度学习管道，使用5折多标签分层交叉验证策略，训练了六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型）来分类十一种视网膜疾病。我们进一步开发了一个元集成模型，通过堆叠出折预测和XGBoost分类器。最终集成模型在内部验证集上表现最佳，宏平均受试者操作特征曲线下面积（AUC）为0.9973。关键的是，模型在三个多样化的实际临床数据集上表现出强大的泛化能力，DR数据集上的AUC为0.7972，AIROGS青光眼数据集上的AUC为0.9126，多标签RFMiD数据集上的宏AUC为0.8800。这项工作为未来大规模合成数据集的研究提供了稳健的基准，并证明了仅在合成数据上训练的模型能够准确分类多种病理并有效泛化到实际临床图像，为加速眼科全面AI系统的开发提供了可行途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of developing multi-label deep learning models for retinal disease classification by leveraging the SynFundus-1M synthetic dataset. Six modern architectures were trained using a 5-fold multi-label stratified cross-validation strategy, and a meta-ensemble model was developed. The ensemble model achieved a macro-average AUC of 0.9973 on internal validation and demonstrated strong generalization to three real-world clinical datasets, with AUCs ranging from 0.7972 to 0.9126.</div>
<div class="mono" style="margin-top:8px">该研究通过利用SynFundus-1M合成数据集来解决多标签深度学习模型在视网膜疾病分类中的开发挑战。使用5折多标签分层交叉验证策略训练了六种现代架构，并开发了一个元集成模型。集成模型在内部验证集上的宏平均AUC达到0.9973，并且在三个真实临床数据集上表现出良好的泛化能力，AUC范围从0.7972到0.9126。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251019_0327.html">20251019_0327</a>
<a href="archive/20251018_0326.html">20251018_0326</a>
<a href="archive/20251017_0325.html">20251017_0325</a>
<a href="archive/20251016_0321.html">20251016_0321</a>
<a href="archive/20251015_0327.html">20251015_0327</a>
<a href="archive/20251014_0326.html">20251014_0326</a>
<a href="archive/20251012_0325.html">20251012_0325</a>
<a href="archive/20251011_0327.html">20251011_0327</a>
<a href="archive/20251010_0328.html">20251010_0328</a>
<a href="archive/20251009_0319.html">20251009_0319</a>
<a href="archive/20251008_0344.html">20251008_0344</a>
<a href="archive/20251007_0345.html">20251007_0345</a>
<a href="archive/20251006_0350.html">20251006_0350</a>
<a href="archive/20251005_0349.html">20251005_0349</a>
<a href="archive/20251004_0351.html">20251004_0351</a>
<a href="archive/20251003_0351.html">20251003_0351</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0340.html">20251001_0340</a>
<a href="archive/20250930_0357.html">20250930_0357</a>
<a href="archive/20250929_0354.html">20250929_0354</a>
<a href="archive/20250928_1405.html">20250928_1405</a>
<a href="archive/20250928_0338.html">20250928_0338</a>
<a href="archive/20250927_2233.html">20250927_2233</a>
<a href="archive/20250925_0328.html">20250925_0328</a>
<a href="archive/20250924_0337.html">20250924_0337</a>
<a href="archive/20250923_0336.html">20250923_0336</a>
<a href="archive/20250922_0334.html">20250922_0334</a>
<a href="archive/20250921_0333.html">20250921_0333</a>
<a href="archive/20250920_0334.html">20250920_0334</a>
<a href="archive/20250919_1904.html">20250919_1904</a>
<a href="archive/20250919_1023.html">20250919_1023</a>
<a href="archive/20250919_0332.html">20250919_0332</a>
<a href="archive/20250918_2047.html">20250918_2047</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250918_0326.html">20250918_0326</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250917_0326.html">20250917_0326</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250916_0328.html">20250916_0328</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
