<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-07 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251207_0325</div>
    <div class="row"><div class="card">
<div class="title">Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Jiaze Li, Yan Lu, Bin Liu, Guojun Yin, Mang Ye</div>
<div class="meta-line">First: 2025-12-03T12:43:16+00:00 · Latest: 2025-12-03T12:43:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03745v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.03745v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双层次模态去偏学习在无监督可见光-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">两阶段学习管道在无监督可见光-红外行人重识别（USL-VI-ReID）中取得了令人鼓舞的结果。它首先进行单模态学习，然后进行跨模态学习以解决模态差异问题。尽管如此，该管道不可避免地引入了模态偏见：在单模态训练中学习的模态特定线索自然地传播到后续的跨模态学习中，影响身份识别和泛化能力。为了解决这一问题，我们提出了一种双层次模态去偏学习（DMDL）框架，该框架在模型和优化两个层面实施去偏。在模型层面，我们提出了一种因果启发调整干预（CAI）模块，用因果建模替代基于似然的建模，防止由模态引起的虚假模式被引入，从而得到低偏见模型。在优化层面，引入了一种协作无偏训练（CBT）策略，通过集成模态特定增强、标签细化和特征对齐来中断模态偏见在数据、标签和特征之间的传播。在基准数据集上的广泛实验表明，DMDL 可以实现模态不变特征学习和更泛化的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of modality bias in unsupervised visible-infrared person re-identification by proposing a Dual-level Modality Debiasing Learning (DMDL) framework. This framework includes a Causality-inspired Adjustment Intervention (CAI) module at the model level and a Collaborative Bias-free Training (CBT) strategy at the optimization level. The CAI module uses causal modeling to prevent spurious patterns from modality-specific cues, while the CBT strategy integrates modality-specific augmentation, label refinement, and feature alignment to interrupt the propagation of modality bias. Experiments show that DMDL improves modality-invariant feature learning and generalization performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种双层模态去偏学习（DMDL）框架，以解决无监督可见-红外行人重识别中的模态偏见问题。该框架在模型层面引入了因果启发调整干预（CAI）模块，在优化层面引入了协作无偏训练（CBT）策略。CAI模块使用因果建模防止引入伪模式，而CBT策略通过集成模态特定增强、标签精炼和特征对齐来中断偏见传播。实验表明，DMDL能够提高模态不变特征学习和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Nianchang Huang, Yi Xu, Ruida Xi, Ruida Xi, Qiang Zhang</div>
<div class="meta-line">First: 2025-11-20T09:45:37+00:00 · Latest: 2025-11-20T09:45:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16184v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.16184v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>域共享学习与渐进对齐在无监督域适应可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">近年来，可见-红外行人重识别（VI-ReID）在公开数据集上取得了显著的性能。然而，由于公开数据集与实际数据之间的差异，大多数现有的VI-ReID算法在实际应用中表现不佳。为了解决这一问题，我们主动研究无监督域适应可见-红外行人重识别（UDA-VI-ReID），旨在将从公开数据中学习到的知识转移到实际数据中，而不牺牲准确性且无需标注新样本。具体而言，我们首先分析了UDA-VI-ReID中的两个基本挑战，即域间模态差异和域内模态差异。然后，我们设计了一种新颖的两阶段模型，即域共享学习与渐进对齐（DSLGA），以处理这些差异。在第一阶段预训练中，DSLGA引入了域共享学习策略（DSLS），通过利用源域和目标域之间的共享信息来缓解由域间模态差异引起的无效预训练。而在第二阶段微调中，DSLGA设计了渐进对齐策略（GAS），通过簇到整体对齐的方式处理由域内模态差异导致的可见光和红外数据之间的跨模态对齐挑战。最后，我们构建了一种新的UDA-VI-ReID测试方法，即CMDA-XD，用于训练和测试不同的UDA-VI-ReID模型。大量实验表明，我们的方法在各种设置下显著优于现有的域适应方法，甚至优于一些监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of Unsupervised Domain Adaptation in Visible-Infrared person Re-Identification (UDA-VI-ReID) by proposing a two-stage model called Domain-Shared Learning and Gradual Alignment (DSLGA). The first stage uses Domain-Shared Learning Strategy (DSLS) to mitigate inter-domain modality discrepancies, while the second stage employs Gradual Alignment Strategy (GAS) to handle intra-domain modality discrepancies. Experiments show that the proposed method significantly outperforms existing domain adaptation methods and even some supervised methods in various settings.</div>
<div class="mono" style="margin-top:8px">该论文通过提出两阶段模型Domain-Shared Learning and Gradual Alignment (DSLGA)来解决可见红外人再识别（UDA-VI-ReID）中的无监督领域适应问题。模型首先使用Domain-Shared Learning Strategy (DSLS) 在预训练阶段缓解跨域模态差异，然后在微调阶段使用Gradual Alignment Strategy (GAS) 处理可见光和红外数据之间的跨模态对齐挑战。作者还引入了一种新的测试方法CMDA-XD。实验结果表明，他们的方法在各种设置下优于现有领域适应方法，甚至优于一些监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws</div>
<div class="meta-line">Authors: Lin Guo, Xiaoqing Luo, Wei Xie, Zhancheng Zhang, Hui Li, Rui Wang, Zhenhua Feng, Xiaoning Song</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-10-30T08:53:13+00:00 · Latest: 2025-10-30T08:53:13+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26268v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.26268v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人类认知法则的红外和可见光图像生成融合方法研究</div>
<div class="mono" style="margin-top:8px">现有的红外和可见光图像融合方法常常面临平衡模态信息的困境。生成式融合方法通过学习数据分布来重建融合图像，但其生成能力仍然有限。此外，模态信息选择的不可解释性进一步影响了复杂场景下融合结果的可靠性和一致性。本文在人类认知法则的启发下重新审视生成式图像融合的本质，并提出了一种新的红外和可见光图像融合方法，称为HCLFuse。首先，HCLFuse研究了无监督融合网络中信息映射的量化理论，从而设计了一个多尺度掩码调节变分瓶颈编码器。该编码器通过后验概率建模和信息分解提取准确且简洁的低级模态信息，从而支持生成高保真的结构细节。此外，扩散模型的概率生成能力与物理法则相结合，形成了一种时间变化的物理引导机制，该机制在不同阶段自适应地调节生成过程，从而增强模型感知数据内在结构的能力并减少对数据质量的依赖。实验结果表明，所提出的方法在多个数据集上的定性和定量评估中均实现了最先进的融合性能，并显著提高了语义分割指标。这充分证明了这种基于人类认知的生成图像融合方法在增强结构一致性和细节质量方面的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper revisits generative infrared and visible image fusion methods by drawing inspiration from human cognitive laws. It proposes HCLFuse, which uses a multi-scale mask-regulated variational bottleneck encoder to extract accurate low-level modal information and integrates probabilistic generative capabilities with physical laws to enhance structural consistency. Experiments show that HCLFuse outperforms existing methods in both qualitative and quantitative evaluations, particularly in semantic segmentation metrics.</div>
<div class="mono" style="margin-top:8px">本文通过引入人类认知法则 revisit 生成红外和可见光图像融合方法，提出了 HCLFuse。该方法采用多尺度掩码调节变分瓶颈编码器来提取准确的低级模态信息，并将概率生成能力与物理法则结合，以增强结构一致性。实验表明，HCLFuse 在多个数据集上的定性和定量评估中均优于现有方法，特别是在语义分割指标上表现出显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion</div>
<div class="meta-line">Authors: Xiaopeng Liu, Yupei Lin, Sen Zhang, Xiao Wang, Yukai Shi, Liang Lin</div>
<div class="meta-line">First: 2025-10-14T08:13:15+00:00 · Latest: 2025-10-14T08:13:15+00:00</div>
<div class="meta-line">Comments: For the first time, angle-based perception was introduced into the multi-modality image fusion task</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12260v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.12260v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visible-infrared image fusion is crucial in key applications such as autonomous driving and nighttime surveillance. Its main goal is to integrate multimodal information to produce enhanced images that are better suited for downstream tasks. Although deep learning based fusion methods have made significant progress, mainstream unsupervised approaches still face serious challenges in practical applications. Existing methods mostly rely on manually designed loss functions to guide the fusion process. However, these loss functions have obvious limitations. On one hand, the reference images constructed by existing methods often lack details and have uneven brightness. On the other hand, the widely used gradient losses focus only on gradient magnitude. To address these challenges, this paper proposes an angle-based perception framework for spatial-sensitive image fusion (AngularFuse). At first, we design a cross-modal complementary mask module to force the network to learn complementary information between modalities. Then, a fine-grained reference image synthesis strategy is introduced. By combining Laplacian edge enhancement with adaptive histogram equalization, reference images with richer details and more balanced brightness are generated. Last but not least, we introduce an angle-aware loss, which for the first time constrains both gradient magnitude and direction simultaneously in the gradient domain. AngularFuse ensures that the fused images preserve both texture intensity and correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and M3FD public datasets show that AngularFuse outperforms existing mainstream methods with clear margin. Visual comparisons further confirm that our method produces sharper and more detailed results in challenging scenes, demonstrating superior fusion capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AngularFuse：空间敏感多模态图像融合中的角度感知研究</div>
<div class="mono" style="margin-top:8px">可见-红外图像融合在自动驾驶和夜间监控等关键应用中至关重要。其主要目标是整合多模态信息，生成更适合下游任务的增强图像。尽管基于深度学习的融合方法取得了显著进展，主流的无监督方法在实际应用中仍面临重大挑战。现有方法主要依赖人工设计的损失函数来指导融合过程。然而，这些损失函数存在明显局限性。一方面，现有方法构建的参考图像往往缺乏细节且亮度不均。另一方面，广泛使用的梯度损失仅关注梯度大小。为解决这些问题，本文提出了一种空间敏感图像融合的角度感知框架（AngularFuse）。首先，我们设计了一种跨模态互补掩码模块，迫使网络学习模态之间的互补信息。然后，引入了一种细粒度的参考图像合成策略。通过结合拉普拉斯边缘增强和自适应直方图均衡，生成了具有更多细节和更平衡亮度的参考图像。最后，我们引入了一种角度感知损失，这是首次在梯度域中同时约束梯度大小和方向。AngularFuse 确保融合图像保留了纹理强度和正确的边缘方向。在 MSRS、RoadScene 和 M3FD 公开数据集上的全面实验表明，AngularFuse 在清晰度方面明显优于现有主流方法。视觉对比进一步证实，我们的方法在具有挑战性的场景中生成了更清晰、更详细的图像，展示了更强的融合能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AngularFuse proposes an angle-based perception framework for spatial-sensitive multi-modality image fusion, addressing the limitations of existing methods by introducing a cross-modal complementary mask module, a fine-grained reference image synthesis strategy, and an angle-aware loss. The method generates reference images with richer details and balanced brightness, and ensures that fused images preserve texture intensity and correct edge orientation. Experiments on public datasets show that AngularFuse outperforms existing methods with clear margins and produces sharper, more detailed results in challenging scenes.</div>
<div class="mono" style="margin-top:8px">AngularFuse 提出了一种基于角度的感知框架，用于空间敏感的多模态图像融合，以解决现有无监督方法的局限性。它引入了跨模态互补掩码模块、精细的参考图像合成策略以及角度感知损失。在 MSRS、RoadScene 和 M3FD 数据集上的实验表明，AngularFuse 在保持纹理强度和正确边缘方向的同时，能够生成更清晰和更详细的融合图像，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2025-09-15T05:10:43+00:00 · Latest: 2025-09-15T05:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.11587v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.11587v1">PDF</a> · <a href="https://github.com/haonanshi0125/HIL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to learn modality-invariant image features from unlabeled cross-modal person datasets by reducing the modality gap while minimizing reliance on costly manual annotations. Existing methods typically address USVI-ReID using cluster-based contrastive learning, which represents a person by a single cluster center. However, they primarily focus on the commonality of images within each cluster while neglecting the finer-grained differences among them. To address the limitation, we propose a Hierarchical Identity Learning (HIL) framework. Since each cluster may contain several smaller sub-clusters that reflect fine-grained variations among images, we generate multiple memories for each existing coarse-grained cluster via a secondary clustering. Additionally, we propose Multi-Center Contrastive Learning (MCCL) to refine representations for enhancing intra-modal clustering and minimizing cross-modal discrepancies. To further improve cross-modal matching quality, we design a Bidirectional Reverse Selection Transmission (BRST) mechanism, which establishes reliable cross-modal correspondences by performing bidirectional matching of pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets demonstrate that the proposed method outperforms existing approaches. The source code is available at: https://github.com/haonanshi0125/HIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外行人重识别的分层身份学习</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在通过减少模态差异并最小化对昂贵的手动注释的依赖，从跨模态行人未标注数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来处理USVI-ReID，通过单个聚类中心表示一个人。然而，它们主要关注每个聚类内部图像的共同性，而忽视了它们之间的细微差异。为了解决这一局限性，我们提出了一种分层身份学习（HIL）框架。由于每个聚类可能包含多个反映图像间细微差异的较小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习（MCCL）以细化表示，增强同模态聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输（BRST）机制，通过双向匹配伪标签来建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法优于现有方法。源代码可在：https://github.com/haonanshi0125/HIL 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of existing methods that focus on commonality within clusters. The Hierarchical Identity Learning (HIL) framework is proposed, which generates multiple memories for each cluster and uses Multi-Center Contrastive Learning to refine representations. Additionally, a Bidirectional Reverse Selection Transmission mechanism is designed to enhance cross-modal matching. Experiments show that the proposed method outperforms existing approaches on the SYSU-MM01 and RegDB datasets.</div>
<div class="mono" style="margin-top:8px">研究旨在通过减少模态差异和减少对人工标注的依赖来提高无监督的可见光-红外人再识别。提出的层次身份学习（HIL）框架为每个聚类生成多个记忆，并使用多中心对比学习（MCCL）来细化表示。此外，还引入了双向反向选择传输（BRST）机制，以增强跨模态匹配质量。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection</div>
<div class="meta-line">Authors: Xiwei Zhang, Chunjin Yang, Yiming Xiao, Runtong Zhang, Fanman Meng</div>
<div class="meta-line">First: 2025-07-16T08:21:41+00:00 · Latest: 2025-07-16T08:21:41+00:00</div>
<div class="meta-line">Comments: 8 main-pages, 3 reference-pages, 5 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12017v1">Abs</a> · <a href="https://arxiv.org/pdf/2507.12017v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised domain adaptive object detection (UDAOD) from the visible domain to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB domain as a unified domain and neglect the multiple subdomains within it, such as daytime, nighttime, and foggy scenes. We argue that decoupling the domain-invariant (DI) and domain-specific (DS) features across these multiple subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper proposes a new SS-DC framework based on a decoupling-coupling strategy. In terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID) module in the aspect of spectral decomposition. Due to the style and content information being highly embedded in different frequency bands, this module can decouple DI and DS components more accurately and interpretably. A novel filter bank-based spectral processing paradigm and a self-distillation-driven decoupling loss are proposed to improve the spectral domain decoupling. In terms of coupling, a new spatial-spectral coupling method is proposed, which realizes joint coupling through spatial and spectral DI feature pyramids. Meanwhile, this paper introduces DS from decoupling to reduce the domain bias. Extensive experiments demonstrate that our method can significantly improve the baseline performance and outperform existing UDAOD methods on multiple RGB-IR datasets, including a new experimental protocol proposed in this paper based on the FLIR-ADAS dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SS-DC：可见光-红外波段空间-光谱解耦与耦合以实现跨可见光-红外域的目标检测适应性</div>
<div class="mono" style="margin-top:8px">从可见光域到红外（RGB-IR）域的无监督域适应性目标检测（UDAOD）具有挑战性。现有方法将RGB域视为统一域，忽视了其中的多个子域，如白天、夜晚和雾天场景。我们认为，在这些多个子域中解耦域不变（DI）和域特定（DS）特征是有益的，以实现RGB-IR域适应。为此，本文提出了一种新的基于解耦-耦合策略的SS-DC框架。在解耦方面，我们从光谱分解的角度设计了一个光谱自适应恒等解耦（SAID）模块。由于风格和内容信息高度嵌入在不同的频率带中，该模块可以更准确和可解释地解耦DI和DS成分。我们提出了一种基于滤波器组的光谱处理范式和一种自我蒸馏驱动的解耦损失，以提高光谱域解耦。在耦合方面，提出了一种新的空间-光谱耦合方法，通过空间和光谱DI特征金字塔实现联合耦合。同时，本文将解耦中的DS引入以减少域偏差。大量实验表明，我们的方法可以显著提高基线性能，并在多个RGB-IR数据集上优于现有UDAOD方法，包括本文基于FLIR-ADAS数据集提出的一种新的实验协议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised domain adaptive object detection (UDAOD) from visible to infrared domains. It proposes a new SS-DC framework that decouples domain-invariant and domain-specific features across multiple subdomains using a Spectral Adaptive Idempotent Decoupling (SAID) module and a novel spectral processing paradigm. The framework also introduces a spatial-spectral coupling method and reduces domain bias. Experiments show that the proposed method significantly improves baseline performance and outperforms existing UDAOD methods on multiple datasets, including a new protocol based on the FLIR-ADAS dataset.</div>
<div class="mono" style="margin-top:8px">本文解决了从可见光域到红外域的无监督领域适应目标检测（UDAOD）的挑战。提出了一种SS-DC框架，该框架在RGB域内的多个子域中解耦领域不变（DI）和领域特定（DS）特征。该框架包括一个光谱自适应恒等解耦（SAID）模块和一种新的空间-光谱耦合方法，这些方法改进了解耦和耦合过程。实验表明，所提出的方法在多个RGB-IR数据集上显著优于现有UDAOD方法，包括基于FLIR-ADAS数据集的新实验协议。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu</div>
<div class="meta-line">First: 2025-05-05T10:36:52+00:00 · Latest: 2025-05-06T07:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.02549v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.02549v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. Existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. In practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. To address this, we introduce a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. To this end, we propose a novel Robust Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning mechanism (RAL) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. Second, to alleviate error accumulation-where the model reinforces its own mistakes-RoDE employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. However, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. To resolve this, we introduce Cluster Consistency Matching (CCM), which aligns clusters across models and modalities by measuring cross-cluster similarity. Extensive experiments on three benchmarks demonstrate the effectiveness of RoDE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒对偶学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（UVI-ReID）旨在无需昂贵标注的情况下，在不同模态中检索行人图像，但面临着模态差距和缺乏监督的挑战。现有方法通常采用自训练结合聚类生成的伪标签，但隐含地假设这些标签总是正确的。然而，在实践中，由于不可避免的伪标签噪声，这一假设会妨碍模型学习。为解决这一问题，我们提出了一种新的学习范式，明确考虑伪标签噪声（PLN），并针对其三个关键挑战：噪声过拟合、错误累积和嘈杂的簇对应关系。为此，我们提出了一种新颖的鲁棒对偶学习框架（RoDE）以减轻噪声伪标签的影响。首先，为了对抗噪声过拟合，我们提出了一种鲁棒自适应学习机制（RAL），该机制动态强调干净样本并降低噪声样本的权重。其次，为了缓解错误累积——即模型强化其自身的错误，RoDE 使用来自彼此伪标签的双模型交替训练，鼓励多样性并防止模型崩溃。然而，这种双模型策略引入了模型和模态之间的簇对齐问题，导致嘈杂的簇对应关系。为解决这一问题，我们引入了簇一致性匹配（CCM），通过测量跨簇相似性来在模型和模态之间对齐簇。在三个基准上的广泛实验表明了RoDE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a Robust Duality Learning framework (RoDE) that explicitly handles pseudo-label noise. RoDE includes a Robust Adaptive Learning mechanism to mitigate noise overfitting and dual models to prevent error accumulation, while Cluster Consistency Matching aligns clusters across models and modalities. Experiments on three benchmarks show RoDE&#x27;s effectiveness in improving UVI-ReID performance despite noisy pseudo-labels.</div>
<div class="mono" style="margin-top:8px">论文通过引入一个明确考虑伪标签噪声的新学习范式，解决了无监督可见红外行人重识别的挑战。提出了一个稳健的二元学习框架（RoDE），包含一种稳健自适应学习机制来应对噪声过拟合，并使用双模型策略防止错误累积。为了在模型和模态之间对齐聚类，引入了聚类一致性匹配（CCM）。实验表明RoDE在减轻伪标签噪声影响方面非常有效。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at \href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作精炼的语义对齐学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在通过跨模态图像的伪标签关联算法统一伪标签，然后设计对比学习框架进行全局特征学习，以匹配不同模态下同一个体的行人图像，而无需人工注释。然而，这些方法忽视了细粒度模式带来的特征表示和伪标签分布的跨模态变化，导致仅优化全局特征时的模态共享学习不足。为解决这一问题，我们提出了一种协作精炼的语义对齐学习（SALCR）框架，该框架为每个模态强调的特定细粒度模式建立优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一种双向伪标签统一的双关联全局学习（DAGI）模块，然后执行细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。基于语义对齐特征及其相应的标签空间，构建优化目标。为缓解来自噪声伪标签的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集，并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有最佳方法。我们的代码可在\href{https://github.com/FranklinLingfeng/code-for-SALCR}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification (USL-VI-ReID) by addressing the limitations of previous methods that focus only on global feature learning and ignore fine-grained cross-modality variations. The proposed Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework introduces a Dual Association with Global Learning (DAGI) module to unify cross-modality pseudo-labels and a Fine-Grained Semantic-Aligned Learning (FGSAL) module to explore part-level semantic-aligned patterns. Additionally, a Global-Part Collaborative Refinement (GPCR) module is used to refine global and part features and optimize inter-instance relationships. Experiments show that SALCR outperforms existing methods in USL-VI-ReID tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了一种语义对齐学习与协作精炼（SALCR）框架，用于无监督可见-红外行人重识别（USL-VI-ReID）。该框架通过强调特定的细粒度模式并实现不同模态标签分布之间的互补对齐来解决先前方法的局限性。框架包括一个双向统一伪标签的Dual Association with Global Learning (DAGI) 模块和一个探索跨模态实例中每个模态强调的细粒度语义对齐模式的Fine-Grained Semantic-Aligned Learning (FGSAL) 模块。此外，还提出了一个全局-部分协作精炼（GPCR）模块，用于动态挖掘全局和部分特征的可靠正样本集并优化实例间关系。实验表明，SALCR在USL-VI-ReID任务中优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible Person Re-Identification</div>
<div class="meta-line">Authors: Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang</div>
<div class="meta-line">First: 2024-12-26T08:03:53+00:00 · Latest: 2025-01-02T11:22:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.19111v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.19111v2">PDF</a> · <a href="https://github.com/1024AILab/ReID-SEPG">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of deep learning has facilitated the application of person re-identification (ReID) technology in intelligent security. Visible-infrared person re-identification (VI-ReID) aims to match pedestrians across infrared and visible modality images enabling 24-hour surveillance. Current studies relying on unsupervised modality transformations as well as inefficient embedding constraints to bridge the spectral differences between infrared and visible images, however, limit their potential performance. To tackle the limitations of the above approaches, this paper introduces a simple yet effective Spectral Enhancement and Pseudo-anchor Guidance Network, named SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement scheme based on frequency domain information and greyscale space, which avoids the information loss typically caused by inefficient modality transformations. Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is introduced to bridge local modality discrepancies while better preserving discriminative identity embeddings. Experimental results on two public benchmark datasets demonstrate the superior performance of SEPG-Net against other state-of-the-art methods. The code is available at https://github.com/1024AILab/ReID-SEPG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>红外可见光行人重识别的光谱增强与伪锚点引导</div>
<div class="mono" style="margin-top:8px">深度学习的发展促进了行人重识别(ReID)技术在智能安全中的应用。可见光-红外行人重识别(VI-ReID)旨在跨红外和可见光模态图像匹配行人，实现24小时监控。当前研究依赖于无监督模态变换以及低效的嵌入约束来弥合红外和可见光图像之间的光谱差异，然而这限制了其潜在性能。为解决上述方法的局限性，本文提出了一种简单有效的光谱增强与伪锚点引导网络，命名为SEPG-Net。具体而言，我们提出了一种基于频域信息和灰度空间的更均匀的光谱增强方案，避免了低效模态变换通常引起的信息损失。此外，引入了伪锚点引导双向聚合(PABA)损失以弥合局部模态差异，同时更好地保留了区分性身份嵌入。在两个公开基准数据集上的实验结果表明，SEPG-Net的性能优于其他最先进的方法。代码可在https://github.com/1024AILab/ReID-SEPG获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current unsupervised modality transformations and inefficient embedding constraints in visible-infrared person re-identification. It proposes SEPG-Net, which includes a spectral enhancement scheme based on frequency domain and greyscale space to avoid information loss, and a Pseudo Anchor-guided Bidirectional Aggregation loss to preserve discriminative identity embeddings. Experiments on two public datasets show that SEPG-Net outperforms other state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本文针对当前可见红外行人重识别（VI-ReID）中无监督模态变换和低效嵌入约束的局限性，提出了SEPG-Net，该网络包含基于频域和灰度空间的光谱增强方案，以避免信息损失，并引入了伪锚点引导双向聚合（PABA）损失，更好地保留了区分性身份嵌入。该方法在两个公开基准数据集上优于其他最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Extended Cross-Modality United Learning for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-26T09:30:26+00:00 · Latest: 2024-12-26T09:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.19134v1">Abs</a> · <a href="https://arxiv.org/pdf/2412.19134v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims to learn modality-invariant features from unlabeled cross-modality datasets and reduce the inter-modality gap. However, the existing methods lack cross-modality clustering or excessively pursue cluster-level association, which makes it difficult to perform reliable modality-invariant features learning. To deal with this issue, we propose a Extended Cross-Modality United Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we design ECUL to naturally integrates intra-modality clustering, inter-modality clustering and inter-modality instance selection, establishing compact and accurate cross-modality associations while reducing the introduction of noisy labels. Moreover, EMCC captures and filters the neighborhood relationships by extending the encoding vector, which further promotes the learning of modality-invariant and camera-invariant knowledge in terms of clustering algorithm. Finally, TSMem provides accurate and generalized proxy points for contrastive learning by updating the memory in stages. Extensive experiments results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL shows promising performance and even outperforms certain supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展跨模态联合学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征并减少模态间差距。然而，现有方法缺乏跨模态聚类或过度追求聚类级别的关联，这使得可靠地学习模态不变特征变得困难。为解决这一问题，我们提出了一种扩展跨模态联合学习（ECUL）框架，结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。具体而言，我们设计ECUL自然地整合了模内聚类、模间聚类和模间实例选择，建立了紧凑而准确的跨模态关联，同时减少了噪声标签的引入。此外，EMCC通过扩展编码向量来捕获和过滤邻域关系，进一步促进了聚类算法中模态不变和相机不变知识的学习。最后，TSMem通过分阶段更新记忆为对比学习提供准确且通用的代理点。在SYSU-MM01和RegDB数据集上的广泛实验结果表明，提出的ECUL表现出色，甚至优于某些监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised learning for visible-infrared person re-identification by proposing an Extended Cross-Modality United Learning (ECUL) framework. This framework includes Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem) to enhance cross-modality clustering and reduce the inter-modality gap. Key findings show that ECUL outperforms existing methods and even surpasses some supervised approaches on SYSU-MM01 and RegDB datasets.</div>
<div class="mono" style="margin-top:8px">论文提出了一种扩展跨模态联合学习（ECUL）框架，以解决可见光-红外人再识别的无监督学习问题。该框架结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem），以集成模内和模间聚类，并减少模间差距。在SYSU-MM01和RegDB数据集上的实验表明，ECUL能够有效学习模态不变特征，并优于某些监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Relieving Universal Label Noise for Unsupervised Visible-Infrared Person Re-Identification by Inferring from Neighbors</div>
<div class="meta-line">Authors: Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin</div>
<div class="meta-line">First: 2024-12-16T04:04:41+00:00 · Latest: 2024-12-16T04:04:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.12220v1">Abs</a> · <a href="https://arxiv.org/pdf/2412.12220v1">PDF</a> · <a href="https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of great research and practical significance yet remains challenging due to the absence of annotations. Existing approaches aim to learn modality-invariant representations in an unsupervised setting. However, these methods often encounter label noise within and across modalities due to suboptimal clustering results and considerable modality discrepancies, which impedes effective training. To address these challenges, we propose a straightforward yet effective solution for USL-VI-ReID by mitigating universal label noise using neighbor information. Specifically, we introduce the Neighbor-guided Universal Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in both homogeneous and heterogeneous spaces with soft labels derived from neighboring samples to reduce label noise. Additionally, we present the Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability by minimizing the influence of unreliable samples. Extensive experiments on the RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing USL-VI-ReID approaches, despite its simplicity. The source code is available at: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过从邻居推断缓解无监督可见-红外行人重识别的通用标签噪声</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）在研究和实践中具有重要意义，但由于缺乏标注而面临挑战。现有方法旨在在无监督设置中学习模态不变表示。然而，这些方法由于聚类结果不佳和模态差异较大，常常遇到模态内和跨模态的标签噪声，这阻碍了有效的训练。为应对这些挑战，我们提出了一种简单而有效的解决方案，通过利用邻居信息缓解USL-VI-ReID中的通用标签噪声。具体而言，我们引入了邻居引导的通用标签校准（N-ULC）模块，该模块用来自邻居样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们提出了邻居引导的动态加权（N-DW）模块，通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，但我们的方法仍优于现有USL-VI-ReID方法。源代码可在：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing label noise issues. It introduces the Neighbor-guided Universal Label Calibration (N-ULC) module to generate soft labels from neighboring samples and the Neighbor-guided Dynamic Weighting (N-DW) module to reduce the impact of unreliable samples. Experiments show that the proposed method outperforms existing approaches on RegDB and SYSU-MM01 datasets, demonstrating its effectiveness despite its simplicity.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决标签噪声问题来提升无监督的可见光-红外行人再识别。方法引入了基于邻居的全局标签校准（N-ULC）模块，从邻近样本生成软标签，并引入了基于邻居的动态加权（N-DW）模块以减少不可靠样本的影响。实验结果显示，该方法在RegDB和SYSU-MM01数据集上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modality-Camera Invariant Clustering for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yiming Yang, Weipeng Hu, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-11T09:31:03+00:00 · Latest: 2024-12-11T09:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.08231v1">Abs</a> · <a href="https://arxiv.org/pdf/2412.08231v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) offers a more flexible and cost-effective alternative compared to supervised methods. This field has gained increasing attention due to its promising potential. Existing methods simply cluster modality-specific samples and employ strong association techniques to achieve instance-to-cluster or cluster-to-cluster cross-modality associations. However, they ignore cross-camera differences, leading to noticeable issues with excessive splitting of identities. Consequently, this undermines the accuracy and reliability of cross-modal associations. To address these issues, we propose a novel Dynamic Modality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID. Specifically, our DMIC naturally integrates Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive Learning (HMCL) into a unified framework, which eliminates both the cross-modality and cross-camera discrepancies in clustering. MIE fuses inter-modal and inter-camera distance coding to bridge the gaps between modalities and cameras at the clustering level. DNC employs two dynamic search strategies to refine the network&#x27;s optimization objective, transitioning from improving discriminability to enhancing cross-modal and cross-camera generalizability. Moreover, HMCL is designed to optimize instance-level and cluster-level distributions. Memories for intra-modality and inter-modality training are updated using randomly selected samples, facilitating real-time exploration of modality-invariant representations. Extensive experiments have demonstrated that our DMIC addresses the limitations present in current clustering approaches and achieve competitive performance, which significantly reduces the performance gap with supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态模态-摄像机不变聚类在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）相比监督方法提供了更灵活和成本效益更高的替代方案。该领域因其潜在的前景而越来越受到关注。现有方法简单地聚类模态特定样本，并采用强关联技术实现跨模态实例到聚类或聚类到聚类的关联。然而，它们忽略了跨摄像机差异，导致身份分割过度，严重影响了跨模态关联的准确性和可靠性。为解决这些问题，我们提出了一种新颖的动态模态-摄像机不变聚类（DMIC）框架用于USL-VI-ReID。具体而言，我们的DMIC将模态-摄像机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）统一到一个框架中，消除了聚类中的跨模态和跨摄像机差异。MIE融合了跨模态和跨摄像机的距离编码，在聚类层面弥合了模态和摄像机之间的差距。DNC采用两种动态搜索策略来细化网络的优化目标，从提高可区分性过渡到增强跨模态和跨摄像机的一般性。此外，HMCL旨在优化实例级和聚类级分布。使用随机选择的样本更新模态内和模态间训练的记忆，促进实时探索模态不变表示。大量实验表明，我们的DMIC解决了当前聚类方法的局限性，实现了竞争力的表现，显著缩小了与监督方法的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a Dynamic Modality-Camera Invariant Clustering (DMIC) framework for unsupervised visible-infrared person re-identification, addressing the issue of excessive identity splitting by integrating Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC), and Hybrid Modality Contrastive Learning (HMCL). The DMIC framework eliminates cross-modality and cross-camera discrepancies, improving the accuracy and reliability of cross-modal associations. Experiments show that DMIC outperforms existing methods and reduces the performance gap with supervised approaches.</div>
<div class="mono" style="margin-top:8px">论文提出了一种动态模态-相机不变聚类（DMIC）框架，用于无监督的可见光-红外行人再识别（USL-VI-ReID），解决了由于跨相机差异导致的身份过度分割问题。DMIC将模态-相机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）整合到一个统一框架中，以消除跨模态和跨相机的差异。实验表明，DMIC在性能上优于现有方法，并显著缩小了与监督方法的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2024-02-01T15:33:17+00:00 · Latest: 2024-12-04T03:55:35+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.00672v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.00672v4">PDF</a> · <a href="https://github.com/FranklinLingfeng/code_for_MULT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) endeavors to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency between the feature space and the pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to quantify the inconsistency between the pseudo-label space and the feature space, subsequently minimizing it. The proposed MULT ensures that the generated pseudo-labels maintain alignment across modalities while upholding structural consistency within intra-modality. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the side effects of noisy pseudo-labels while simultaneously aligning different modalities, coupled with an Alternative Modality-Invariant Representation Learning (AMIRL) framework. Experiments demonstrate that our proposed method outperforms existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. Code is available at https://github.com/FranklinLingfeng/code_for_MULT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索同质和异质一致标签关联以进行无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，从不同模态中检索同一身份的行人图像。尽管先前的工作集中在建立跨模态伪标签关联以弥合模态差距，但它们忽略了在特征空间和伪标签空间之间保持实例级别的同质性和异质一致性，导致粗略的关联。为此，我们引入了一个统一模态标签转移（MULT）模块，该模块同时考虑了同质和异质的细粒度实例结构，从而产生高质量的跨模态标签关联。该模块利用同质和异质亲和力来量化伪标签空间与特征空间之间的不一致性，随后最小化这种不一致性。所提出的MULT确保生成的伪标签在不同模态之间保持对齐，同时在同模态内保持结构一致性。此外，我们还提出了一种简单的即插即用在线跨记忆标签精炼（OCLR）模块，以进一步减轻噪声伪标签的副作用，同时对齐不同模态，结合了一个交替模态不变表示学习（AMIRL）框架。实验表明，我们提出的方法优于现有的USL-VI-ReID方法，突显了我们MULT在与其他跨模态关联方法相比的优势。代码可在https://github.com/FranklinLingfeng/code_for_MULT获取。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Spectral Attention for Unsupervised RGB-IR Face Verification and Person Re-identification</div>
<div class="meta-line">Authors: Kshitij Nikhal, Cedric Nimpa Fondje, Benjamin S. Riggan</div>
<div class="meta-line">First: 2024-11-28T15:38:15+00:00 · Latest: 2024-11-28T15:38:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.19215v1">Abs</a> · <a href="https://arxiv.org/pdf/2411.19215v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-spectral biometrics, such as matching imagery of faces or persons from visible (RGB) and infrared (IR) bands, have rapidly advanced over the last decade due to increasing sensitivity, size, quality, and ubiquity of IR focal plane arrays and enhanced analytics beyond the visible spectrum. Current techniques for mitigating large spectral disparities between RGB and IR imagery often include learning a discriminative common subspace by exploiting precisely curated data acquired from multiple spectra. Although there are challenges with determining robust architectures for extracting common information, a critical limitation for supervised methods is poor scalability in terms of acquiring labeled data. Therefore, we propose a novel unsupervised cross-spectral framework that combines (1) a new pseudo triplet loss with cross-spectral voting, (2) a new cross-spectral attention network leveraging multiple subspaces, and (3) structured sparsity to perform more discriminative cross-spectral clustering. We extensively compare our proposed RGB-IR biometric learning framework (and its individual components) with recent and previous state-of-the-art models on two challenging benchmark datasets: DEVCOM Army Research Laboratory Visible-Thermal Face Dataset (ARL-VTF) and RegDB person re-identification dataset, and, in some cases, achieve performance superior to completely supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨光谱注意力机制在无监督RGB-IR人脸验证和行人再识别中的应用</div>
<div class="mono" style="margin-top:8px">跨光谱生物特征识别，如将可见光（RGB）和红外（IR）波段的面部或人体图像进行匹配，由于红外焦平面阵列的灵敏度、大小、质量和普及性不断提高，以及超出可见光谱的增强分析技术，已在过去十年中迅速发展。当前缓解RGB和IR图像之间巨大光谱差异的技术通常包括通过利用多光谱精确收集的数据来学习一个判别性公共子空间。尽管提取公共信息的稳健架构存在挑战，但监督方法的一个关键限制是获取标注数据的可扩展性差。因此，我们提出了一种新的无监督跨光谱框架，结合了（1）一种新的伪三元损失与跨光谱投票，（2）一种利用多个子空间的新跨光谱注意力网络，以及（3）结构化稀疏性以进行更具判别性的跨光谱聚类。我们详细比较了我们提出的RGB-IR生物特征学习框架（及其各个组件）与两个具有挑战性的基准数据集DEVCOM陆军研究实验室可见-红外人脸数据集（ARL-VTF）和RegDB行人再识别数据集上的最新和先前的最先进模型，并在某些情况下，其性能优于完全监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised face verification and person re-identification using RGB and IR images. The method introduces a novel unsupervised cross-spectral framework with a pseudo triplet loss, cross-spectral attention network, and structured sparsity. Key findings show that this approach outperforms supervised methods on two benchmark datasets, achieving superior performance in some cases.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决光谱差异问题，改进无监督的RGB-IR人脸验证和行人再识别。方法提出了一种新的无监督跨光谱框架，包括伪三元损失、跨光谱注意力网络和结构化稀疏性。关键发现表明，该框架在两个基准数据集上优于最近和之前的先进模型，有时甚至超过了监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement</div>
<div class="meta-line">Authors: De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T04:40:30+00:00 · Latest: 2024-11-03T13:50:53+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.12711v4">Abs</a> · <a href="https://arxiv.org/pdf/2305.12711v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under the assumption that the prediction or label distribution of each example should be similar to its nearest neighbors. Extensive experimental results on the public SYSU-MM01 and RegDB datasets demonstrate the effectiveness of the proposed method, surpassing existing state-of-the-art approach by a large margin of 7.76% mAP on average, which even surpasses some supervised VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作学习与邻域引导标签精炼的无监督可见-红外行人再识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人再识别（USL-VI-ReID）旨在从跨模态的未标注数据集中学习模态不变特征，这对于视频监控系统中的实际应用至关重要。解决USL-VI-ReID任务的关键在于解决跨模态数据关联问题，以便进行后续的异质联合学习。为了解决这一问题，我们提出了一种双最优传输标签分配（DOTLA）框架，以同时将一种模态生成的标签分配给其对应的模态。提出的DOTLA机制通过相互强化和高效的解决方案来解决跨模态数据关联问题，从而有效减少了某些不足和噪声标签关联的副作用。此外，我们还提出了一种跨模态邻域一致性引导的标签精炼和正则化模块，在假设每个示例的预测或标签分布应与其最近邻相似的前提下，消除不准确的监督信号带来的负面影响。在公开的SYSU-MM01和RegDB数据集上的广泛实验结果表明，所提出的方法具有有效性，平均mAP比现有最先进的方法高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the unsupervised visible-infrared person re-identification (USL-VI-ReID) task by proposing a Dual Optimal Transport Label Assignment (DOTLA) framework to solve the cross-modality data association problem. It also introduces a cross-modality neighbor consistency guided label refinement and regularization module to improve label accuracy. The method achieves superior performance, outperforming existing state-of-the-art approaches by 7.76% mAP on average on public SYSU-MM01 and RegDB datasets.</div>
<div class="mono" style="margin-top:8px">论文通过提出一种双最优传输标签分配（DOTLA）框架来解决跨模态数据关联问题，并引入了跨模态邻居一致性引导的标签精炼模块以提高标签准确性。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法在平均精度（mAP）上比现有最先进的方法高出7.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T03:27:46+00:00 · Latest: 2024-11-03T13:48:21+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.12673v4">Abs</a> · <a href="https://arxiv.org/pdf/2305.12673v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to match pedestrian images of the same identity from different modalities without annotations. Existing works mainly focus on alleviating the modality gap by aligning instance-level features of the unlabeled samples. However, the relationships between cross-modality clusters are not well explored. To this end, we propose a novel bilateral cluster matching-based learning framework to reduce the modality gap by matching cross-modality clusters. Specifically, we design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm through optimizing the maximum matching problem in a bipartite graph. Then, the matched pairwise clusters utilize shared visible and infrared pseudo-labels during the model training. Under such a supervisory signal, a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework is proposed to align features jointly at a cluster-level. Meanwhile, the cross-modality Consistency Constraint (CC) is proposed to explicitly reduce the large modality discrepancy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the effectiveness of the proposed method, surpassing state-of-the-art approaches by a large margin of 8.76% mAP on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效双边跨模态簇匹配的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，匹配不同模态下同一身份的行人图像。现有工作主要集中在通过对齐未标注样本的实例级特征来缓解模态差异。然而，跨模态簇之间的关系尚未得到充分探索。为此，我们提出了一种新颖的双边簇匹配学习框架，通过匹配跨模态簇来减少模态差异。具体而言，我们通过在二分图中优化最大匹配问题设计了一种多对多双边跨模态簇匹配（MBCCM）算法。然后，匹配的成对簇在模型训练过程中利用共享的可见光和红外伪标签。在这样的监督信号下，我们提出了一种模态特定和模态无关的对比学习框架，以在簇级联合对齐特征。同时，我们提出了跨模态一致性约束（CC）以显式地减少模态差异。在公开的SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法具有很高的有效性，平均mAP比最先进的方法高出8.76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a novel bilateral cluster matching framework. It introduces a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm to match cross-modality clusters and a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework to align features at a cluster-level. The method also includes a cross-modality consistency constraint to reduce modality discrepancy. Experiments on SYSU-MM01 and RegDB datasets show significant improvement over existing methods, with an average mAP increase of 8.76%.</div>
<div class="mono" style="margin-top:8px">论文提出了一种新颖的双边集群匹配框架来解决无监督的可见光-红外行人再识别问题。该框架包括一种Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) 算法用于匹配跨模态集群，以及一种Modality-Specific and Modality-Agnostic (MSMA) 对比学习框架用于在集群级别对齐特征。同时提出了跨模态一致性约束（CC）以显式地减少模态间的差异。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法在平均mAP上比现有方法高出8.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-02-29T10:37:49+00:00 · Latest: 2024-10-24T09:00:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.19026v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.19026v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotations, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on commonality, overlooking divergence and variety. To address the problem, we propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes method for USVI-ReID. In brief, we generate the hard prototype by selecting the sample with the maximum distance from the cluster center. We theoretically show that the hard prototype is used in the contrastive loss to emphasize divergence. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. The dynamic prototype is used to encourage the variety. Finally, we introduce a progressive learning strategy to gradually shift the model&#x27;s attention towards divergence and variety, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习共性、差异性和多样性以进行无监督可见光-红外人再识别</div>
<div class="mono" style="margin-top:8px">无监督可见光-红外人再识别（USVI-ReID）旨在无需标注的情况下，在红外图像中匹配指定的人并在可见图像中进行匹配，反之亦然。USVI-ReID 是一个具有挑战性但尚未充分探索的任务。现有大多数方法使用基于聚类的对比学习来解决 USVI-ReID，简单地将聚类中心作为人的表示。然而，聚类中心主要关注共性，忽略了差异性和多样性。为了解决这个问题，我们提出了一种用于 USVI-ReID 的渐进对比学习与硬动态原型方法。简而言之，我们通过选择与聚类中心距离最大的样本来生成硬原型。我们从理论上证明，硬原型用于对比损失中以强调差异性。此外，我们不是将查询图像严格对齐到特定的原型，而是通过在聚类内随机选择样本来生成动态原型。动态原型用于鼓励多样性。最后，我们引入了一种渐进学习策略，逐步将模型的注意力转向差异性和多样性，避免聚类退化。在公开的 SYSU-MM01 和 RegDB 数据集上进行的大量实验验证了所提出方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID), focusing on matching individuals between visible and infrared images without annotations. To overcome the limitations of existing cluster-based methods that primarily capture commonality, the authors propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes method. This method introduces hard prototypes to emphasize divergence and dynamic prototypes to encourage variety. Additionally, a progressive learning strategy is employed to shift the model&#x27;s focus towards these aspects, preventing cluster deterioration. Experiments on SYSU-MM01 and RegDB datasets demonstrate the proposed method&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">论文针对无监督可见红外行人重识别（USVI-ReID）任务，关注现有基于聚类的方法主要捕捉共性而忽视差异性和多样性的问题。为此，作者提出了一种渐进对比学习与硬动态原型方法。该方法通过引入硬原型来强调差异性，动态原型来鼓励多样性，并通过渐进学习策略确保模型逐步关注这些方面。实验结果在SYSU-MM01和RegDB数据集上验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-01-12T01:24:04+00:00 · Latest: 2024-07-29T09:40:11+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.06825v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.06825v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a promising yet challenging retrieval task. The key challenges in USL-VI-ReID are to effectively generate pseudo-labels and establish pseudo-label correspondences across modalities without relying on any prior annotations. Recently, clustered pseudo-label methods have gained more attention in USL-VI-ReID. However, previous methods fell short of fully exploiting the individual nuances, as they simply utilized a single memory that represented an identity to establish cross-modality correspondences, resulting in ambiguous cross-modality correspondences. To address the problem, we propose a Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a Cross-Modality Clustering (CMC) module to generate the pseudo-labels through clustering together both two modality samples. To associate cross-modality clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM) module, ensuring that optimization explicitly focuses on the nuances of individual perspectives and establishes reliable cross-modality correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module to narrow the modality gap while mitigating the effect of noise pseudo-labels through a soft many-to-many alignment strategy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the reliability of the established cross-modality correspondences and the effectiveness of our MMM. The source codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多记忆匹配在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）是一项有前景但极具挑战性的检索任务。USL-VI-ReID 的关键挑战在于有效生成伪标签，并在不依赖任何先验注释的情况下在模态间建立伪标签对应关系。最近，聚类伪标签方法在 USL-VI-ReID 中获得了更多关注。然而，之前的方法未能充分利用个体差异，因为它们仅利用单一记忆来表示身份以建立跨模态对应关系，导致跨模态对应关系模糊。为解决这一问题，我们提出了一种多记忆匹配（MMM）框架用于 USL-VI-ReID。我们首先设计了一个跨模态聚类（CMC）模块，通过聚类两个模态样本来生成伪标签。为了关联跨模态聚类伪标签，我们设计了一个多记忆学习和匹配（MMLM）模块，确保优化明确关注个体视角的差异并建立可靠的跨模态对应关系。最后，我们设计了一个软聚类级对齐（SCA）模块，通过软多对多对齐策略缩小模态差距并减轻噪声伪标签的影响。在公开的 SYSU-MM01 和 RegDB 数据集上的广泛实验表明，建立的跨模态对应关系的可靠性以及我们提出的 MMM 的有效性。源代码将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Multi-Memory Matching (MMM) framework. It introduces a Cross-Modality Clustering (CMC) module to generate pseudo-labels and a Multi-Memory Learning and Matching (MMLM) module to establish reliable cross-modality correspondences. Additionally, a Soft Cluster-level Alignment (SCA) module is designed to reduce the modality gap and mitigate noise. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the proposed method in establishing accurate cross-modality correspondences.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多记忆匹配（MMM）框架来解决无监督可见红外行人重识别的挑战。它引入了跨模态聚类（CMC）模块生成伪标签，并设计了多记忆学习和匹配（MMLM）模块来建立可靠的跨模态对应关系。此外，还设计了软簇级对齐（SCA）模块以减少模态差距并减轻噪声的影响。在SYSU-MM01和RegDB数据集上的实验表明，所提出的方法在建立准确的跨模态对应关系方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="https://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a challenging retrieval task that aims to retrieve cross-modality pedestrian images without using any label information. In this task, the large cross-modality variance makes it difficult to generate reliable cross-modality labels, and the lack of annotations also provides additional difficulties for learning modality-invariant features. In this paper, we first deduce an optimization objective for unsupervised VI-ReID based on the mutual information between the model&#x27;s cross-modality input and output. With equivalent derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable cross-modality matching) are obtained. Under their guidance, we design a loop iterative training strategy alternating between model training and cross-modality matching. In the matching stage, a uniform prior guided optimal transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched visible and infrared prototypes. In the training stage, we utilize this matching information to introduce prototype-based contrastive learning for minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive experimental results on benchmarks demonstrate the effectiveness of our method, e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人重识别（USVI-ReID）是一项具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出一个无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一种交替进行模型训练和跨模态匹配的循环迭代训练策略。在匹配阶段，我们提出了一种均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化类内和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如，在SYSU-MM01和RegDB上，无任何注释的情况下，Rank-1精度分别为60.6%和90.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: &#x27;Sharpness&#x27; (entropy minimization), &#x27;Fairness&#x27; (uniform label distribution), and &#x27;Fitness&#x27; (reliable cross-modality matching). The method employs a loop iterative training strategy that alternates between model training and cross-modality matching. Specifically, a uniform prior guided optimal transport assignment is used to select matched visible and infrared prototypes, and prototype-based contrastive learning is introduced to minimize intra- and cross-modality entropy. The method achieves 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div>
<div class="mono" style="margin-top:8px">该论文通过基于互信息的方法解决无监督可见红外行人再识别的挑战，提出了三个学习原则：“Sharpness”（熵最小化）、“Fairness”（均匀标签分布）和“Fitness”（可靠的跨模态匹配）。作者提出了一种交替进行模型训练和跨模态匹配的循环迭代训练策略。具体来说，他们使用均匀先验引导的最优传输分配来选择匹配的原型，并引入基于原型的对比学习来最小化熵。实验结果显示，该方法在SYSU-MM01和RegDB上分别达到了60.6%和90.3%的Rank-1准确率，且无需任何标注。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Xiangbo Yin, Jiangming Shi, Yachao Zhang, Yang Lu, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-05-09T08:17:06+00:00 · Latest: 2024-05-09T08:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.05613v1">Abs</a> · <a href="https://arxiv.org/pdf/2405.05613v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a formidable challenge, which aims to match pedestrian images across visible and infrared modalities without any annotations. Recently, clustered pseudo-label methods have become predominant in USVI-ReID, although the inherent noise in pseudo-labels presents a significant obstacle. Most existing works primarily focus on shielding the model from the harmful effects of noise, neglecting to calibrate noisy pseudo-labels usually associated with hard samples, which will compromise the robustness of the model. To address this issue, we design a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for USVI-ReID. To be specific, we first introduce a straightforward yet potent Noisy Pseudo-label Calibration module to correct noisy pseudo-labels. Due to the high intra-class variations, noisy pseudo-labels are difficult to calibrate completely. Therefore, we introduce a Neighbor Relation Learning module to reduce high intra-class variations by modeling potential interactions between all samples. Subsequently, we devise an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences. On that basis, we design a Memory Hybrid Learning module to jointly learn modality-specific and modality-invariant information. Comprehensive experiments conducted on two widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR outperforms the current state-of-the-art GUR with an average Rank-1 improvement of 10.3%. The source codes will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒的邻居关系伪标签学习方法在无监督可见光-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见光-红外行人重识别（USVI-ReID）面临巨大挑战，旨在在没有标注的情况下匹配可见光和红外模态的行人图像。最近，聚类伪标签方法在USVI-ReID中占据主导地位，尽管伪标签中的固有噪声构成了重大障碍。现有大多数工作主要集中在保护模型免受噪声的负面影响，而忽视了通常与硬样本相关的噪声伪标签的校准，这会损害模型的鲁棒性。为解决这一问题，我们设计了一种鲁棒的邻居关系伪标签学习框架（RPNR）用于USVI-ReID。具体而言，我们首先引入了一个简单而有效的噪声伪标签校准模块来纠正噪声伪标签。由于类内变异性高，噪声伪标签难以完全校准。因此，我们引入了一个邻居关系学习模块，通过建模所有样本之间的潜在交互来降低类内变异性。随后，我们设计了一种最优传输原型匹配模块以建立可靠的跨模态对应关系。在此基础上，我们设计了一种记忆混合学习模块以联合学习模态特定和模态不变信息。在两个广泛认可的基准数据集SYSU-MM01和RegDB上进行的全面实验表明，RPNR在平均Rank-1上优于当前最先进的GUR，提高了10.3%。源代码将很快发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework. It introduces a Noisy Pseudo-label Calibration module to correct noisy pseudo-labels, a Neighbor Relation Learning module to model interactions between samples, and an Optimal Transport Prototype Matching module to establish cross-modality correspondences. Experimental results on SYSU-MM01 and RegDB benchmarks show that RPNR outperforms the current state-of-the-art method GUR by 10.3% in terms of Rank-1 accuracy.</div>
<div class="mono" style="margin-top:8px">论文提出了一种鲁棒伪标签学习与邻域关系（RPNR）框架来解决无监督可见-红外行人重识别（USVI-ReID）的问题。该框架包括一个伪标签校准模块来纠正噪声伪标签，一个邻域关系学习模块来建模样本之间的潜在交互，以及一个最优传输原型匹配模块来建立可靠的跨模态对应关系。在SYSU-MM01和RegDB基准上的实验结果显示，RPNR方法比当前最先进的方法GUR提高了10.3%的平均Rank-1性能。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment</div>
<div class="meta-line">Authors: Yexin Liu, Weiming Zhang, Athanasios V. Vasilakos, Lin Wang</div>
<div class="meta-line">First: 2024-04-10T02:03:14+00:00 · Latest: 2024-04-10T02:03:14+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.06683v1">Abs</a> · <a href="https://arxiv.org/pdf/2404.06683v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) has recently gained great attention due to its potential for enhancing human detection in diverse environments without labeling. Previous methods utilize intra-modality clustering and cross-modality feature matching to achieve UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be generated in the clustering process, and 2) the cross-modality feature alignment via matching the marginal distribution of visible and infrared modalities may misalign the different identities from two modalities. In this paper, we first conduct a theoretic analysis where an interpretable generalization upper bound is introduced. Based on the analysis, we then propose a novel unsupervised cross-modality person re-identification framework (PRAISE). Specifically, to address the first challenge, we propose a pseudo-label correction strategy that utilizes a Beta Mixture Model to predict the probability of mis-clustering based network&#x27;s memory effect and rectifies the correspondence by adding a perceptual term to contrastive learning. Next, we introduce a modality-level alignment strategy that generates paired visible-infrared latent features and reduces the modality gap by aligning the labeling function of visible and infrared features to learn identity discriminative and modality-invariant features. Experimental results on two benchmark datasets demonstrate that our method achieves state-of-the-art performance than the unsupervised visible-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于伪标签校正和模态级对齐的无监督可见光-红外ReID</div>
<div class="mono" style="margin-top:8px">无监督可见光-红外人员重识别（UVI-ReID）由于其在不同环境中增强人类检测的潜力而引起了广泛关注，无需标注。先前的方法利用同模态聚类和跨模态特征匹配来实现UVI-ReID。然而，存在两个挑战：1）聚类过程中可能会生成噪声伪标签；2）通过匹配可见光和红外模态的边缘分布来进行跨模态特征对齐可能会导致两种模态不同身份的错位。在本文中，我们首先进行理论分析，引入了一个可解释的泛化上界。基于分析，我们提出了一种新颖的无监督跨模态人员重识别框架（PRAISE）。具体而言，为了解决第一个挑战，我们提出了一种伪标签校正策略，利用Beta混合模型预测基于网络记忆效应的误聚类概率，并通过在对比学习中添加感知项来纠正对应关系。接下来，我们引入了一种模态级对齐策略，生成可见光-红外配对的潜在特征，并通过对齐可见光和红外特征的标签函数来减少模态差距，从而学习身份判别性和模态不变性的特征。在两个基准数据集上的实验结果表明，我们的方法在无监督可见光-ReID方法中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in unsupervised visible-infrared person re-identification by proposing a novel framework called PRAISE. It introduces a pseudo-label correction strategy using a Beta Mixture Model to rectify mis-clustering and a modality-level alignment strategy to align the labeling functions of visible and infrared features. The method outperforms existing unsupervised visible-ReID methods on two benchmark datasets.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的无监督可见-红外人再识别框架PRAISE，以解决伪标签噪声和跨模态身份错位的问题。该方法使用Beta混合模型和感知项的伪标签校正策略来改进聚类过程，并使用模态级对齐策略对可见和红外特征的标签函数进行对齐，以学习身份判别性和模态不变性特征。实验结果表明，PRAISE在两个基准数据集上的性能优于现有的无监督可见-ReID方法。</div>
</details>
</div>
<div class="card">
<div class="title">Application of machine learning to gas flaring</div>
<div class="meta-line">Authors: Rong Lu</div>
<div class="meta-line">First: 2023-01-11T03:30:10+00:00 · Latest: 2023-01-11T03:30:10+00:00</div>
<div class="meta-line">Comments: Doctoral dissertation, Colorado School of Mines, 2020. See https://hdl.handle.net/11124/176331</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2301.04141v1">Abs</a> · <a href="https://arxiv.org/pdf/2301.04141v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Currently in the petroleum industry, operators often flare the produced gas instead of commodifying it. The flaring magnitudes are large in some states, which constitute problems with energy waste and CO2 emissions. In North Dakota, operators are required to estimate and report the volume flared. The questions are, how good is the quality of this reporting, and what insights can be drawn from it? Apart from the company-reported statistics, which are available from the North Dakota Industrial Commission (NDIC), flared volumes can be estimated via satellite remote sensing, serving as an unbiased benchmark. Since interpretation of the Landsat 8 imagery is hindered by artifacts due to glow, the estimated volumes based on the Visible Infrared Imaging Radiometer Suite (VIIRS) are used. Reverse geocoding is performed for comparing and contrasting the NDIC and VIIRS data at different levels, such as county and oilfield. With all the data gathered and preprocessed, Bayesian learning implemented by MCMC methods is performed to address three problems: county level model development, flaring time series analytics, and distribution estimation. First, there is heterogeneity among the different counties, in the associations between the NDIC and VIIRS volumes. In light of such, models are developed for each county by exploiting hierarchical models. Second, the flaring time series, albeit noisy, contains information regarding trends and patterns, which provide some insights into operator approaches. Gaussian processes are found to be effective in many different pattern recognition scenarios. Third, distributional insights are obtained through unsupervised learning. The negative binomial and GMMs are found to effectively describe the oilfield flare count and flared volume distributions, respectively. Finally, a nearest-neighbor-based approach for operator level monitoring and analytics is introduced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器学习在天然气放空应用中的研究</div>
<div class="mono" style="margin-top:8px">目前在石油行业中，操作者通常会放空产生的天然气而不是将其商品化。在某些州，放空量很大，这构成了能源浪费和二氧化碳排放的问题。在北达科他州，操作者被要求估计和报告放空的体积。问题是，这些报告的质量如何？从中可以得出什么见解？除了来自北达科他州工业委员会（NDIC）的公司报告统计数据外，还可以通过卫星遥感估算放空体积，作为无偏基准。由于Landsat 8图像受到由于发光引起的伪影影响，因此使用了基于可见红外成像辐射计套件（VIIRS）的估算体积。通过逆地理编码比较NDIC和VIIRS数据，以不同级别（如县和油田）进行比较。收集并预处理所有数据后，通过MCMC方法实施贝叶斯学习以解决三个问题：县一级模型开发、放空时间序列分析和分布估计。首先，不同县之间存在异质性，NDIC和VIIRS体积之间的关联性不同。因此，通过利用分层模型为每个县开发了模型。其次，尽管放空时间序列噪声较大，但其中包含有关趋势和模式的信息，提供了有关操作者方法的一些见解。高斯过程在许多不同的模式识别场景中被发现是有效的。第三，通过无监督学习获得了分布见解。负二项分布和GMM分别有效地描述了油田放空次数和放空体积的分布。最后，介绍了一种基于最近邻的方法进行操作者级别监控和分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to assess the quality of reported flaring volumes by operators in North Dakota and explore insights from satellite data. The research uses Bayesian learning with MCMC methods to develop county-specific models, analyze flaring time series, and estimate distributions. Key findings include county-specific heterogeneity in the relationship between reported and satellite-estimated flaring volumes, effective use of Gaussian processes for trend analysis, and the negative binomial and Gaussian mixture models for describing flaring distributions.</div>
<div class="mono" style="margin-top:8px">研究旨在评估北达科他州运营商报告的排放量质量，并探索卫星数据中的见解。研究使用MCMC方法的贝叶斯学习来开发县特定模型、分析排放时间序列并估计分布。关键发现包括报告和卫星估计排放量之间关系的县内异质性，高斯过程在识别排放时间序列中的模式方面非常有效，以及负二项式和高斯混合模型分别准确描述了油井排放次数和排放量的分布。此外，还提出了一种基于最近邻的方法，用于运营商级别的监控和分析。</div>
</details>
</div>
<div class="card">
<div class="title">Interactive Feature Embedding for Infrared and Visible Image Fusion</div>
<div class="meta-line">Authors: Fan Zhao, Wenda Zhao, Huchuan Lu</div>
<div class="meta-line">First: 2022-11-09T13:34:42+00:00 · Latest: 2022-11-09T13:34:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2211.04877v1">Abs</a> · <a href="https://arxiv.org/pdf/2211.04877v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General deep learning-based methods for infrared and visible image fusion rely on the unsupervised mechanism for vital information retention by utilizing elaborately designed loss functions. However, the unsupervised mechanism depends on a well designed loss function, which cannot guarantee that all vital information of source images is sufficiently extracted. In this work, we propose a novel interactive feature embedding in self-supervised learning framework for infrared and visible image fusion, attempting to overcome the issue of vital information degradation. With the help of self-supervised learning framework, hierarchical representations of source images can be efficiently extracted. In particular, interactive feature embedding models are tactfully designed to build a bridge between the self-supervised learning and infrared and visible image fusion learning, achieving vital information retention. Qualitative and quantitative evaluations exhibit that the proposed method performs favorably against state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>红外和可见光图像融合的交互式特征嵌入</div>
<div class="mono" style="margin-top:8px">基于一般深度学习的方法在红外和可见光图像融合中依赖于利用精心设计的损失函数来保留重要信息的无监督机制。然而，无监督机制依赖于精心设计的损失函数，不能保证所有源图像的重要信息都被充分提取。在本文中，我们提出了一种新的在自监督学习框架下的交互式特征嵌入方法，旨在克服重要信息退化的问题。借助自监督学习框架，可以高效地提取源图像的层次表示。特别是，交互式特征嵌入模型巧妙地设计以在自监督学习和红外与可见光图像融合学习之间建立桥梁，实现重要信息的保留。定性和定量评估表明，所提出的方法优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a novel interactive feature embedding method in a self-supervised learning framework for infrared and visible image fusion. The method aims to overcome the issue of vital information degradation by utilizing hierarchical representations and interactive feature embedding models. Experimental results show that the proposed method outperforms state-of-the-art methods in both qualitative and quantitative evaluations.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决无监督方法的局限性，提高红外和可见光图像融合中关键信息的保留。作者提出了一种新颖的交互式特征嵌入方法，基于自我监督学习框架来增强层次表示的提取。实验结果表明，所提出的方法在定性和定量评估中均优于现有最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration</div>
<div class="meta-line">Authors: Di Wang, Jinyuan Liu, Xin Fan, Risheng Liu</div>
<div class="meta-line">First: 2022-05-24T07:51:57+00:00 · Latest: 2022-05-24T07:51:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2205.11876v1">Abs</a> · <a href="https://arxiv.org/pdf/2205.11876v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent learning-based image fusion methods have marked numerous progress in pre-registered multi-modality data, but suffered serious ghosts dealing with misaligned multi-modality data, due to the spatial deformation and the difficulty narrowing cross-modality discrepancy. To overcome the obstacles, in this paper, we present a robust cross-modality generation-registration paradigm for unsupervised misaligned infrared and visible image fusion (IVIF). Specifically, we propose a Cross-modality Perceptual Style Transfer Network (CPSTN) to generate a pseudo infrared image taking a visible image as input. Benefiting from the favorable geometry preservation ability of the CPSTN, the generated pseudo infrared image embraces a sharp structure, which is more conducive to transforming cross-modality image alignment into mono-modality registration coupled with the structure-sensitive of the infrared image. In this case, we introduce a Multi-level Refinement Registration Network (MRRN) to predict the displacement vector field between distorted and pseudo infrared images and reconstruct registered infrared image under the mono-modality setting. Moreover, to better fuse the registered infrared images and visible images, we present a feature Interaction Fusion Module (IFM) to adaptively select more meaningful features for fusion in the Dual-path Interaction Fusion Network (DIFN). Extensive experimental results suggest that the proposed method performs superior capability on misaligned cross-modality image fusion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于跨模态图像生成和配准的无监督红外和可见光图像融合</div>
<div class="mono" style="margin-top:8px">基于学习的图像融合方法在预配准的多模态数据方面取得了显著进展，但在处理错位的多模态数据时，由于空间变形和跨模态差异难以缩小，遭受了严重的鬼影问题。为克服这些障碍，本文提出了一种鲁棒的跨模态生成配准框架，用于无监督的红外和可见光图像融合（IVIF）。具体而言，我们提出了一种跨模态感知风格迁移网络（CPSTN），以可见光图像为输入生成伪红外图像。得益于CPSTN良好的几何保持能力，生成的伪红外图像具有清晰的结构，这更有利于将跨模态图像对齐转化为单模态配准，结合红外图像的结构敏感性。在这种情况下，我们引入了一种多级细化配准网络（MRRN）来预测失真和伪红外图像之间的位移向量场，并在单模态设置下重建配准的红外图像。此外，为了更好地融合配准的红外图像和可见光图像，我们提出了一种特征交互融合模块（IFM），在双路径交互融合网络（DIFN）中自适应选择更具有意义的特征进行融合。广泛的实验结果表明，所提出的方法在跨模态图像融合方面具有更强的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fusing misaligned infrared and visible images by proposing a cross-modality generation-registration paradigm. It introduces a Cross-modality Perceptual Style Transfer Network (CPSTN) to generate a pseudo infrared image from a visible image, and a Multi-level Refinement Registration Network (MRRN) to align the pseudo infrared image with the original distorted one. Additionally, a Dual-path Interaction Fusion Network (DIFN) with a Feature Interaction Fusion Module (IFM) is used to fuse the registered infrared and visible images. The method demonstrates superior performance in handling misaligned cross-modality image fusion compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本文提出了一种稳健的跨模态生成-注册框架，用于融合错位的红外和可见光图像。该框架包括一个跨模态感知风格迁移网络（CPSTN），用于从可见光图像生成伪红外图像，以及一个多级细化注册网络（MRRN），用于对齐图像。此外，还提出了一种特征交互融合模块（IFM），用于在双路径交互融合网络（DIFN）中适应性地选择更多有意义的特征进行融合。实验结果表明，该方法在处理跨模态图像融合方面具有优越的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras</div>
<div class="meta-line">Authors: Yubin Guo, Haobo Jiang, Xinlei Qi, Jin Xie, Cheng-Zhong Xu, Hui Kong</div>
<div class="meta-line">First: 2022-04-30T12:58:35+00:00 · Latest: 2022-04-30T12:58:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2205.00257v1">Abs</a> · <a href="https://arxiv.org/pdf/2205.00257v1">PDF</a> · <a href="https://github.com/whitecrow1027/VIS-TIR-Datasets">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-spectrum depth estimation aims to provide a depth map in all illumination conditions with a pair of dual-spectrum images. It is valuable for autonomous vehicle applications when the vehicle is equipped with two cameras of different modalities. However, images captured by different-modality cameras can be photometrically quite different. Therefore, cross-spectrum depth estimation is a very challenging problem. Moreover, the shortage of large-scale open-source datasets also retards further research in this field. In this paper, we propose an unsupervised visible-light image guided cross-spectrum (i.e., thermal and visible-light, TIR-VIS in short) depth estimation framework given a pair of RGB and thermal images captured from a visible-light camera and a thermal one. We first adopt a base depth estimation network using RGB-image pairs. Then we propose a multi-scale feature transfer network to transfer features from the TIR-VIS domain to the VIS domain at the feature level to fit the trained depth estimation network. At last, we propose a cross-spectrum depth cycle consistency to improve the depth result of dual-spectrum image pairs. Meanwhile, we release a large dual-spectrum depth estimation dataset with visible-light and far-infrared stereo images captured in different scenes to the society. The experiment result shows that our method achieves better performance than the compared existing methods. Our datasets is available at https://github.com/whitecrow1027/VIS-TIR-Datasets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of cross-spectrum depth estimation using a pair of dual-modality images, specifically thermal and visible-light images. The authors propose an unsupervised framework that uses a base depth estimation network trained on RGB images and a multi-scale feature transfer network to adapt thermal features to the visible-light domain. The method also includes a cross-spectrum depth cycle consistency to enhance the depth estimation. The authors also release a large dataset for dual-spectrum depth estimation. Experiments show that their approach outperforms existing methods.</div>
<div class="mono" style="margin-top:8px">论文针对双模态图像的跨谱深度估计挑战，特别是为自动驾驶车辆设计。提出了一种无监督框架，使用基于RGB图像的基深度估计网络，并通过多尺度特征转移网络将热图像适应可见光域。该方法还包括跨谱深度循环一致性以提高深度准确性。作者还发布了包含可见光和远红外立体图像的大规模数据集，用于该领域的研究。实验结果表明，该方法在性能上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Image Fusion Method based on Feature Mutual Mapping</div>
<div class="meta-line">Authors: Dongyu Rao, Xiao-Jun Wu, Tianyang Xu, Guoyang Chen</div>
<div class="meta-line">First: 2022-01-25T07:50:14+00:00 · Latest: 2022-01-29T12:27:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2201.10152v2">Abs</a> · <a href="https://arxiv.org/pdf/2201.10152v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning-based image fusion approaches have obtained wide attention in recent years, achieving promising performance in terms of visual perception. However, the fusion module in the current deep learning-based methods suffers from two limitations, \textit{i.e.}, manually designed fusion function, and input-independent network learning. In this paper, we propose an unsupervised adaptive image fusion method to address the above issues. We propose a feature mutual mapping fusion module and dual-branch multi-scale autoencoder. More specifically, we construct a global map to measure the connections of pixels between the input source images. % The found mapping relationship guides the image fusion. Besides, we design a dual-branch multi-scale network through sampling transformation to extract discriminative image features. We further enrich feature representations of different scales through feature aggregation in the decoding process. Finally, we propose a modified loss function to train the network with efficient convergence property. Through sufficient training on infrared and visible image data sets, our method also shows excellent generalized performance in multi-focus and medical image fusion. Our method achieves superior performance in both visual perception and objective evaluation. Experiments prove that the performance of our proposed method on a variety of image fusion tasks surpasses other state-of-the-art methods, proving the effectiveness and versatility of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于特征互映射的无监督图像融合方法</div>
<div class="mono" style="margin-top:8px">基于深度学习的图像融合方法近年来获得了广泛关注，在视觉感知方面取得了令人瞩目的性能。然而，当前基于深度学习的方法的融合模块存在两个局限性，即人工设计的融合函数和输入无关的网络学习。在本文中，我们提出了一种无监督自适应图像融合方法来解决上述问题。我们提出了一种特征互映射融合模块和双分支多尺度自编码器。具体来说，我们构建了一个全局映射来衡量输入源图像之间像素的连接关系。找到的映射关系指导图像融合。此外，我们通过采样变换设计了一种双分支多尺度网络来提取具有区分性的图像特征。我们还在解码过程中通过特征聚合进一步丰富了不同尺度的特征表示。最后，我们提出了一种改进的损失函数来训练网络，具有高效的收敛特性。通过在红外和可见光图像数据集上充分训练，我们的方法在多焦点和医学图像融合方面也表现出优秀的泛化性能。我们的方法在视觉感知和客观评估方面均取得了优异的性能。实验表明，我们提出的方法在各种图像融合任务上的性能超越了其他最先进的方法，证明了我们方法的有效性和通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes an unsupervised adaptive image fusion method to address the limitations of manually designed fusion functions and input-independent network learning in current deep learning-based methods. It introduces a feature mutual mapping fusion module and a dual-branch multi-scale autoencoder. The method constructs a global map to measure pixel connections between input images and uses a dual-branch network to extract discriminative features. Experimental results show that the proposed method outperforms other state-of-the-art methods in both visual perception and objective evaluation, demonstrating its effectiveness and versatility in various image fusion tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了一种无监督自适应图像融合方法，以解决当前基于深度学习的图像融合方法中手动设计的融合函数和输入无关的网络学习的局限性。该方法引入了特征互映射融合模块和双分支多尺度自编码器。通过构建全局图来测量输入图像之间的像素连接，并使用双分支多尺度网络提取判别性特征。实验结果表明，所提出的方法在多种图像融合任务中在视觉感知和客观评估方面均优于其他最先进的方法，证明了该方法的有效性和通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-UDA: Unsupervised Domain Adaptive Thermal Object Detection using Meta-Learning</div>
<div class="meta-line">Authors: Vibashan VS, Domenick Poster, Suya You, Shuowen Hu, Vishal M. Patel</div>
<div class="meta-line">Venue: WACV 2022</div>
<div class="meta-line">First: 2021-10-07T02:28:18+00:00 · Latest: 2021-10-07T02:28:18+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2022</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2110.03143v1">Abs</a> · <a href="https://arxiv.org/pdf/2110.03143v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object detectors trained on large-scale RGB datasets are being extensively employed in real-world applications. However, these RGB-trained models suffer a performance drop under adverse illumination and lighting conditions. Infrared (IR) cameras are robust under such conditions and can be helpful in real-world applications. Though thermal cameras are widely used for military applications and increasingly for commercial applications, there is a lack of robust algorithms to robustly exploit the thermal imagery due to the limited availability of labeled thermal data. In this work, we aim to enhance the object detection performance in the thermal domain by leveraging the labeled visible domain data in an Unsupervised Domain Adaptation (UDA) setting. We propose an algorithm agnostic meta-learning framework to improve existing UDA methods instead of proposing a new UDA strategy. We achieve this by meta-learning the initial condition of the detector, which facilitates the adaptation process with fine updates without overfitting or getting stuck at local optima. However, meta-learning the initial condition for the detection scenario is computationally heavy due to long and intractable computation graphs. Therefore, we propose an online meta-learning paradigm which performs online updates resulting in a short and tractable computation graph. To this end, we demonstrate the superiority of our method over many baselines in the UDA setting, producing a state-of-the-art thermal detector for the KAIST and DSIAC datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Meta-UDA：使用元学习的无监督领域适应热目标检测</div>
<div class="mono" style="margin-top:8px">在大规模RGB数据集上训练的目标检测器被广泛应用于实际应用中。然而，这些RGB训练模型在不良光照和照明条件下性能下降。红外（IR）相机在这种条件下表现稳健，有助于实际应用。尽管热成像相机广泛用于军事应用，并且越来越多地用于商业应用，但由于标注的热图像数据有限，缺乏稳健的算法来充分利用热图像信息。在本文中，我们旨在通过在无监督领域适应（UDA）设置中利用可见光域的标注数据来提高热域的目标检测性能。我们提出了一种算法无关的元学习框架，以改进现有的UDA方法，而不是提出新的UDA策略。我们通过元学习检测器的初始条件来实现这一点，这有助于适应过程中的微调更新，而不至于过拟合或陷入局部最优。然而，由于长且难以处理的计算图，检测场景中的检测器初始条件的元学习计算量很大。因此，我们提出了一种在线元学习范式，进行在线更新，从而产生简短且易于处理的计算图。为此，我们在UDA设置中展示了我们方法优于许多基线方法的优越性，生成了KAIST和DSIAC数据集上的最先进的热检测器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance drop of RGB-trained object detectors under adverse lighting conditions by leveraging labeled visible data for thermal object detection using an unsupervised domain adaptation (UDA) approach. The authors propose a meta-learning framework to improve existing UDA methods by meta-learning the initial condition of the detector, which enables efficient adaptation without overfitting. The method is computationally optimized through an online meta-learning paradigm, resulting in a short and tractable computation graph. Experimental results on the KAIST and DSIAC datasets show that the proposed method outperforms many baselines and achieves state-of-the-art performance in thermal object detection.</div>
<div class="mono" style="margin-top:8px">研究旨在通过利用标记的可见光数据在无监督领域适应设置中提高热成像中的目标检测性能。作者提出了一种元学习框架来优化检测器的初始条件，从而实现高效的适应过程，避免过拟合。实验表明，该方法在KAIST和DSIAC数据集上优于现有基线，达到最先进的性能。通过在线更新，该方法减少了计算复杂性，使其更适合实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">FuseVis: Interpreting neural networks for image fusion using per-pixel saliency visualization</div>
<div class="meta-line">Authors: Nishant Kumar, Stefan Gumhold</div>
<div class="meta-line">First: 2020-12-06T10:03:02+00:00 · Latest: 2020-12-06T10:03:02+00:00</div>
<div class="meta-line">Comments: 30 pages, 9 figures, MDPI Journal (Computers)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2012.08932v1">Abs</a> · <a href="https://arxiv.org/pdf/2012.08932v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image fusion helps in merging two or more images to construct a more informative single fused image. Recently, unsupervised learning based convolutional neural networks (CNN) have been utilized for different types of image fusion tasks such as medical image fusion, infrared-visible image fusion for autonomous driving as well as multi-focus and multi-exposure image fusion for satellite imagery. However, it is challenging to analyze the reliability of these CNNs for the image fusion tasks since no groundtruth is available. This led to the use of a wide variety of model architectures and optimization functions yielding quite different fusion results. Additionally, due to the highly opaque nature of such neural networks, it is difficult to explain the internal mechanics behind its fusion results. To overcome these challenges, we present a novel real-time visualization tool, named FuseVis, with which the end-user can compute per-pixel saliency maps that examine the influence of the input image pixels on each pixel of the fused image. We trained several image fusion based CNNs on medical image pairs and then using our FuseVis tool, we performed case studies on a specific clinical application by interpreting the saliency maps from each of the fusion methods. We specifically visualized the relative influence of each input image on the predictions of the fused image and showed that some of the evaluated image fusion methods are better suited for the specific clinical application. To the best of our knowledge, currently, there is no approach for visual analysis of neural networks for image fusion. Therefore, this work opens up a new research direction to improve the interpretability of deep fusion networks. The FuseVis tool can also be adapted in other deep neural network based image processing applications to make them interpretable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FuseVis：使用像素级显著性可视化解释图像融合的神经网络</div>
<div class="mono" style="margin-top:8px">图像融合有助于将两张或多张图像合并成一张更具信息量的融合图像。近年来，基于无监督学习的卷积神经网络（CNN）被用于不同类型的图像融合任务，如医学图像融合、自动驾驶中的红外可见光图像融合以及卫星图像的多焦点和多曝光图像融合。然而，由于没有地面真实值可供参考，分析这些CNN在图像融合任务中的可靠性具有挑战性。这导致了广泛使用不同模型架构和优化函数，从而产生了不同的融合结果。此外，由于这些神经网络的高度不透明性，很难解释其融合结果背后的内部机制。为了解决这些挑战，我们提出了一种名为FuseVis的新型实时可视化工具，用户可以使用该工具计算每个输入图像像素对融合图像中每个像素影响的像素级显著性图。我们对医学图像对训练了几种基于图像融合的CNN，然后使用我们的FuseVis工具，对特定临床应用进行了案例研究，通过解释每种融合方法的显著性图来解释融合结果。我们特别可视化了每张输入图像对融合图像预测的相对影响，并展示了其中一些评估的图像融合方法更适合特定的临床应用。据我们所知，目前没有方法用于神经网络的图像融合可视化分析。因此，这项工作为提高深度融合网络的可解释性开辟了新的研究方向。FuseVis工具也可以适应其他基于深度神经网络的图像处理应用，使其可解释。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper presents FuseVis, a tool for visualizing the influence of input pixels on the output of image fusion neural networks. Motivated by the need to understand and improve the reliability of unsupervised CNNs used in image fusion, the authors developed FuseVis to generate per-pixel saliency maps. Key findings include the ability to interpret the internal mechanics of different fusion methods and identify which methods are more suitable for specific clinical applications.</div>
<div class="mono" style="margin-top:8px">论文介绍了FuseVis，这是一种用于可视化图像融合神经网络中输入像素对输出影响的工具。作者受解释和提高各种图像融合任务中所用的无监督CNN可靠性的需求驱动，开发了FuseVis以生成像素级的显著性图。主要发现包括能够识别哪些输入图像对融合图像的影响更大，并且某些融合方法更适合特定的临床应用。这项工作增强了深度融合网络的可解释性，并开辟了新的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search</div>
<div class="meta-line">Authors: Prashant Pandey, Aayush Kumar Tyagi, Sameer Ambekar, Prathosh AP</div>
<div class="meta-line">Venue: ECCV 2020 Spotlight</div>
<div class="meta-line">First: 2020-06-15T19:07:55+00:00 · Latest: 2020-07-17T12:07:42+00:00</div>
<div class="meta-line">Comments: ECCV 2020 [Spotlight]</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2006.08696v2">Abs</a> · <a href="https://arxiv.org/pdf/2006.08696v2">PDF</a> · <a href="https://github.com/ambekarsameer96/GLSS">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Segmentation of the pixels corresponding to human skin is an essential first step in multiple applications ranging from surveillance to heart-rate estimation from remote-photoplethysmography. However, the existing literature considers the problem only in the visible-range of the EM-spectrum which limits their utility in low or no light settings where the criticality of the application is higher. To alleviate this problem, we consider the problem of skin segmentation from the Near-infrared images. However, Deep learning based state-of-the-art segmentation techniques demands large amounts of labelled data that is unavailable for the current problem. Therefore we cast the skin segmentation problem as that of target-independent Unsupervised Domain Adaptation (UDA) where we use the data from the Red-channel of the visible-range to develop skin segmentation algorithm on NIR images. We propose a method for target-independent segmentation where the &#x27;nearest-clone&#x27; of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. We prove the existence of &#x27;nearest-clone&#x27; and propose a method to find it through an optimization algorithm over the latent space of a Deep generative model based on variational inference. We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain despite not having access to the target NIR data. Additionally, we report state-of-the-art results for adaption from Synthia to Cityscapes which is a popular setting in Unsupervised Domain Adaptation for semantic segmentation. The code and datasets are available at https://github.com/ambekarsameer96/GLSS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督领域适应在近红外图像语义分割中的皮肤分割</div>
<div class="mono" style="margin-top:8px">人类皮肤像素的分割是多个应用（从监视到远程光电流图估计心率）中的一个必不可少的初始步骤。然而，现有文献仅考虑了电磁波谱的可见光范围，这限制了它们在低光或无光设置中的应用，而在这些设置中，应用的紧迫性更高。为了解决这个问题，我们考虑了从近红外图像中分割皮肤的问题。然而，基于深度学习的最先进的分割技术需要大量的标注数据，而当前问题中这些数据是不可用的。因此，我们将皮肤分割问题重新定义为目标无关的无监督领域适应（UDA）问题，使用可见光范围的红通道数据来开发仅在源域上训练的分割网络。我们提出了一种目标无关的分割方法，其中在源域中搜索目标图像的“最近克隆”并将其用作仅在源域上训练的分割网络的代理。我们证明了“最近克隆”的存在，并提出了一种通过基于变分推断的深度生成模型的潜在空间上的优化算法来查找它的方法。尽管没有访问目标近红外数据，我们证明了所提出的方法在两个新创建的近红外领域皮肤分割数据集上的有效性，超过了最先进的无监督领域适应分割方法。此外，我们报告了从Synthia到Cityscapes的适应结果，这是无监督领域适应中语义分割的一个流行设置。代码和数据集可在https://github.com/ambekarsameer96/GLSS/获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of skin segmentation in Near-infrared (NIR) images, which is crucial for applications in low-light conditions. Due to the lack of labeled NIR data, the authors propose an unsupervised domain adaptation method using visible-range data as a proxy. They search for the &#x27;nearest-clone&#x27; of a target image in the source domain and use it to train a segmentation network. The method is validated on two newly created NIR skin segmentation datasets and shows superior performance compared to existing UDA methods. Additionally, it achieves state-of-the-art results for adapting from Synthia to Cityscapes, a common benchmark in UDA for semantic segmentation.</div>
<div class="mono" style="margin-top:8px">研究旨在解决在近红外（NIR）图像中进行皮肤分割的问题，这对于低光条件下的应用至关重要。由于缺乏NIR数据的标注，研究提出了一种无监督域适应方法，使用可见光范围的数据作为代理。该方法在源域中搜索目标图像的‘最近克隆’并使用它来训练分割网络。实验表明，所提出的方法在两个新创建的NIR皮肤分割数据集上优于现有UDA技术，并且在从Synthia到Cityscapes的无监督域适应语义分割中达到了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">A Cross-Modal Image Fusion Method Guided by Human Visual Characteristics</div>
<div class="meta-line">Authors: Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Yanning Zhang</div>
<div class="meta-line">First: 2019-12-18T13:07:20+00:00 · Latest: 2020-06-20T09:43:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/1912.08577v4">Abs</a> · <a href="https://arxiv.org/pdf/1912.08577v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The characteristics of feature selection, nonlinear combination and multi-task auxiliary learning mechanism of the human visual perception system play an important role in real-world scenarios, but the research of image fusion theory based on the characteristics of human visual perception is less. Inspired by the characteristics of human visual perception, we propose a robust multi-task auxiliary learning optimization image fusion theory. Firstly, we combine channel attention model with nonlinear convolutional neural network to select features and fuse nonlinear features. Then, we analyze the impact of the existing image fusion loss on the image fusion quality, and establish the multi-loss function model of unsupervised learning network. Secondly, aiming at the multi-task auxiliary learning mechanism of human visual perception system, we study the influence of multi-task auxiliary learning mechanism on image fusion task on the basis of single task multi-loss network model. By simulating the three characteristics of human visual perception system, the fused image is more consistent with the mechanism of human brain image fusion. Finally, in order to verify the superiority of our algorithm, we carried out experiments on the combined vision system image data set, and extended our algorithm to the infrared and visible image and the multi-focus image public data set for experimental verification. The experimental results demonstrate the superiority of our fusion theory over state-of-arts in generality and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种受人类视觉特性指导的跨模态图像融合方法</div>
<div class="mono" style="margin-top:8px">人类视觉感知系统的特征选择、非线性组合和多任务辅助学习机制在现实场景中起着重要作用，但基于人类视觉感知特性的图像融合理论研究较少。受人类视觉感知特性启发，我们提出了一种鲁棒的多任务辅助学习优化图像融合理论。首先，我们将通道注意力模型与非线性卷积神经网络结合，选择特征并融合非线性特征。然后，我们分析现有图像融合损失对图像融合质量的影响，并建立了无监督学习网络的多损失函数模型。其次，在单任务多损失网络模型的基础上，针对人类视觉感知系统中的多任务辅助学习机制，研究了多任务辅助学习机制对图像融合任务的影响。通过模拟人类视觉感知系统的三种特性，融合图像更符合人类大脑图像融合机制。最后，为了验证我们算法的优越性，我们在联合视觉系统图像数据集上进行了实验，并将我们的算法扩展到红外和可见光图像以及多焦距图像公共数据集进行实验验证。实验结果表明，我们的融合理论在通用性和鲁棒性方面优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a robust image fusion method inspired by human visual perception characteristics. It uses a channel attention model combined with a nonlinear convolutional neural network for feature selection and fusion, and establishes a multi-loss function model for unsupervised learning. The method also incorporates a multi-task auxiliary learning mechanism to improve image fusion quality. Experimental results on various image datasets show that this method outperforms existing techniques in terms of generality and robustness.</div>
<div class="mono" style="margin-top:8px">本文提出了一种受人类视觉感知特性启发的鲁棒多任务辅助学习优化图像融合理论。结合通道注意力模型和非线性卷积神经网络来选择和融合非线性特征。作者建立了无监督学习的多损失函数模型，并研究了多任务辅助学习机制对图像融合任务的影响。实验结果表明，该方法在通用性和鲁棒性方面优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251206_0327.html">20251206_0327</a>
<a href="archive/20251205_0328.html">20251205_0328</a>
<a href="archive/20251204_0328.html">20251204_0328</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0332.html">20251202_0332</a>
<a href="archive/20251201_0325.html">20251201_0325</a>
<a href="archive/20251130_0324.html">20251130_0324</a>
<a href="archive/20251129_0326.html">20251129_0326</a>
<a href="archive/20251128_0325.html">20251128_0325</a>
<a href="archive/20251127_0325.html">20251127_0325</a>
<a href="archive/20251126_0326.html">20251126_0326</a>
<a href="archive/20251125_0324.html">20251125_0324</a>
<a href="archive/20251124_0325.html">20251124_0325</a>
<a href="archive/20251123_0324.html">20251123_0324</a>
<a href="archive/20251122_0326.html">20251122_0326</a>
<a href="archive/20251121_0325.html">20251121_0325</a>
<a href="archive/20251120_0326.html">20251120_0326</a>
<a href="archive/20251119_0325.html">20251119_0325</a>
<a href="archive/20251118_0325.html">20251118_0325</a>
<a href="archive/20251117_0323.html">20251117_0323</a>
<a href="archive/20251116_0322.html">20251116_0322</a>
<a href="archive/20251115_0325.html">20251115_0325</a>
<a href="archive/20251114_0325.html">20251114_0325</a>
<a href="archive/20251113_0326.html">20251113_0326</a>
<a href="archive/20251112_0325.html">20251112_0325</a>
<a href="archive/20251111_0322.html">20251111_0322</a>
<a href="archive/20251110_0320.html">20251110_0320</a>
<a href="archive/20251109_0320.html">20251109_0320</a>
<a href="archive/20251108_0322.html">20251108_0322</a>
<a href="archive/20251107_0322.html">20251107_0322</a>
<a href="archive/20251106_0323.html">20251106_0323</a>
<a href="archive/20251105_0322.html">20251105_0322</a>
<a href="archive/20251104_0322.html">20251104_0322</a>
<a href="archive/20251103_0320.html">20251103_0320</a>
<a href="archive/20251102_0319.html">20251102_0319</a>
<a href="archive/20251101_0329.html">20251101_0329</a>
<a href="archive/20251031_0322.html">20251031_0322</a>
<a href="archive/20251030_0325.html">20251030_0325</a>
<a href="archive/20251029_0324.html">20251029_0324</a>
<a href="archive/20251028_1054.html">20251028_1054</a>
<a href="archive/20251028_0320.html">20251028_0320</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0325.html">20251026_0325</a>
<a href="archive/20251025_0327.html">20251025_0327</a>
<a href="archive/20251024_0327.html">20251024_0327</a>
<a href="archive/20251023_0327.html">20251023_0327</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0329.html">20251021_0329</a>
<a href="archive/20251020_0318.html">20251020_0318</a>
<a href="archive/20251019_0327.html">20251019_0327</a>
<a href="archive/20251018_0326.html">20251018_0326</a>
<a href="archive/20251017_0325.html">20251017_0325</a>
<a href="archive/20251016_0321.html">20251016_0321</a>
<a href="archive/20251015_0327.html">20251015_0327</a>
<a href="archive/20251014_0326.html">20251014_0326</a>
<a href="archive/20251012_0325.html">20251012_0325</a>
<a href="archive/20251011_0327.html">20251011_0327</a>
<a href="archive/20251010_0328.html">20251010_0328</a>
<a href="archive/20251009_0319.html">20251009_0319</a>
<a href="archive/20251008_0344.html">20251008_0344</a>
<a href="archive/20251007_0345.html">20251007_0345</a>
<a href="archive/20251006_0350.html">20251006_0350</a>
<a href="archive/20251005_0349.html">20251005_0349</a>
<a href="archive/20251004_0351.html">20251004_0351</a>
<a href="archive/20251003_0351.html">20251003_0351</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0340.html">20251001_0340</a>
<a href="archive/20250930_0357.html">20250930_0357</a>
<a href="archive/20250929_0354.html">20250929_0354</a>
<a href="archive/20250928_1405.html">20250928_1405</a>
<a href="archive/20250928_0338.html">20250928_0338</a>
<a href="archive/20250927_2233.html">20250927_2233</a>
<a href="archive/20250925_0328.html">20250925_0328</a>
<a href="archive/20250924_0337.html">20250924_0337</a>
<a href="archive/20250923_0336.html">20250923_0336</a>
<a href="archive/20250922_0334.html">20250922_0334</a>
<a href="archive/20250921_0333.html">20250921_0333</a>
<a href="archive/20250920_0334.html">20250920_0334</a>
<a href="archive/20250919_1904.html">20250919_1904</a>
<a href="archive/20250919_1023.html">20250919_1023</a>
<a href="archive/20250919_0332.html">20250919_0332</a>
<a href="archive/20250918_2047.html">20250918_2047</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250918_0326.html">20250918_0326</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250917_0326.html">20250917_0326</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250916_0328.html">20250916_0328</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
