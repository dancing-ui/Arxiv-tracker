<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-11 03:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251011_0327</div>
    <div class="row"><div class="card">
<div class="title">Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical   Mitosis Classification</div>
<div class="meta-line">Authors: Mieko Ochi, Bae Yuan</div>
<div class="meta-line">First: 2025-08-29T03:24:57+00:00 · Latest: 2025-09-18T10:00:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.02591v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.02591v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mitotic figures are classified into typical and atypical variants, with
atypical counts correlating strongly with tumor aggressiveness. Accurate
differentiation is therefore essential for patient prognostication and resource
allocation, yet remains challenging even for expert pathologists. Here, we
leveraged Pathology Foundation Models (PFMs) pre-trained on large
histopathology datasets and applied parameter-efficient fine-tuning via
low-rank adaptation. In addition, we incorporated ConvNeXt V2, a
state-of-the-art convolutional neural network architecture, to complement PFMs.
During training, we employed a fisheye transform to emphasize mitoses and
Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled
multiple PFMs to integrate complementary morphological insights, achieving
competitive balanced accuracy on the Preliminary Evaluation Phase dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIDOG 2025 轨道2：非典型分裂分类的病理基础模型集成</div>
<div class="mono" style="margin-top:8px">分裂像被分类为典型和非典型变体，非典型计数与肿瘤侵袭性密切相关。因此，准确区分对于患者的预后评估和资源分配至关重要，即使是专家病理学家也面临挑战。我们利用在大规模组织病理学数据集上预训练的病理基础模型（PFMs），并通过低秩适应进行参数高效的微调。此外，我们引入了最先进的卷积神经网络架构ConvNeXt V2来补充PFMs。在训练过程中，我们使用鱼眼变换强调分裂像，并使用ImageNet目标图像进行频域适应。最后，我们集成多个PFMs以整合互补的形态学见解，在初步评估阶段数据集上实现了竞争力的平衡准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aimed to improve the accurate classification of atypical mitotic figures, which are crucial for tumor aggressiveness assessment. The authors used Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and fine-tuned them with low-rank adaptation. They also incorporated ConvNeXt V2 and used a fisheye transform and Fourier Domain Adaptation for training. The ensemble of PFMs achieved competitive balanced accuracy in the Preliminary Evaluation Phase dataset.</div>
<div class="mono" style="margin-top:8px">研究旨在提高对非典型分裂象的准确区分，这对患者的预后至关重要。研究人员使用了在大规模组织病理学数据集上预训练的Pathology Foundation Models (PFMs)，并通过低秩适应进行了微调。他们还引入了ConvNeXt V2，并使用了鱼眼变换和图像域适应进行训练。PFMs的集成在初步评估阶段的数据集上达到了竞争性的平衡准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2025-09-15T05:10:43+00:00 · Latest: 2025-09-15T05:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.11587v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.11587v1">PDF</a> · <a href="https://github.com/haonanshi0125/HIL">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督可见-红外行人重识别的分层身份学习</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）旨在通过减少模态差异并最小化昂贵的手动注释依赖性，从跨模态的无标签行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来处理USVI-ReID，通过单个聚类中心表示一个人。然而，它们主要关注每个聚类内的图像共性，而忽视了它们之间的细微差异。为了解决这一局限性，我们提出了一种分层身份学习（HIL）框架。由于每个聚类可能包含多个反映图像间细微差异的较小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习（MCCL）以细化表示，增强同模态聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输（BRST）机制，通过双向匹配伪标签建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法优于现有方法。源代码可在：https://github.com/haonanshi0125/HIL 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification by addressing the limitations of existing methods that focus on commonalities within clusters. The Hierarchical Identity Learning (HIL) framework is proposed, which generates multiple memories for each cluster through secondary clustering and uses Multi-Center Contrastive Learning (MCCL) to refine representations. Additionally, a Bidirectional Reverse Selection Transmission (BRST) mechanism is designed to enhance cross-modal matching. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing approaches.</div>
<div class="mono" style="margin-top:8px">研究旨在通过减少模态差异和减少对人工标注的依赖来提升无监督可见光-红外行人再识别。提出的层次身份学习（HIL）框架通过二级聚类为每个粗粒度聚类生成多个记忆，并引入多中心对比学习（MCCL）和双向反向选择传输（BRST）机制以增强模内聚类和跨模态匹配。在SYSU-MM01和RegDB数据集上的实验表明，该方法优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for   Gastric Tissue Classification</div>
<div class="meta-line">Authors: Mustafa Yurdakul, Sakir Tasdemir</div>
<div class="meta-line">First: 2025-09-11T08:24:50+00:00 · Latest: 2025-09-11T08:24:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.09242v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.09242v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoAtNeX:一种基于注意力增强的ConvNeXtV2-Transformer混合模型用于胃组织分类</div>
<div class="mono" style="margin-top:8px">背景与目的早期诊断胃病对于预防致命结果至关重要。尽管组织病理学检查仍然是诊断的金标准，但其完全由手工操作，使得评估劳动密集且容易受到病理学家之间变异的影响。关键发现可能被遗漏，缺乏标准化程序降低了一致性。这些限制突显了需要自动化、可靠且高效的胃组织分析方法。方法在本研究中，提出了一种名为CoAtNeXt的新型混合模型，用于胃组织图像分类。该模型基于CoAtNet架构，通过用增强的ConvNeXtV2块替换其MBConv层来构建。此外，还集成了卷积块注意力模块（CBAM），以通过通道和空间注意力机制提高局部特征提取。该架构被扩展以在计算效率和分类性能之间取得平衡。CoAtNeXt在两个公开可用的数据集HMU-GC-HE-30K（用于八类分类）和GasHisSDB（用于二类分类）上进行了评估，并与10种卷积神经网络（CNN）和10种视觉变换器（ViT）模型进行了比较。结果CoAtNeXt在HMU-GC-HE-30K上达到了96.47%的准确率、96.60%的精确率、96.47%的召回率、96.45%的F1分数和99.89%的AUC。在GasHisSDB上，其准确率为98.29%、精确率为98.07%、召回率为98.41%、F1分数为98.23%和AUC为99.90%。它在所有测试的CNN和ViT模型中表现最佳，并超越了文献中的先前研究。结论实验结果表明，CoAtNeXt是一种稳健的架构，适用于胃组织图像的组织病理学分类，提供二类和多类的性能。其潜在地有助于通过提高诊断准确性和减少工作量来辅助病理学家。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to improve the accuracy and efficiency of gastric tissue classification through an automated method. CoAtNeXt, a hybrid model combining CoAtNet and ConvNeXtV2 with CBAM, was developed. It achieved high accuracy, precision, recall, F1 score, and AUC on two datasets, surpassing existing CNN and ViT models.</div>
<div class="mono" style="margin-top:8px">研究旨在通过自动化方法提高胃组织分类的准确性和效率。提出了结合CoAtNet和增强的ConvNeXtV2块及CBAM的CoAtNeXt模型。在HMU-GC-HE-30K和GasHisSDB数据集上，CoAtNeXt实现了高准确率、精确率、召回率、F1分数和AUC，超越了现有CNN和ViT模型。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Classification of Normal and Atypical Mitotic Figures Using   ConvNeXt V2: MIDOG 2025 Track 2</div>
<div class="meta-line">Authors: Yosuke Yamagishi, Shouhei Hanaoka</div>
<div class="meta-line">First: 2025-08-26T09:11:12+00:00 · Latest: 2025-08-26T09:11:12+00:00</div>
<div class="meta-line">Comments: MIDOG 2025 solution</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.18831v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.18831v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents our solution for the MIDOG 2025 Challenge Track 2, which
focuses on binary classification of normal mitotic figures (NMFs) versus
atypical mitotic figures (AMFs) in histopathological images. Our approach
leverages a ConvNeXt V2 base model with center cropping preprocessing and
5-fold cross-validation ensemble strategy. The method addresses key challenges
including severe class imbalance, high morphological variability, and domain
heterogeneity across different tumor types, species, and scanners. Through
strategic preprocessing with 60% center cropping and mixed precision training,
our model achieved robust performance on the diverse MIDOG 2025 dataset. The
solution demonstrates the effectiveness of modern convolutional architectures
for mitotic figure subtyping while maintaining computational efficiency through
careful architectural choices and training optimizations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用ConvNeXt V2自动分类正常和异常分裂象：MIDOG 2025赛道2</div>
<div class="mono" style="margin-top:8px">本文介绍了我们对MIDOG 2025挑战赛道2的解决方案，该挑战专注于在组织病理学图像中对正常分裂象（NMFs）与异常分裂象（AMFs）进行二分类。我们的方法利用了ConvNeXt V2基础模型，采用中心裁剪预处理和5折交叉验证集成策略。该方法解决了包括严重类别不平衡、高形态学变异性以及不同肿瘤类型、物种和扫描器之间的领域异质性在内的关键挑战。通过60%中心裁剪和混合精度训练的战略性预处理，我们的模型在多样的MIDOG 2025数据集上实现了稳健的性能。该解决方案展示了现代卷积架构在分裂象亚型分类中的有效性，同时通过精心选择的架构和训练优化保持了计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a solution for the MIDOG 2025 Challenge Track 2, focusing on classifying normal mitotic figures versus atypical mitotic figures using a ConvNeXt V2 model with center cropping preprocessing and 5-fold cross-validation. The method tackles challenges such as class imbalance and morphological variability by employing strategic preprocessing and mixed precision training, achieving robust performance on the dataset. The solution highlights the effectiveness of modern convolutional architectures for mitotic figure subtyping while maintaining computational efficiency.</div>
<div class="mono" style="margin-top:8px">本文提出了一种针对MIDOG 2025挑战赛第二赛道的解决方案，使用ConvNeXt V2模型结合中心裁剪预处理和5折交叉验证方法来区分正常分裂图和异常分裂图。该方法解决了类别不平衡和形态学变异性等挑战，实现了在数据集上的稳健性能。该解决方案突出了现代卷积架构的有效性，同时通过策略性预处理和训练优化保持了计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Multi-label Classification of Eleven Retinal Diseases: A   Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic   Dataset</div>
<div class="meta-line">Authors: Jerry Cao-Xue, Tien Comlekoglu, Keyi Xue, Guanliang Wang, Jiang Li, Gordon Laurie</div>
<div class="meta-line">First: 2025-08-21T22:09:53+00:00 · Latest: 2025-08-21T22:09:53+00:00</div>
<div class="meta-line">Comments: 25 pages, 6 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15986v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15986v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于现代架构的大规模合成数据集上十一种视网膜疾病自动多标签分类：基准测试与元集成</div>
<div class="mono" style="margin-top:8px">由于患者隐私问题和高昂的成本，开发用于视网膜疾病分类的多标签深度学习模型往往受到大型专家注释临床数据集稀缺的阻碍。最近发布的SynFundus-1M，一个高保真度的合成数据集，包含超过一百万张眼底图像，为克服这些障碍提供了新的机会。为了为这一新资源建立基础性能基准，我们开发了一个端到端的深度学习管道，使用五折多标签分层交叉验证策略，训练了六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型）来分类十一种视网膜疾病。我们进一步开发了一个元集成模型，通过堆叠五折外预测和XGBoost分类器。最终集成模型在内部验证集上表现最佳，宏平均受试者操作特征曲线下面积（AUC）为0.9973。关键的是，模型在三个多样化的实际临床数据集上表现出强大的泛化能力，DR数据集上的AUC为0.7972，AIROGS青光眼数据集上的AUC为0.9126，多标签RFMiD数据集上的宏AUC为0.8800。这项工作为未来大规模合成数据集的研究提供了稳健的基准，并证明了仅在合成数据上训练的模型能够准确分类多种病理并有效泛化到实际临床图像，为加速眼科全面AI系统的开发提供了可行途径。</div>
</details>
</div>
<div class="card">
<div class="title">RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in   Streetscape Images from Open Government Metadata</div>
<div class="meta-line">Authors: John S. O&#x27;Meara, Jared Hwang, Zeyu Wang, Michael Saugstad, Jon E. Froehlich</div>
<div class="meta-line">Venue: ICCV</div>
<div class="meta-line">First: 2025-08-13T01:22:48+00:00 · Latest: 2025-08-13T01:22:48+00:00</div>
<div class="meta-line">Comments: Accepted to the ICCV&#x27;25 Workshop on Vision Foundation Models and
  Generative AI for Accessibility: Challenges and Opportunities</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.09415v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.09415v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RampNet：街道景观图像中基于开放政府元数据自举坡道检测的两阶段管道</div>
<div class="mono" style="margin-top:8px">坡道对于城市无障碍至关重要，但在图像中稳健地检测它们仍然是一个开放问题，因为缺乏大规模、高质量的数据集。尽管先前的工作试图通过众包或手动标注数据来提高数据可用性，但这些努力往往在质量和规模上都存在不足。在本文中，我们介绍并评估了一个名为RampNet的两阶段管道，以扩大坡道检测数据集并提高模型性能。在第一阶段，我们通过自动将政府提供的坡道位置数据翻译为全景图像的像素坐标，生成了超过210,000个标注的Google街景（GSV）全景图像数据集。在第二阶段，我们从生成的数据集训练了一个坡道检测模型（修改后的ConvNeXt V2），实现了最先进的性能。为了评估我们管道的两个阶段，我们将其与手动标注的全景图像进行了比较。我们生成的数据集达到了94.0%的精确率和92.5%的召回率，我们的检测模型达到了0.9236的AP，远超先前的工作。我们的工作贡献了第一个大规模、高质量的坡道检测数据集、基准和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RampNet is a two-stage pipeline designed to enhance curb ramp detection in street images. In Stage 1, the authors auto-translate government-provided curb ramp location data into pixel coordinates in panoramic images, generating a dataset of over 210,000 annotated panoramas. In Stage 2, they train a modified ConvNeXt V2 model on this dataset, achieving state-of-the-art performance with 0.9236 AP. The generated dataset and model significantly outperform previous work, with 94.0% precision and 92.5% recall, and contribute the first large-scale, high-quality curb ramp detection dataset and benchmark.</div>
<div class="mono" style="margin-top:8px">研究旨在提高街道图像中坡道的稳健检测，这对于城市无障碍至关重要。RampNet双阶段管道首先通过将政府提供的数据翻译成像素坐标生成超过210,000个标注的Google街景全景图。第二阶段使用修改后的ConvNeXt V2模型进行训练，实现了94.0%的精确度、92.5%的召回率和0.9236的AP，超过了先前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop   Prediction in Integrated Circuit Design</div>
<div class="meta-line">Authors: Youngmin Seo, Yunhyeong Kwon, Younghun Park, HwiRyong Kim, Seungho Eum, Jinha Kim, Taigon Song, Juho Kim, Unsang Park</div>
<div class="meta-line">First: 2025-07-25T12:07:16+00:00 · Latest: 2025-07-25T12:07:16+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.19197v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.19197v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate spatial prediction of power integrity issues, such as IR drop, is
critical for reliable VLSI design. However, traditional simulation-based
solvers are computationally expensive and difficult to scale. We address this
challenge by reformulating IR drop estimation as a pixel-wise regression task
on heterogeneous multi-channel physical maps derived from circuit layouts.
Prior learning-based methods treat all input layers (e.g., metal, via, and
current maps) equally, ignoring their varying importance to prediction
accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention
(WACA) mechanism, which recursively enhances weak feature channels while
suppressing over-dominant ones through a two-stage gating strategy. Integrated
into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and
balanced feature representation. On the public ICCAD-2023 benchmark, our method
outperforms the ICCAD-2023 contest winner by reducing mean absolute error by
61.1% and improving F1-score by 71.0%. These results demonstrate that
channel-wise heterogeneity is a key inductive bias in physical layout analysis
for VLSI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WACA-UNet: 弱性意识通道注意机制在集成电路设计中静态IR降预测</div>
<div class="mono" style="margin-top:8px">准确的空间预测电力完整性问题，如IR降，对于可靠的VLSI设计至关重要。然而，传统的基于仿真的求解器计算成本高昂且难以扩展。我们通过将IR降估计重新表述为基于异构多通道物理图的像素级回归任务来应对这一挑战。先前的学习方法将所有输入层（例如，金属、通孔和电流图）视为同等重要，忽略了它们对预测准确性的不同重要性。为了解决这一问题，我们提出了一种新颖的弱性意识通道注意（WACA）机制，该机制通过两阶段门控策略递归增强弱特征通道并抑制主导通道。将该机制集成到基于ConvNeXtV2的注意力U-Net中，我们的方法能够实现自适应和平衡的特征表示。在公开的ICCAD-2023基准测试中，我们的方法通过降低平均绝对误差61.1%和提高F1分数71.0%击败了ICCAD-2023竞赛的获胜者。这些结果表明，通道级异质性是VLSI物理布局分析中的关键归纳偏置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the accuracy and efficiency of predicting IR drop in VLSI design by reformulating the problem as a pixel-wise regression task. The proposed WACA-UNet integrates a Weakness-Aware Channel Attention mechanism into a ConvNeXtV2-based attention U-Net to address the varying importance of different input layers. The method significantly outperforms the previous winner of the ICCAD-2023 contest, reducing mean absolute error by 61.1% and improving F1-score by 71.0% on the ICCAD-2023 benchmark.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决传统仿真方法的计算效率问题，提高集成电路设计中IR电压降预测的准确性。提出了一种弱性感知通道注意（WACA）机制，将其集成到基于ConvNeXtV2的注意力U-Net中，以增强弱特征通道并抑制主导通道。该方法在ICCAD-2023基准测试中显著优于竞赛获胜者，将均方绝对误差降低了61.1%，F1分数提高了71.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal   Imaging Exams</div>
<div class="meta-line">Authors: Leonor Fernandes, Tiago Gonçalves, João Matos, Luis Filipe Nakayama, Jaime S. Cardoso</div>
<div class="meta-line">First: 2025-07-13T14:11:41+00:00 · Latest: 2025-07-13T14:11:41+00:00</div>
<div class="meta-line">Comments: 10 pages. Under review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.09640v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.09640v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>眼科视网膜成像检查中短路和解缠的分离与评估</div>
<div class="mono" style="margin-top:8px">糖尿病视网膜病变（DR）是工作年龄成人视力丧失的主要原因。虽然筛查可以降低失明的风险，但传统成像往往成本高昂且难以获取。人工智能（AI）算法提供了一种可扩展的诊断解决方案，但公平性和泛化性的问题仍然存在。本研究评估了图像训练模型在DR预测中的公平性和性能，以及解缠作为偏见缓解技术的影响，使用了多BRSET眼底数据集。三种模型，ConvNeXt V2、DINOv2和Swin V2，均在黄斑图像上训练以预测DR和敏感属性（如年龄和性别/性别），并评估了公平性，应用了解缠以减少偏见。所有模型在诊断DR方面均表现出高预测性能（最高达94%的AUROC），并且能够合理预测年龄和性别/性别（分别为91%和77%的AUROC）。公平性评估表明存在差异，例如DINOv2中年龄组之间10%的AUROC差距。从DR预测中分离敏感属性的效果因所选模型而异。解缠提高了DINOv2的性能（2%的AUROC提升），但在ConvNeXt V2和Swin V2中导致性能下降（分别为7%和3%）。这些发现突显了在眼底成像中分离细粒度特征的复杂性，并强调了医疗成像AI中的公平性的重要性，以确保公平可靠的医疗保健解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Event Detection: Current Approaches and Defining the New   Playground through LLMs and VLMs</div>
<div class="meta-line">Authors: Abhishek Dey, Aabha Bothera, Samhita Sarikonda, Rishav Aryan, Sanjay Kumar Podishetty, Akshay Havalgi, Gaurav Singh, Saurabh Srivastava</div>
<div class="meta-line">First: 2025-05-16T04:07:21+00:00 · Latest: 2025-05-16T04:07:21+00:00</div>
<div class="meta-line">Comments: Accepted at NLDB 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.10836v1">Abs</a> · <a href="http://arxiv.org/pdf/2505.10836v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we study the challenges of detecting events on social media,
where traditional unimodal systems struggle due to the rapid and multimodal
nature of data dissemination. We employ a range of models, including unimodal
ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced
generative models like GPT-4o, and LLaVA. Additionally, we also study the
effect of providing multimodal generative models (such as GPT-4o) with a single
modality to assess their efficacy. Our results indicate that while multimodal
approaches notably outperform unimodal counterparts, generative approaches
despite having a large number of parameters, lag behind supervised methods in
precision. Furthermore, we also found that they lag behind instruction-tuned
models because of their inability to generate event classes correctly. During
our error analysis, we discovered that common social media issues such as leet
speak, text elongation, etc. are effectively handled by generative approaches
but are hard to tackle using supervised approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态事件检测：当前方法与通过LLMs和VLMs定义的新领域</div>
<div class="mono" style="margin-top:8px">在本文中，我们研究了在社交媒体上检测事件的挑战，由于数据传播的快速和多模态性，传统的单模态系统难以应对。我们使用了包括单模态ModernBERT和ConvNeXt-V2、多模态融合技术以及先进的生成模型如GPT-4o和LLaVA等一系列模型。此外，我们还研究了向单模态生成模型（如GPT-4o）提供单模态数据对其效果的影响。我们的结果表明，尽管多模态方法显著优于单模态方法，但生成方法尽管参数量大，但在精确度上仍落后于监督方法。此外，我们还发现，由于无法正确生成事件类别，它们在指令调优模型面前也落后。在我们的错误分析中，我们发现常见的社交媒体问题如leetspeak、文本拉长等，生成方法能够有效处理，但监督方法难以解决。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of detecting events on social media using traditional unimodal systems, which struggle with the rapid and multimodal nature of data. The authors use a variety of models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and generative models like GPT-4o and LLaVA. They find that multimodal approaches outperform unimodal ones, but generative models, despite their large parameter count, fall behind supervised methods in precision. The study also reveals that generative models are better at handling common social media issues like leet speak and text elongation compared to supervised models.</div>
<div class="mono" style="margin-top:8px">本文研究了在社交媒体上检测事件的挑战，传统单模态系统难以应对数据的快速和多模态特性。研究使用了包括ModernBERT、ConvNeXt-V2、多模态融合技术和生成模型如GPT-4o和LLaVA等多种模型。研究发现，多模态方法在性能上优于单模态方法，但生成模型尽管参数量大，但在精度上仍落后于监督方法。此外，生成模型在生成正确的事件类别方面不如指令调优模型有效，而它们在处理常见的社交媒体问题如leet speak和文本拉长方面表现更好。</div>
</details>
</div>
<div class="card">
<div class="title">DyCE: Dynamically Configurable Exiting for Deep Learning Compression and   Real-time Scaling</div>
<div class="meta-line">Authors: Qingyuan Wang, Barry Cardiff, Antoine Frappé, Benoit Larras, Deepu John</div>
<div class="meta-line">Venue: Future Generation Computer Systems 171 (2025) 107837</div>
<div class="meta-line">First: 2024-03-04T03:09:28+00:00 · Latest: 2025-05-07T23:56:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2403.01695v3">Abs</a> · <a href="http://arxiv.org/pdf/2403.01695v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional deep learning (DL) model compression and scaling methods focus
on altering the model&#x27;s components, impacting the results across all samples
uniformly. However, since samples vary in difficulty, a dynamic model that
adapts computation based on sample complexity offers a novel perspective for
compression and scaling. Despite this potential, existing dynamic models are
typically monolithic and model-specific, limiting their generalizability as
broad compression and scaling methods. Additionally, most deployed DL systems
are fixed, unable to adjust their scale once deployed and, therefore, cannot
adapt to the varying real-time demands. This paper introduces DyCE, a
dynamically configurable system that can adjust the performance-complexity
trade-off of a DL model at runtime without requiring re-initialization or
redeployment on inference hardware. DyCE achieves this by adding small exit
networks to intermediate layers of the original model, allowing computation to
terminate early if acceptable results are obtained. DyCE also decouples the
design of an efficient dynamic model, facilitating easy adaptation to new base
models and potential general use in compression and scaling. We also propose
methods for generating optimized configurations and determining the types and
positions of exit networks to achieve desired performance and complexity
trade-offs. By enabling simple configuration switching, DyCE provides
fine-grained performance tuning in real-time. We demonstrate the effectiveness
of DyCE through image classification tasks using deep convolutional neural
networks (CNNs). DyCE significantly reduces computational complexity by 23.5%
for ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with accuracy
reductions of less than 0.5%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyCE：深度学习压缩和实时扩展的动态可配置退出</div>
<div class="mono" style="margin-top:8px">传统的深度学习（DL）模型压缩和扩展方法侧重于改变模型组件，对所有样本的影响是统一的。然而，由于样本的难度不同，一个根据样本复杂性动态调整计算的模型为压缩和扩展提供了新的视角。尽管如此，现有的动态模型通常是单一的且模型特定的，限制了它们作为广泛适用的压缩和扩展方法的通用性。此外，大多数部署的DL系统是固定的，一旦部署后无法调整其规模，因此无法适应不断变化的实时需求。本文介绍了DyCE，这是一种可以在运行时调整DL模型性能-复杂性权衡的动态可配置系统，无需重新初始化或重新部署到推理硬件上。DyCE通过在原始模型的中间层添加小型退出网络来实现这一点，允许在获得可接受结果时提前终止计算。DyCE还解耦了高效动态模型的设计，便于新基础模型的快速适应，并可能在压缩和扩展中通用。我们还提出了生成优化配置和确定退出网络类型及位置的方法，以实现所需的性能和复杂性权衡。通过简单的配置切换，DyCE提供了实时的细粒度性能调整。我们通过使用深度卷积神经网络（CNNs）进行图像分类任务来证明DyCE的有效性。DyCE在ImageNet上将ResNet152和ConvNextv2-tiny的计算复杂性分别减少了23.5%和25.9%，准确率降低不到0.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DyCE is a dynamically configurable system that adjusts the performance-complexity trade-off of deep learning models at runtime without re-initialization or redeployment. It introduces small exit networks to intermediate layers of the original model, allowing early termination of computation if acceptable results are achieved. DyCE demonstrates a significant reduction in computational complexity by 23.5% for ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with minimal accuracy loss. This approach offers fine-grained performance tuning and broad applicability in compression and scaling.</div>
<div class="mono" style="margin-top:8px">DyCE 是一种动态可配置系统，可以在运行时调整深度学习模型的性能-复杂度权衡。它在原始模型的中间层引入了小型退出网络，如果达到了可接受的结果则可以提前终止计算。DyCE 在 ImageNet 上将 ResNet152 的计算复杂度降低了 23.5%，ConvNextv2-tiny 降低了 25.9%，同时准确率损失小于 0.5%。该系统允许在无需重新初始化或重新部署的情况下进行细粒度的实时性能调整。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Duality Learning for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu</div>
<div class="meta-line">First: 2025-05-05T10:36:52+00:00 · Latest: 2025-05-06T07:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.02549v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.02549v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (UVI-ReID) aims to
retrieve pedestrian images across different modalities without costly
annotations, but faces challenges due to the modality gap and lack of
supervision. Existing methods often adopt self-training with
clustering-generated pseudo-labels but implicitly assume these labels are
always correct. In practice, however, this assumption fails due to inevitable
pseudo-label noise, which hinders model learning. To address this, we introduce
a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),
characterized by three key challenges: noise overfitting, error accumulation,
and noisy cluster correspondence. To this end, we propose a novel Robust
Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy
pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning
mechanism (RAL) is proposed to dynamically emphasize clean samples while
down-weighting noisy ones. Second, to alleviate error accumulation-where the
model reinforces its own mistakes-RoDE employs dual distinct models that are
alternately trained using pseudo-labels from each other, encouraging diversity
and preventing collapse. However, this dual-model strategy introduces
misalignment between clusters across models and modalities, creating noisy
cluster correspondence. To resolve this, we introduce Cluster Consistency
Matching (CCM), which aligns clusters across models and modalities by measuring
cross-cluster similarity. Extensive experiments on three benchmarks demonstrate
the effectiveness of RoDE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒对偶学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（UVI-ReID）旨在无需昂贵标注的情况下，在不同模态中检索行人图像，但面临着模态差距和缺乏监督的挑战。现有方法通常采用自训练结合聚类生成的伪标签，但隐含地假设这些标签总是正确的。然而，在实践中，由于不可避免的伪标签噪声，这一假设会失败，阻碍模型学习。为解决这一问题，我们提出了一种新的学习范式，明确考虑伪标签噪声（PLN），并定义了三个关键挑战：噪声过拟合、错误累积和嘈杂的簇对应。为此，我们提出了一种新颖的鲁棒对偶学习框架（RoDE）以减轻噪声伪标签的影响。首先，为了对抗噪声过拟合，我们提出了一种鲁棒自适应学习机制（RAL），动态强调干净样本并降低噪声样本的权重。其次，为了缓解错误累积，RoDE 使用相互的伪标签交替训练两个不同的模型，鼓励多样性并防止模型崩溃。然而，这种双模型策略引入了模型和模态之间的簇对齐问题，导致嘈杂的簇对应。为解决这一问题，我们引入了簇一致性匹配（CCM），通过测量跨簇相似性来对齐模型和模态之间的簇。在三个基准上的广泛实验表明了RoDE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (UVI-ReID) by proposing a Robust Duality Learning framework (RoDE) to handle pseudo-label noise. RoDE includes a Robust Adaptive Learning mechanism to emphasize clean samples and down-weight noisy ones, dual models that train each other to prevent error accumulation, and Cluster Consistency Matching to align clusters across models and modalities. Experiments on three benchmarks show the effectiveness of RoDE in mitigating the effects of noisy pseudo-labels.</div>
<div class="mono" style="margin-top:8px">论文通过引入一个明确考虑伪标签噪声的新学习范式，解决了无监督可见红外行人重识别（UVI-ReID）的挑战。提出了一个稳健的二元学习框架（RoDE），包括一个稳健自适应学习机制来处理噪声过拟合，以及双独立模型来防止错误累积。为了在模型和模态之间对齐集群，引入了集群一致性匹配（CCM）。在三个基准上的实验表明，RoDE在缓解伪标签噪声的影响方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-Aligned Learning with Collaborative Refinement for Unsupervised   VI-ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao</div>
<div class="meta-line">First: 2025-04-27T13:58:12+00:00 · Latest: 2025-05-06T03:12:50+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19244v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19244v2">PDF</a> · <a href="https://github.com/FranklinLingfeng/code-for-SALCR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于协作精炼的语义对齐学习在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需人类注释的情况下，匹配不同模态下同一个体的行人图像。先前的方法通过标签关联算法统一跨模态图像的伪标签，然后设计对比学习框架进行全局特征学习。然而，这些方法忽略了由细粒度模式带来的特征表示和伪标签分布的跨模态变化。这种洞察导致仅优化全局特征时，模态共享学习不足。为解决这一问题，我们提出了一种基于协作精炼的语义对齐学习（SALCR）框架，该框架为每个模态强调的特定细粒度模式建立优化目标，从而实现不同模态标签分布的互补对齐。具体而言，我们首先引入了一个双向全局学习关联模块（DAGI），以双向方式统一跨模态实例的伪标签。随后，执行细粒度语义对齐学习（FGSAL）模块，从跨模态实例中探索每个模态强调的部分级语义对齐模式。基于语义对齐特征及其相应的标签空间，构建优化目标。为缓解来自噪声伪标签的副作用，我们提出了一种全局-部分协作精炼（GPCR）模块，动态挖掘全局和部分特征的可靠正样本集，并优化实例间关系。大量实验表明，所提出的方法具有优越的性能，优于现有方法。我们的代码可在https://github.com/FranklinLingfeng/code-for-SALCR 获取。</div>
</details>
</div>
<div class="card">
<div class="title">BARIS: Boundary-Aware Refinement with Environmental Degradation Priors   for Robust Underwater Instance Segmentation</div>
<div class="meta-line">Authors: Pin-Chi Pan, Soo-Chang Pei</div>
<div class="meta-line">First: 2025-04-28T10:00:22+00:00 · Latest: 2025-04-28T10:00:22+00:00</div>
<div class="meta-line">Comments: 15 pages, 9 figures, and 11 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19643v1">Abs</a> · <a href="http://arxiv.org/pdf/2504.19643v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Underwater instance segmentation is challenging due to adverse visual
conditions such as light attenuation, scattering, and color distortion, which
degrade model performance. In this work, we propose BARIS-Decoder
(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that
enhances segmentation accuracy through feature refinement. To address
underwater degradations, we introduce the Environmental Robust Adapter (ERA),
which efficiently models underwater degradation patterns while reducing
trainable parameters by over 90\% compared to full fine-tuning. The integration
of BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves
state-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B
backbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the
effectiveness of BARIS-ERA in advancing underwater instance segmentation,
providing a robust and efficient solution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BARIS：边界感知细化与环境退化先验的水下实例分割</div>
<div class="mono" style="margin-top:8px">水下实例分割由于不良的视觉条件（如光衰减、散射和颜色失真）导致模型性能下降而具有挑战性。本文提出了一种名为BARIS-Decoder（边界感知细化解码器）的框架，通过特征细化来提高分割准确性。为应对水下退化，我们引入了环境鲁棒适配器（ERA），它能够高效地建模水下退化模式，相比全量微调减少超过90%的可训练参数。将BARIS-Decoder与ERA调优结合，称为BARIS-ERA，实现了最先进的性能，分别在Swin-B和ConvNeXt V2骨干网络上超越Mask R-CNN 3.4 mAP和3.8 mAP。我们的研究结果表明，BARIS-ERA在推进水下实例分割方面具有有效性，提供了一种稳健且高效的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of underwater instance segmentation due to visual degradations like light attenuation and scattering. It proposes BARIS-Decoder, which refines segmentation features, and introduces ERA to model underwater degradation patterns efficiently. BARIS-ERA, combining these, outperforms Mask R-CNN by 3.4 mAP with Swin-B and 3.8 mAP with ConvNeXt V2, demonstrating its effectiveness in robust underwater instance segmentation.</div>
<div class="mono" style="margin-top:8px">研究旨在解决由于光照衰减和散射等视觉退化因素导致的水下实例分割难题。提出了一种BARIS-Decoder，通过特征细化来提高分割准确性。引入了环境鲁棒适配器（ERA），以高效地建模水下退化模式，同时将可训练参数减少了90%以上。BARIS-ERA框架在使用Swin-B骨干网络时比Mask R-CNN高出3.4 mAP，在使用ConvNeXt V2时高出3.8 mAP，展示了其在鲁棒水下实例分割中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">8-Calves Image dataset</div>
<div class="meta-line">Authors: Xuyang Fang, Sion Hannuna, Neill Campbell</div>
<div class="meta-line">First: 2025-03-17T23:47:52+00:00 · Latest: 2025-04-25T22:31:53+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.13777v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.13777v2">PDF</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the 8-Calves dataset, a benchmark for evaluating object
detection and identity preservation in occlusion-rich, temporally consistent
environments. Comprising a 1-hour video (67,760 frames) of eight Holstein
Friesian calves with unique coat patterns and 900 static frames, the dataset
emphasizes real-world challenges like prolonged occlusions, motion blur, and
pose variation. By fine-tuning 28 object detectors (YOLO variants,
transformers) and evaluating 23 pretrained backbones (ResNet, ConvNextV2,
ViTs), we expose critical architectural trade-offs: smaller models (e.g.,
ConvNextV2 Nano, 15.6M parameters) excel in efficiency and retrieval accuracy,
while pure vision transformers lag in occlusion-heavy settings. The dataset&#x27;s
structured design-fixed camera views, natural motion, and verified
identities-provides a reproducible testbed for object detection challenges
(mAP50:95: 56.5-66.4%), bridging synthetic simplicity and domain-specific
complexity. The dataset and benchmark code are all publicly available at
https://huggingface.co/datasets/tonyFang04/8-calves. Limitations include
partial labeling and detector bias, addressed in later sections.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>8头牛图像数据集</div>
<div class="mono" style="margin-top:8px">我们介绍了8-Calves数据集，这是一个用于评估物体检测和遮挡环境中身份保留基准的数据集。该数据集包含一小时视频（67,760帧）中的八头荷斯坦-弗里斯兰牛，每头牛都有独特的毛皮图案，以及900张静态图像，强调了现实世界中的挑战，如长时间遮挡、运动模糊和姿态变化。通过微调28种物体检测器（YOLO变体、变压器）并评估23种预训练骨干网络（ResNet、ConvNextV2、ViTs），我们揭示了关键的架构权衡：较小的模型（例如，ConvNextV2 Nano，参数量15.6M）在效率和检索准确性方面表现出色，而纯视觉变压器在遮挡密集的环境中表现落后。数据集的结构化设计——固定摄像机视角、自然运动和验证身份——提供了一个可重复的测试平台，用于物体检测挑战（mAP50:95：56.5-66.4%），连接了合成的简单性和领域特定的复杂性。数据集和基准代码均在https://huggingface.co/datasets/tonyFang04/8-calves公开。局限性包括部分标注和检测器偏差，这些问题在后续部分中有所讨论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The 8-Calves dataset is created to evaluate object detection and identity preservation in complex, occlusion-rich environments. It includes a 1-hour video and 900 static frames of eight Holstein Friesian calves, highlighting real-world challenges such as prolonged occlusions and pose variations. By fine-tuning 28 object detectors and evaluating 23 pretrained backbones, the study reveals that smaller models like ConvNextV2 Nano perform better in terms of efficiency and retrieval accuracy, while pure vision transformers struggle in occlusion-heavy settings. The dataset provides a structured and reproducible benchmark for object detection challenges, with a mean average precision (mAP50:95) ranging from 56.5% to 66.4%. The dataset and benchmark code are publicly available.</div>
<div class="mono" style="margin-top:8px">8-Calves数据集旨在评估在复杂、遮挡丰富的环境中的人体检测和身份保持能力。该数据集包含一段1小时的视频和900张静态图像，展示了如长时间遮挡和姿态变化等真实世界的挑战。通过微调28种物体检测器和评估23种预训练骨干网络，研究发现较小的模型如ConvNextV2 Nano在效率和检索准确性方面表现更佳，而纯视觉变压器在遮挡密集的环境中表现较差。该数据集提供了一个结构化且可重复的基准测试，用于物体检测挑战，平均精度（mAP50:95）范围在56.5%到66.4%之间。数据集和基准代码已公开可用。</div>
</details>
</div>
<div class="card">
<div class="title">MaxGlaViT: A novel lightweight vision transformer-based approach for   early diagnosis of glaucoma stages from fundus images</div>
<div class="meta-line">Authors: Mustafa Yurdakul, Kubra Uyar, Sakir Tasdemir</div>
<div class="meta-line">First: 2025-02-24T13:48:04+00:00 · Latest: 2025-02-24T13:48:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.17154v1">Abs</a> · <a href="http://arxiv.org/pdf/2502.17154v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Glaucoma is a prevalent eye disease that progresses silently without
symptoms. If not detected and treated early, it can cause permanent vision
loss. Computer-assisted diagnosis systems play a crucial role in timely and
efficient identification. This study introduces MaxGlaViT, a lightweight model
based on the restructured Multi-Axis Vision Transformer (MaxViT) for early
glaucoma detection. First, MaxViT was scaled to optimize block and channel
numbers, resulting in a lighter architecture. Second, the stem was enhanced by
adding attention mechanisms (CBAM, ECA, SE) after convolution layers to improve
feature learning. Third, MBConv structures in MaxViT blocks were replaced by
advanced DL blocks (ConvNeXt, ConvNeXtV2, InceptionNeXt). The model was
evaluated using the HDV1 dataset, containing fundus images of different
glaucoma stages. Additionally, 40 CNN and 40 ViT models were tested on HDV1 to
validate MaxGlaViT&#x27;s efficiency. Among CNN models, EfficientB6 achieved the
highest accuracy (84.91%), while among ViT models, MaxViT-Tiny performed best
(86.42%). The scaled MaxViT reached 87.93% accuracy. Adding ECA to the stem
block increased accuracy to 89.01%. Replacing MBConv with ConvNeXtV2 further
improved it to 89.87%. Finally, integrating ECA in the stem and ConvNeXtV2 in
MaxViT blocks resulted in 92.03% accuracy. Testing 80 DL models for glaucoma
stage classification, this study presents a comprehensive and comparative
analysis. MaxGlaViT outperforms experimental and state-of-the-art models,
achieving 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% f1-score,
and 87.12% Cohen&#x27;s kappa score.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MaxGlaViT：一种基于重构多轴视觉变换器的新型轻量级视网膜图像青光眼早期诊断方法</div>
<div class="mono" style="margin-top:8px">青光眼是一种常见的无声进展的眼病，如果不及时诊断和治疗，会导致永久性视力丧失。计算机辅助诊断系统在及时和高效识别方面发挥着重要作用。本研究介绍了一种基于重构多轴视觉变换器（MaxViT）的轻量级模型MaxGlaViT，用于早期青光眼检测。首先，MaxViT被缩放以优化块和通道数量，从而实现更轻的架构。其次，通过在卷积层之后添加注意力机制（CBAM、ECA、SE）增强茎部，以提高特征学习。第三，MaxViT块中的MBConv结构被先进的DL块（ConvNeXt、ConvNeXtV2、InceptionNeXt）所取代。该模型使用包含不同青光眼阶段视网膜图像的HDV1数据集进行评估。此外，还测试了40个CNN模型和40个ViT模型以验证MaxGlaViT的效率。在CNN模型中，EfficientB6的准确率最高（84.91%），而在ViT模型中，MaxViT-Tiny表现最佳（86.42%）。缩放后的MaxViT达到了87.93%的准确率。在茎块中添加ECA将准确率提高到89.01%。用ConvNeXtV2替换MBConv进一步提高了准确率至89.87%。最后，将ECA集成到茎块中，ConvNeXtV2集成到MaxViT块中，准确率达到了92.03%。在测试80个深度学习模型进行青光眼阶段分类时，本研究提供了全面且具有对比性的分析。MaxGlaViT在准确率、精确率、召回率、F1分数和科恩κ分数方面均优于实验和最先进的模型，分别达到了92.03%、92.33%、92.03%、92.13%和87.12%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces MaxGlaViT, a lightweight model based on the restructured Multi-Axis Vision Transformer (MaxViT), for early glaucoma detection. It optimizes block and channel numbers, enhances the stem with attention mechanisms, and replaces MBConv with advanced DL blocks. Evaluations on the HDV1 dataset showed that MaxGlaViT achieved 92.03% accuracy, outperforming other CNN and ViT models. Key improvements included adding ECA to the stem block and using ConvNeXtV2 in MaxViT blocks.</div>
<div class="mono" style="margin-top:8px">研究旨在利用重构的多轴视觉变换器（MaxViT）开发一种轻量级模型以实现早期青光眼诊断。模型MaxGlaViT通过调整块和通道数量、在茎部增加注意力机制以及用先进的DL块替换MBConv进行了优化。MaxGlaViT在HDV1数据集上的青光眼阶段分类中达到了92.03%的准确率，超过了测试的CNN和ViT模型。</div>
</details>
</div>
<div class="card">
<div class="title">Extended Cross-Modality United Learning for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-26T09:30:26+00:00 · Latest: 2024-12-26T09:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.19134v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.19134v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展跨模态联合学习在无监督可见光-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见光-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征并减少模态间差异。然而，现有方法缺乏跨模态聚类或过度追求聚类级别的关联，这使得可靠地学习模态不变特征变得困难。为解决这一问题，我们提出了一种扩展跨模态联合学习（ECUL）框架，结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem）模块。具体而言，我们设计ECUL以自然地整合模态内聚类、模态间聚类和模态间实例选择，建立紧凑且准确的跨模态关联，同时减少引入噪声标签。此外，EMCC通过扩展编码向量来捕获和过滤邻域关系，进一步促进聚类算法中模态不变和相机不变知识的学习。最后，TSMem通过分阶段更新记忆为对比学习提供准确且通用的代理点。在SYSU-MM01和RegDB数据集上的广泛实验结果表明，提出的ECUL表现出色，并且甚至优于某些监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised learning for visible-infrared person re-identification by proposing an Extended Cross-Modality United Learning (ECUL) framework. This framework includes Extended Modality-Camera Clustering (EMCC) and Two-Step Memory Updating Strategy (TSMem) to integrate intra-modality and inter-modality clustering, and to reduce the inter-modality gap. The experiments on SYSU-MM01 and RegDB datasets show that ECUL outperforms certain supervised methods and demonstrates promising performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种扩展跨模态联合学习（ECUL）框架，以解决可见光-红外人再识别的无监督学习问题。该框架结合了扩展模态-相机聚类（EMCC）和两步记忆更新策略（TSMem），整合了模态内和模态间聚类及实例选择，从而减少模态间差异。实验结果表明，ECUL在SYSU-MM01和RegDB数据集上的表现优于某些监督方法，显示出良好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Relieving Universal Label Noise for Unsupervised Visible-Infrared Person   Re-Identification by Inferring from Neighbors</div>
<div class="meta-line">Authors: Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin</div>
<div class="meta-line">First: 2024-12-16T04:04:41+00:00 · Latest: 2024-12-16T04:04:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.12220v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.12220v1">PDF</a> · <a href="https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of
great research and practical significance yet remains challenging due to the
absence of annotations. Existing approaches aim to learn modality-invariant
representations in an unsupervised setting. However, these methods often
encounter label noise within and across modalities due to suboptimal clustering
results and considerable modality discrepancies, which impedes effective
training. To address these challenges, we propose a straightforward yet
effective solution for USL-VI-ReID by mitigating universal label noise using
neighbor information. Specifically, we introduce the Neighbor-guided Universal
Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in
both homogeneous and heterogeneous spaces with soft labels derived from
neighboring samples to reduce label noise. Additionally, we present the
Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability
by minimizing the influence of unreliable samples. Extensive experiments on the
RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing
USL-VI-ReID approaches, despite its simplicity. The source code is available
at: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过从邻居推断缓解通用标签噪声以实现无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）具有重要的研究和实际意义，但由于缺乏标注而仍然具有挑战性。现有方法旨在在无监督设置中学习跨模态不变的表示。然而，这些方法常常由于聚类结果不佳和模态差异较大而遇到跨模态的标签噪声问题，这阻碍了有效的训练。为了解决这些挑战，我们提出了一种简单而有效的解决方案，通过利用邻居信息缓解通用标签噪声以实现USL-VI-ReID。具体而言，我们引入了邻居引导的通用标签校准（N-ULC）模块，该模块用来自邻居样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们还提出了邻居引导的动态加权（N-DW）模块，通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，但我们的方法仍优于现有USL-VI-ReID方法。源代码可在：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve unsupervised visible-infrared person re-identification (USL-VI-ReID) by addressing label noise issues. The method introduces the Neighbor-guided Universal Label Calibration (N-ULC) module, which uses soft labels from neighboring samples to reduce label noise, and the Neighbor-guided Dynamic Weighting (N-DW) module to minimize the impact of unreliable samples. Experiments on RegDB and SYSU-MM01 datasets show that this approach outperforms existing methods despite its simplicity.</div>
<div class="mono" style="margin-top:8px">研究通过使用邻居信息来缓解无监督可见-红外行人再识别（USL-VI-ReID）中的普遍标签噪声问题，提出了一种方法。该方法引入了邻居引导的普遍标签校准（N-ULC）模块，该模块使用来自邻居样本的软标签而不是硬伪标签，并引入了邻居引导的动态加权（N-DW）模块以减少不可靠样本的影响。实验结果表明，该方法在RegDB和SYSU-MM01数据集上优于现有方法，尽管其结构简单。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modality-Camera Invariant Clustering for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Yiming Yang, Weipeng Hu, Haifeng Hu</div>
<div class="meta-line">First: 2024-12-11T09:31:03+00:00 · Latest: 2024-12-11T09:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.08231v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.08231v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
offers a more flexible and cost-effective alternative compared to supervised
methods. This field has gained increasing attention due to its promising
potential. Existing methods simply cluster modality-specific samples and employ
strong association techniques to achieve instance-to-cluster or
cluster-to-cluster cross-modality associations. However, they ignore
cross-camera differences, leading to noticeable issues with excessive splitting
of identities. Consequently, this undermines the accuracy and reliability of
cross-modal associations. To address these issues, we propose a novel Dynamic
Modality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID.
Specifically, our DMIC naturally integrates Modality-Camera Invariant Expansion
(MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive
Learning (HMCL) into a unified framework, which eliminates both the
cross-modality and cross-camera discrepancies in clustering. MIE fuses
inter-modal and inter-camera distance coding to bridge the gaps between
modalities and cameras at the clustering level. DNC employs two dynamic search
strategies to refine the network&#x27;s optimization objective, transitioning from
improving discriminability to enhancing cross-modal and cross-camera
generalizability. Moreover, HMCL is designed to optimize instance-level and
cluster-level distributions. Memories for intra-modality and inter-modality
training are updated using randomly selected samples, facilitating real-time
exploration of modality-invariant representations. Extensive experiments have
demonstrated that our DMIC addresses the limitations present in current
clustering approaches and achieve competitive performance, which significantly
reduces the performance gap with supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态模态-相机不变聚类在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）相比监督方法提供了更灵活和成本效益更高的替代方案。该领域因其有希望的潜力而越来越受到关注。现有方法简单地聚类模态特定样本，并采用强关联技术实现实例到聚类或聚类到聚类的跨模态关联。然而，它们忽略了跨相机差异，导致身份分割过度，从而削弱了跨模态关联的准确性和可靠性。为解决这些问题，我们提出了一种新颖的动态模态-相机不变聚类（DMIC）框架用于USL-VI-ReID。具体而言，我们的DMIC自然地将模态-相机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）统一到一个框架中，消除了聚类中的跨模态和跨相机差异。MIE融合了跨模态和跨相机的距离编码，在聚类层面弥合了模态和相机之间的差距。DNC采用两种动态搜索策略来细化网络的优化目标，从提高可区分性过渡到增强跨模态和跨相机的一般性。此外，HMCL旨在优化实例级和聚类级的分布。使用随机选择的样本更新模态内和跨模态训练的记忆，促进实时探索模态不变表示。大量实验表明，我们的DMIC解决了当前聚类方法的局限性，实现了竞争力的性能，显著减少了与监督方法的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a Dynamic Modality-Camera Invariant Clustering (DMIC) framework for unsupervised visible-infrared person re-identification (USL-VI-ReID) to address the limitations of existing methods that ignore cross-camera differences, leading to identity splitting. DMIC integrates Modality-Camera Invariant Expansion (MIE), Dynamic Neighborhood Clustering (DNC), and Hybrid Modality Contrastive Learning (HMCL) to eliminate cross-modality and cross-camera discrepancies. Experiments show that DMIC achieves competitive performance, reducing the performance gap with supervised methods.</div>
<div class="mono" style="margin-top:8px">论文提出了一种动态模态-摄像机不变聚类（DMIC）框架，用于无监督的可见-红外行人再识别（USL-VI-ReID）。DMIC 将模态-摄像机不变扩展（MIE）、动态邻域聚类（DNC）和混合模态对比学习（HMCL）集成到一个统一框架中，以解决跨模态和跨摄像机差异的问题。该框架通过融合跨模态和跨摄像机的距离编码、采用动态搜索策略以及优化实例级和聚类级分布来提高跨模态和跨摄像机的关联准确性。实验结果表明，DMIC 能够克服现有聚类方法的局限性，并且在性能上接近监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Homogeneous and Heterogeneous Consistent Label Associations   for Unsupervised Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao</div>
<div class="meta-line">First: 2024-02-01T15:33:17+00:00 · Latest: 2024-12-04T03:55:35+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.00672v4">Abs</a> · <a href="http://arxiv.org/pdf/2402.00672v4">PDF</a> · <a href="https://github.com/FranklinLingfeng/code_for_MULT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID)
endeavors to retrieve pedestrian images of the same identity from different
modalities without annotations. While prior work focuses on establishing
cross-modality pseudo-label associations to bridge the modality-gap, they
ignore maintaining the instance-level homogeneous and heterogeneous consistency
between the feature space and the pseudo-label space, resulting in coarse
associations. In response, we introduce a Modality-Unified Label Transfer
(MULT) module that simultaneously accounts for both homogeneous and
heterogeneous fine-grained instance-level structures, yielding high-quality
cross-modality label associations. It models both homogeneous and heterogeneous
affinities, leveraging them to quantify the inconsistency between the
pseudo-label space and the feature space, subsequently minimizing it. The
proposed MULT ensures that the generated pseudo-labels maintain alignment
across modalities while upholding structural consistency within intra-modality.
Additionally, a straightforward plug-and-play Online Cross-memory Label
Refinement (OCLR) module is proposed to further mitigate the side effects of
noisy pseudo-labels while simultaneously aligning different modalities, coupled
with an Alternative Modality-Invariant Representation Learning (AMIRL)
framework. Experiments demonstrate that our proposed method outperforms
existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of
our MULT in comparison to other cross-modality association methods. Code is
available at https://github.com/FranklinLingfeng/code_for_MULT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索一致标签关联的同质性和异质性在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，从不同模态中检索同一身份的行人图像。先前的工作主要集中在建立跨模态的伪标签关联以弥补模态差异，但忽略了在特征空间和伪标签空间之间保持实例级的同质性和异质一致性，导致关联粗糙。为此，我们引入了一个模态统一标签转移（MULT）模块，该模块同时考虑了同质性和异质性的细粒度实例级结构，从而产生高质量的跨模态标签关联。该模块利用同质性和异质性亲和力来量化伪标签空间与特征空间之间的不一致性，并对其进行最小化。所提出的MULT确保生成的伪标签在不同模态之间保持对齐，同时在同模态内部保持结构一致性。此外，我们还提出了一种简单的即插即用在线跨记忆标签精炼（OCLR）模块，以进一步减轻噪声伪标签的副作用，同时对齐不同模态，结合了一种替代模态不变表示学习（AMIRL）框架。实验表明，我们提出的方法在现有的USL-VI-ReID方法中表现更优，突显了我们MULT在与其他跨模态关联方法相比的优势。代码可在https://github.com/FranklinLingfeng/code_for_MULT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by introducing a Modality-Unified Label Transfer (MULT) module that considers both homogeneous and heterogeneous fine-grained instance-level structures. The MULT module models affinities to quantify inconsistencies between the pseudo-label space and the feature space, thereby improving cross-modality label associations. Additionally, an Online Cross-memory Label Refinement (OCLR) module and an Alternative Modality-Invariant Representation Learning (AMIRL) framework are proposed to refine pseudo-labels and align different modalities. Experimental results show that the proposed method outperforms existing state-of-the-art USL-VI-ReID methods.</div>
<div class="mono" style="margin-top:8px">该论文通过引入同时考虑同质性和异质性细粒度实例结构的Modality-Unified Label Transfer (MULT) 模块，解决了无监督可见-红外行人重识别 (USL-VI-ReID) 的挑战。MULT 模块通过建模亲和力来量化伪标签空间与特征空间之间的不一致性，从而改善跨模态标签关联。此外，还提出了在线跨记忆标签精炼 (OCLR) 模块和替代模态不变表示学习 (AMIRL) 框架，以进一步精炼伪标签并使不同模态对齐。实验结果表明，所提出的方法优于现有最先进的 USL-VI-ReID 方法。</div>
</details>
</div>
<div class="card">
<div class="title">I Spy With My Little Eye: A Minimum Cost Multicut Investigation of   Dataset Frames</div>
<div class="meta-line">Authors: Katharina Prasse, Isaac Bravo, Stefanie Walter, Margret Keuper</div>
<div class="meta-line">First: 2024-12-02T09:09:47+00:00 · Latest: 2024-12-02T09:09:47+00:00</div>
<div class="meta-line">Comments: WACV25 applications track</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.01296v1">Abs</a> · <a href="http://arxiv.org/pdf/2412.01296v1">PDF</a> · <a href="https://github.com/KathPra/MP4VisualFrameDetection">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual framing analysis is a key method in social sciences for determining
common themes and concepts in a given discourse. To reduce manual effort, image
clustering can significantly speed up the annotation process. In this work, we
phrase the clustering task as a Minimum Cost Multicut Problem [MP]. Solutions
to the MP have been shown to provide clusterings that maximize the posterior
probability, solely from provided local, pairwise probabilities of two images
belonging to the same cluster. We discuss the efficacy of numerous embedding
spaces to detect visual frames and show its superiority over other clustering
methods. To this end, we employ the climate change dataset \textit{ClimateTV}
which contains images commonly used for visual frame analysis. For broad visual
frames, DINOv2 is a suitable embedding space, while ConvNeXt V2 returns a
larger number of clusters which contain fine-grain differences, i.e. speech and
protest. Our insights into embedding space differences in combination with the
optimal clustering - by definition - advances automated visual frame detection.
Our code can be found at https://github.com/KathPra/MP4VisualFrameDetection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我用我的小眼睛观察：最小成本多切问题在数据集帧中的研究</div>
<div class="mono" style="margin-top:8px">视觉框架分析是社会科学中的关键方法，用于确定给定话语中的共同主题和概念。为了减少人工努力，图像聚类可以显著加快注释过程。在本文中，我们将聚类任务表述为最小成本多切问题[MP]。MP的解已被证明能够仅从提供的两个图像属于同一聚类的局部、成对概率中最大化后验概率。我们讨论了多种嵌入空间以检测视觉框架，并展示了其在其他聚类方法上的优越性。为此，我们使用了包含常用于视觉框架分析的图像的气候变化数据集ClimateTV。对于广泛的视觉框架，DINOv2是一个合适的嵌入空间，而ConvNeXt V2返回包含更多细粒度差异的更多聚类，例如演讲和抗议。我们对嵌入空间差异的见解与最优聚类定义相结合，促进了自动化视觉框架检测。我们的代码可以在https://github.com/KathPra/MP4VisualFrameDetection找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of visual framing analysis by formulating the clustering task as a Minimum Cost Multicut Problem (MP). The study evaluates various embedding spaces to detect visual frames in the ClimateTV dataset, demonstrating that DINOv2 is effective for broad visual frames, while ConvNeXt V2 provides more detailed clusters with fine-grain differences. The research shows that MP solutions maximize the posterior probability of clusterings based on local, pairwise image similarities, thereby advancing automated visual frame detection.</div>
<div class="mono" style="margin-top:8px">该研究通过将聚类任务表述为最小成本多切分割问题（MP），解决视觉框架分析的挑战。研究在ClimateTV数据集中评估了多种嵌入空间来检测视觉框架，发现DINOv2适用于检测宏观视觉框架，而ConvNeXt V2则提供更详细的簇，包含细粒度差异。研究结果表明，MP解决方案能够最大化基于局部成对图像相似性的聚类后验概率，从而推进了自动视觉框架检测的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with   Neighbor-Guided Label Refinement</div>
<div class="meta-line">Authors: De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T04:40:30+00:00 · Latest: 2024-11-03T13:50:53+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12711v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12711v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作学习与邻域引导标签精炼的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督学习可见-红外行人重识别（USL-VI-ReID）旨在从未标记的跨模态数据集中学习模态不变特征，这对于视频监控系统中的实际应用至关重要。解决USL-VI-ReID任务的关键在于解决跨模态数据关联问题，以便进行进一步的异质联合学习。为了解决这一问题，我们提出了一种双最优传输标签分配（DOTLA）框架，以同时将一种模态生成的标签分配给其对应的模态。提出的DOTLA机制将跨模态数据关联问题转化为一种相互强化和高效的解决方案，从而有效减少了某些不足和噪声标签关联的副作用。此外，我们还提出了一种跨模态邻域一致性引导的标签精炼和正则化模块，在假设每个示例的预测或标签分布应与其最近邻相似的前提下，消除不准确监督信号带来的负面影响。在公开的SYSU-MM01和RegDB数据集上的广泛实验结果表明，所提出的方法具有很高的有效性，平均mAP比现有最先进的方法高出7.76%，甚至超过了某些监督VI-ReID方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the unsupervised visible-infrared person re-identification (USL-VI-ReID) task by proposing a Dual Optimal Transport Label Assignment (DOTLA) framework to solve the cross-modality data association problem. It also introduces a cross-modality neighbor consistency guided label refinement module to improve label accuracy. Experimental results on public datasets SYSU-MM01 and RegDB show that the proposed method outperforms existing state-of-the-art approaches by 7.76% in mean average precision (mAP).</div>
<div class="mono" style="margin-top:8px">论文提出了一个双最优传输标签分配（DOTLA）框架来解决无监督的可见红外行人重识别（USL-VI-ReID）问题。该框架将一种模态生成的标签分配给另一种模态，增强跨模态数据关联。此外，还引入了一个跨模态邻居一致性引导的标签精炼模块，以提高标签准确性。在SYSU-MM01和RegDB数据集上的实验结果表明，所提出的方法在平均精度（mAP）上比现有最先进的方法高出7.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised   Visible-Infrared Person ReID</div>
<div class="meta-line">Authors: De Cheng, Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao</div>
<div class="meta-line">Venue: ACM MM 2023</div>
<div class="meta-line">First: 2023-05-22T03:27:46+00:00 · Latest: 2024-11-03T13:48:21+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2023</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2305.12673v4">Abs</a> · <a href="http://arxiv.org/pdf/2305.12673v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to
match pedestrian images of the same identity from different modalities without
annotations. Existing works mainly focus on alleviating the modality gap by
aligning instance-level features of the unlabeled samples. However, the
relationships between cross-modality clusters are not well explored. To this
end, we propose a novel bilateral cluster matching-based learning framework to
reduce the modality gap by matching cross-modality clusters. Specifically, we
design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)
algorithm through optimizing the maximum matching problem in a bipartite graph.
Then, the matched pairwise clusters utilize shared visible and infrared
pseudo-labels during the model training. Under such a supervisory signal, a
Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework
is proposed to align features jointly at a cluster-level. Meanwhile, the
cross-modality Consistency Constraint (CC) is proposed to explicitly reduce the
large modality discrepancy. Extensive experiments on the public SYSU-MM01 and
RegDB datasets demonstrate the effectiveness of the proposed method, surpassing
state-of-the-art approaches by a large margin of 8.76% mAP on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效双边跨模态簇匹配的无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）旨在无需标注的情况下，匹配不同模态下同一身份的行人图像。现有工作主要集中在通过对齐未标注样本的实例级特征来缓解模态差异。然而，跨模态簇之间的关系尚未得到充分探索。为此，我们提出了一种新颖的双边簇匹配学习框架，通过匹配跨模态簇来减少模态差异。具体地，我们通过在二分图中优化最大匹配问题设计了一种多对多双边跨模态簇匹配（MBCCM）算法。然后，在模型训练过程中，匹配的成对簇利用共享的可见光和红外伪标签。在这样的监督信号下，我们提出了一种模态特定和模态无关的对比学习框架，以在簇级联合对齐特征。同时，我们提出了跨模态一致性约束（CC）以显式地减少模态差异。在公开的SYSU-MM01和RegDB数据集上的广泛实验表明，所提出的方法具有很高的有效性，平均mAP比现有最佳方法高出8.76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses unsupervised visible-infrared person re-identification by proposing a bilateral cluster matching framework. It introduces a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm to match cross-modality clusters and a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework to align features at a cluster-level. The method also includes a cross-modality consistency constraint to reduce modality discrepancy. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms existing approaches by 8.76% in mean average precision.</div>
<div class="mono" style="margin-top:8px">论文提出了一种双边簇匹配框架来解决无监督的可见光-红外行人重识别问题。引入了Many-to-many Bilateral Cross-Modality Cluster Matching算法来跨模态匹配簇，并提出了一种模态特定和模态无关对比学习框架来在簇级别对齐特征。还提出了跨模态一致性约束以减少模态差异。在SYSU-MM01和RegDB数据集上的实验表明，该方法在平均mAP上比现有方法高出8.76%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Commonality, Divergence and Variety for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-02-29T10:37:49+00:00 · Latest: 2024-10-24T09:00:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.19026v3">Abs</a> · <a href="http://arxiv.org/pdf/2402.19026v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
match specified people in infrared images to visible images without
annotations, and vice versa. USVI-ReID is a challenging yet under-explored
task. Most existing methods address the USVI-ReID using cluster-based
contrastive learning, which simply employs the cluster center as a
representation of a person. However, the cluster center primarily focuses on
commonality, overlooking divergence and variety. To address the problem, we
propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes
method for USVI-ReID. In brief, we generate the hard prototype by selecting the
sample with the maximum distance from the cluster center. We theoretically show
that the hard prototype is used in the contrastive loss to emphasize
divergence. Additionally, instead of rigidly aligning query images to a
specific prototype, we generate the dynamic prototype by randomly picking
samples within a cluster. The dynamic prototype is used to encourage the
variety. Finally, we introduce a progressive learning strategy to gradually
shift the model&#x27;s attention towards divergence and variety, avoiding cluster
deterioration. Extensive experiments conducted on the publicly available
SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习共性、差异性和多样性以进行无监督的可见光-红外人体重识别</div>
<div class="mono" style="margin-top:8px">无监督的可见光-红外人体重识别（USVI-ReID）旨在无需标注的情况下，在红外图像中匹配指定的人并在可见图像中进行反向匹配，反之亦然。USVI-ReID 是一个具有挑战性但尚未充分探索的任务。现有大多数方法使用基于聚类的对比学习来解决 USVI-ReID，简单地将聚类中心作为人的表示。然而，聚类中心主要关注共性，忽视了差异性和多样性。为了解决这一问题，我们提出了一种用于 USVI-ReID 的渐进对比学习与硬动态原型方法。简而言之，我们通过选择与聚类中心距离最大的样本生成硬原型。我们从理论上证明，硬原型用于对比损失中以强调差异。此外，我们不是将查询图像严格对齐到特定的原型，而是通过在聚类内随机选择样本生成动态原型。动态原型用于鼓励多样性。最后，我们引入了一种渐进学习策略，逐步将模型的注意力转向差异性和多样性，避免聚类退化。在公开的 SYSU-MM01 和 RegDB 数据集上进行的大量实验验证了所提出方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Progressive Contrastive Learning with Hard and Dynamic Prototypes method. This method generates hard prototypes to emphasize divergence and dynamic prototypes to encourage variety, while a progressive learning strategy ensures the model focuses on these aspects. Experiments on SYSU-MM01 and RegDB datasets demonstrate the method&#x27;s effectiveness in handling USVI-ReID tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了一种渐进对比学习与硬动态原型方法，以解决无监督可见红外行人重识别（USVI-ReID）的挑战。该方法通过选择与聚类中心距离最远的样本生成硬原型，以强调差异性；并通过随机选择聚类内的样本生成动态原型，以促进多样性。引入了渐进学习策略，逐步将模型的关注点转向差异性和多样性，避免聚类退化。在SYSU-MM01和RegDB数据集上的实验验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Scale Propagation Network for Generalizable Depth Completion</div>
<div class="meta-line">Authors: Haotian Wang, Meng Yang, Xinhu Zheng, Gang Hua</div>
<div class="meta-line">First: 2024-10-24T03:53:06+00:00 · Latest: 2024-10-24T03:53:06+00:00</div>
<div class="meta-line">Comments: Major revision in IEEE Transactions on Pattern Analysis and Machine
  Intelligence</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.18408v1">Abs</a> · <a href="http://arxiv.org/pdf/2410.18408v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Depth completion, inferring dense depth maps from sparse measurements, is
crucial for robust 3D perception. Although deep learning based methods have
made tremendous progress in this problem, these models cannot generalize well
across different scenes that are unobserved in training, posing a fundamental
limitation that yet to be overcome. A careful analysis of existing deep neural
network architectures for depth completion, which are largely borrowing from
successful backbones for image analysis tasks, reveals that a key design
bottleneck actually resides in the conventional normalization layers. These
normalization layers are designed, on one hand, to make training more stable,
on the other hand, to build more visual invariance across scene scales.
However, in depth completion, the scale is actually what we want to robustly
estimate in order to better generalize to unseen scenes. To mitigate, we
propose a novel scale propagation normalization (SP-Norm) method to propagate
scales from input to output, and simultaneously preserve the normalization
operator for easy convergence. More specifically, we rescale the input using
learned features of a single-layer perceptron from the normalized input, rather
than directly normalizing the input as conventional normalization layers. We
then develop a new network architecture based on SP-Norm and the ConvNeXt V2
backbone. We explore the composition of various basic blocks and architectures
to achieve superior performance and efficient inference for generalizable depth
completion. Extensive experiments are conducted on six unseen datasets with
various types of sparse depth maps, i.e., randomly sampled 0.1\%/1\%/10\% valid
pixels, 4/8/16/32/64-line LiDAR points, and holes from Structured-Light. Our
model consistently achieves the best accuracy with faster speed and lower
memory when compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于泛化深度完成的尺度传播网络</div>
<div class="mono" style="margin-top:8px">深度完成，从稀疏测量推断密集深度图，对于稳健的3D感知至关重要。尽管基于深度学习的方法在这一问题上取得了巨大进展，但这些模型在不同训练未观察到的场景中泛化能力较差，这构成了一个根本性的限制。对现有深度神经网络架构的仔细分析表明，深度完成的关键设计瓶颈实际上在于传统的归一化层。这些归一化层一方面旨在使训练更加稳定，另一方面旨在在场景尺度上建立更多的视觉不变性。然而，在深度完成中，尺度是我们需要稳健估计的，以便更好地泛化到未见过的场景。为了解决这一问题，我们提出了一种新的尺度传播归一化（SP-Norm）方法，用于从输入传播尺度，并同时保留归一化操作以实现快速收敛。具体来说，我们使用单层感知器从归一化输入中学习到的特征重新缩放输入，而不是直接对输入进行常规归一化。然后，我们基于SP-Norm和ConvNeXt V2骨干网络开发了一种新的网络架构。我们探索了各种基本块和架构的组合，以实现更好的性能和高效的泛化深度完成推理。我们在六个不同类型的稀疏深度图数据集上进行了广泛的实验，包括随机采样的0.1%/1%/10%有效像素，4/8/16/32/64线LiDAR点，以及结构光中的孔洞。与最先进的方法相比，我们的模型在速度更快、内存更低的情况下始终实现了最佳的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of depth completion, where dense depth maps are inferred from sparse measurements. It proposes a novel scale propagation normalization (SP-Norm) method to better generalize across different scenes. The SP-Norm method rescales the input using learned features from a single-layer perceptron, while preserving normalization for easy convergence. The authors develop a new network architecture based on SP-Norm and the ConvNeXt V2 backbone, achieving superior performance and efficient inference. Extensive experiments on six unseen datasets show that the proposed method outperforms state-of-the-art methods in terms of accuracy, speed, and memory efficiency.</div>
<div class="mono" style="margin-top:8px">论文针对从稀疏测量中推断密集深度图的深度完成问题，提出了一种新颖的尺度传播归一化（SP-Norm）方法以更好地跨不同场景泛化。SP-Norm方法通过单层感知器学习特征对输入进行重新缩放，同时保持归一化以实现快速收敛。作者基于SP-Norm和ConvNeXt V2骨干网络开发了一种新的网络架构，实现了卓越的性能和高效的推理。在六个未见过的数据集上的广泛实验表明，所提出的方法在准确率、速度和内存效率方面均优于现有最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Ensemble of ConvNeXt V2 and MaxViT for Long-Tailed CXR Classification   with View-Based Aggregation</div>
<div class="meta-line">Authors: Yosuke Yamagishi, Shouhei Hanaoka</div>
<div class="meta-line">Venue: MICCAI</div>
<div class="meta-line">First: 2024-10-14T16:49:14+00:00 · Latest: 2024-10-15T06:11:05+00:00</div>
<div class="meta-line">Comments: Solution paper for MICCAI CXR-LT 2024 challenge. 4th place in Subtask
  2, 5th in Subtask 1</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.10710v2">Abs</a> · <a href="http://arxiv.org/pdf/2410.10710v2">PDF</a> · <a href="https://github.com/yamagishi0824/cxrlt24-multiview-pp">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we present our solution for the MICCAI 2024 CXR-LT challenge,
achieving 4th place in Subtask 2 and 5th in Subtask 1. We leveraged an ensemble
of ConvNeXt V2 and MaxViT models, pretrained on an external chest X-ray
dataset, to address the long-tailed distribution of chest findings. The
proposed method combines state-of-the-art image classification techniques,
asymmetric loss for handling class imbalance, and view-based prediction
aggregation to enhance classification performance. Through experiments, we
demonstrate the advantages of our approach in improving both detection accuracy
and the handling of the long-tailed distribution in CXR findings. The code is
available at https://github.com/yamagishi0824/cxrlt24-multiview-pp.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConvNeXt V2和MaxViT集成用于基于视图聚合的长尾胸部X光分类</div>
<div class="mono" style="margin-top:8px">在本工作中，我们提出了对MICCAI 2024 CXR-LT挑战赛的解决方案，在子任务2中获得第4名，在子任务1中获得第5名。我们利用了在外部胸部X光数据集上预训练的ConvNeXt V2和MaxViT模型来解决胸部发现的长尾分布问题。所提出的方法结合了最先进的图像分类技术、处理类别不平衡的不对称损失以及基于视图的预测聚合，以提高分类性能。通过实验，我们展示了我们方法在提高检测准确性和处理胸部X光发现的长尾分布方面的优势。代码可在https://github.com/yamagishi0824/cxrlt24-multiview-pp获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the long-tailed distribution of chest findings in chest X-ray (CXR) classification by using an ensemble of ConvNeXt V2 and MaxViT models, combined with view-based aggregation. The method employs asymmetric loss to handle class imbalance and achieves 4th place in Subtask 2 and 5th in Subtask 1 of the MICCAI 2024 CXR-LT challenge. The experiments show improvements in detection accuracy and handling the long-tailed distribution of CXR findings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用ConvNeXt V2和MaxViT模型的集成，并结合视图聚合，解决胸部X光片（CXR）分类中的长尾分布问题。方法采用不对称损失来处理类别不平衡，并在MICCAI 2024 CXR-LT挑战赛中分别获得子任务2的第4名和子任务1的第5名。实验表明，该方法在检测准确性和处理CXR发现的长尾分布方面有所改进。</div>
</details>
</div>
<div class="card">
<div class="title">Stage-Wise and Prior-Aware Neural Speech Phase Prediction</div>
<div class="meta-line">Authors: Fei Liu, Yang Ai, Hui-Peng Du, Ye-Xin Lu, Rui-Chen Zheng, Zhen-Hua Ling</div>
<div class="meta-line">First: 2024-10-07T12:45:20+00:00 · Latest: 2024-10-07T12:45:20+00:00</div>
<div class="meta-line">Comments: Accepted by SLT2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.04990v1">Abs</a> · <a href="http://arxiv.org/pdf/2410.04990v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase
Prediction (SP-NSPP) model, which predicts the phase spectrum from input
amplitude spectrum by two-stage neural networks. In the initial
prior-construction stage, we preliminarily predict a rough prior phase spectrum
from the amplitude spectrum. The subsequent refinement stage transforms the
amplitude spectrum into a refined high-quality phase spectrum conditioned on
the prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone
and adopt adversarial training by innovatively introducing a phase spectrum
discriminator (PSD). To further improve the continuity of the refined phase, we
also incorporate a time-frequency integrated difference (TFID) loss in the
refinement stage. Experimental results confirm that, compared to neural
network-based no-prior phase prediction methods, the proposed SP-NSPP achieves
higher phase prediction accuracy, thanks to introducing the coarse phase priors
and diverse training criteria. Compared to iterative phase estimation
algorithms, our proposed SP-NSPP does not require multiple rounds of staged
iterations, resulting in higher generation efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阶段式和先验感知神经语音相位预测</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的阶段式和先验感知神经语音相位预测（SP-NSPP）模型，该模型通过两阶段神经网络从输入幅度谱预测相位谱。在初始先验构建阶段，我们从幅度谱初步预测一个粗糙的先验相位谱。随后的细化阶段将幅度谱转换为基于先验相位的高质量细化相位谱。两个阶段的网络均使用ConvNeXt v2块作为骨干，并采用通过创新引入相位谱判别器（PSD）的对抗训练。为了进一步提高细化相位的连续性，我们在细化阶段还引入了时间频率集成差值（TFID）损失。实验结果表明，与基于神经网络的无先验相位预测方法相比，提出的SP-NSPP具有更高的相位预测精度，这得益于引入了粗糙的相位先验和多样的训练标准。与迭代相位估计算法相比，我们提出的SP-NSPP不需要多轮阶段迭代，从而提高了生成效率。</div>
</details>
</div>
<div class="card">
<div class="title">MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial   Representation Learning</div>
<div class="meta-line">Authors: Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, Nico Lang</div>
<div class="meta-line">Venue: ECCV 2024</div>
<div class="meta-line">First: 2024-05-04T23:16:48+00:00 · Latest: 2024-07-29T10:35:50+00:00</div>
<div class="meta-line">Comments: Accepted for ECCV 2024. Data and code:
  https://vishalned.github.io/mmearth Update arXiv v2 (ECCV): 1. Dataset fix:
  Removed duplicates and corrected ERA5 yearly statistics. 2. Data augmentation
  fix: Random crops are now aligned. 3. Test metrics fix: Metrics are now
  overall instead of mini-batch averages, matching GEO-Bench metrics. 4.
  Pretrained on MMEarth v001 &amp; evaluated on GEO-Bench v1.0</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2405.02771v2">Abs</a> · <a href="http://arxiv.org/pdf/2405.02771v2">PDF</a> · <a href="https://vishalned.github.io/mmearth">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The volume of unlabelled Earth observation (EO) data is huge, but many
important applications lack labelled training data. However, EO data offers the
unique opportunity to pair data from different modalities and sensors
automatically based on geographic location and time, at virtually no human
labor cost. We seize this opportunity to create MMEarth, a diverse multi-modal
pretraining dataset at global scale. Using this new corpus of 1.2 million
locations, we propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to
learn general-purpose representations for optical satellite images. Our
approach builds on the ConvNeXt V2 architecture, a fully convolutional masked
autoencoder (MAE). Drawing upon a suite of multi-modal pretext tasks, we
demonstrate that our MP-MAE approach outperforms both MAEs pretrained on
ImageNet and MAEs pretrained on domain-specific satellite images. This is shown
on several downstream tasks including image classification and semantic
segmentation. We find that pretraining with multi-modal pretext tasks notably
improves the linear probing performance compared to pretraining on optical
satellite images only. This also leads to better label efficiency and parameter
efficiency which are crucial aspects in global scale applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMEarth：探索多模态前置任务在地理空间表示学习中的应用</div>
<div class="mono" style="margin-top:8px">地球观测（EO）数据的未标记数据量巨大，但许多重要应用缺乏标注训练数据。然而，EO数据提供了根据地理位置和时间自动配对不同模态和传感器数据的独特机会，几乎无需人力成本。我们抓住这一机会，创建了MMEarth，这是一个全球规模的多模态预训练数据集。利用这个包含120万位置的新语料库，我们提出了一种多前置任务掩码自编码器（MP-MAE）方法，用于学习光学卫星图像的一般表示。我们的方法基于ConvNeXt V2架构，这是一种全卷积掩码自编码器（MAE）。通过一系列多模态前置任务，我们证明了我们的MP-MAE方法在图像分类和语义分割等下游任务上优于在ImageNet和领域特定卫星图像上预训练的MAE。这表明，使用多模态前置任务进行预训练显著提高了线性探针性能，相较于仅使用光学卫星图像进行预训练。这还导致了更好的标签效率和参数效率，这是全球规模应用中的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MMEarth explores multi-modal pretext tasks for geospatial representation learning using unlabelled Earth observation data. The approach leverages a diverse multi-modal pretraining dataset of 1.2 million locations and a Multi-Pretext Masked Autoencoder (MP-MAE) based on ConvNeXt V2. Experimental results show that MP-MAE outperforms MAEs pretrained on ImageNet or domain-specific satellite images on downstream tasks like image classification and semantic segmentation, particularly in terms of label and parameter efficiency for global-scale applications.</div>
<div class="mono" style="margin-top:8px">MMEarth 利用未标记的地球观测数据，通过自动配对不同模态和传感器，创建了一个全球规模的多模态预训练数据集。它提出了一种基于 ConvNeXt V2 的 Multi-Pretext Masked Autoencoder (MP-MAE) 方法，该方法在图像分类和语义分割等下游任务上优于 ImageNet 和特定领域卫星图像的预训练。多模态先验任务显著提高了线性探针性能，并提高了标签和参数效率，这对于全球规模的应用至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Memory Matching for Unsupervised Visible-Infrared Person   Re-Identification</div>
<div class="meta-line">Authors: Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-01-12T01:24:04+00:00 · Latest: 2024-07-29T09:40:11+00:00</div>
<div class="meta-line">Comments: Accepted by ECCV2024</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2401.06825v2">Abs</a> · <a href="http://arxiv.org/pdf/2401.06825v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a
promising yet challenging retrieval task. The key challenges in USL-VI-ReID are
to effectively generate pseudo-labels and establish pseudo-label
correspondences across modalities without relying on any prior annotations.
Recently, clustered pseudo-label methods have gained more attention in
USL-VI-ReID. However, previous methods fell short of fully exploiting the
individual nuances, as they simply utilized a single memory that represented an
identity to establish cross-modality correspondences, resulting in ambiguous
cross-modality correspondences. To address the problem, we propose a
Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a
Cross-Modality Clustering (CMC) module to generate the pseudo-labels through
clustering together both two modality samples. To associate cross-modality
clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM)
module, ensuring that optimization explicitly focuses on the nuances of
individual perspectives and establishes reliable cross-modality
correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module
to narrow the modality gap while mitigating the effect of noise pseudo-labels
through a soft many-to-many alignment strategy. Extensive experiments on the
public SYSU-MM01 and RegDB datasets demonstrate the reliability of the
established cross-modality correspondences and the effectiveness of our MMM.
The source codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多记忆匹配在无监督可见-红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USL-VI-ReID）是一项有前景但极具挑战性的检索任务。USL-VI-ReID 的关键挑战在于有效生成伪标签，并在不依赖任何先验注释的情况下建立跨模态的伪标签对应关系。最近，聚类伪标签方法在 USL-VI-ReID 中获得了更多关注。然而，之前的方法未能充分利用个体差异，因为它们仅利用单一记忆来表示身份以建立跨模态对应关系，导致跨模态对应关系模糊。为了解决这一问题，我们提出了一种 USL-VI-ReID 的多记忆匹配（MMM）框架。我们首先设计了一个跨模态聚类（CMC）模块，通过聚类两种模态样本来生成伪标签。为了关联跨模态聚类伪标签，我们设计了一个多记忆学习和匹配（MMLM）模块，确保优化明确关注个体视角的差异并建立可靠的跨模态对应关系。最后，我们设计了一个软聚类级对齐（SCA）模块，通过软多对多对齐策略缩小模态差距并减轻噪声伪标签的影响。在公开的 SYSU-MM01 和 RegDB 数据集上的广泛实验表明，建立的跨模态对应关系的可靠性以及我们 MMM 的有效性。源代码将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification by proposing a Multi-Memory Matching (MMM) framework. It introduces a Cross-Modality Clustering (CMC) module to generate pseudo-labels and a Multi-Memory Learning and Matching (MMLM) module to establish reliable cross-modality correspondences. The Soft Cluster-level Alignment (SCA) module further refines these correspondences. Experiments on SYSU-MM01 and RegDB datasets show the effectiveness of the proposed method in generating accurate cross-modality correspondences.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多记忆匹配（MMM）框架来解决无监督可见红外行人重识别的挑战。该框架包含跨模态聚类（CMC）模块生成伪标签，多记忆学习和匹配（MMLM）模块建立可靠的跨模态对应关系，以及软簇级对齐（SCA）模块减少模态差距并减轻噪声影响。在SYSU-MM01和RegDB数据集上的实验表明，该方法能够有效建立准确的跨模态对应关系。</div>
</details>
</div>
<div class="card">
<div class="title">Mutual Information Guided Optimal Transport for Unsupervised   Visible-Infrared Person Re-identification</div>
<div class="meta-line">Authors: Zhizhong Zhang, Jiangming Wang, Xin Tan, Yanyun Qu, Junping Wang, Yong Xie, Yuan Xie</div>
<div class="meta-line">First: 2024-07-17T17:32:07+00:00 · Latest: 2024-07-17T17:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2407.12758v1">Abs</a> · <a href="http://arxiv.org/pdf/2407.12758v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised visible infrared person re-identification (USVI-ReID) is a
challenging retrieval task that aims to retrieve cross-modality pedestrian
images without using any label information. In this task, the large
cross-modality variance makes it difficult to generate reliable cross-modality
labels, and the lack of annotations also provides additional difficulties for
learning modality-invariant features. In this paper, we first deduce an
optimization objective for unsupervised VI-ReID based on the mutual information
between the model&#x27;s cross-modality input and output. With equivalent
derivation, three learning principles, i.e., &quot;Sharpness&quot; (entropy
minimization), &quot;Fairness&quot; (uniform label distribution), and &quot;Fitness&quot; (reliable
cross-modality matching) are obtained. Under their guidance, we design a loop
iterative training strategy alternating between model training and
cross-modality matching. In the matching stage, a uniform prior guided optimal
transport assignment (&quot;Fitness&quot;, &quot;Fairness&quot;) is proposed to select matched
visible and infrared prototypes. In the training stage, we utilize this
matching information to introduce prototype-based contrastive learning for
minimizing the intra- and cross-modality entropy (&quot;Sharpness&quot;). Extensive
experimental results on benchmarks demonstrate the effectiveness of our method,
e.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any
annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互信息引导的最优传输在无监督可见红外行人重识别中的应用</div>
<div class="mono" style="margin-top:8px">无监督可见红外行人重识别（USVI-ReID）是一项具有挑战性的检索任务，旨在在不使用任何标签信息的情况下检索跨模态的行人图像。在这个任务中，巨大的跨模态差异使得生成可靠的跨模态标签变得困难，而缺乏注释也增加了学习模态不变特征的难度。在本文中，我们首先基于模型的跨模态输入和输出之间的互信息推导出无监督VI-ReID的优化目标。通过等价推导，我们获得了三个学习原则，即“锐度”（熵最小化）、“公平性”（均匀标签分布）和“适应性”（可靠的跨模态匹配）。在这些原则的指导下，我们设计了一种交替进行模型训练和跨模态匹配的循环训练策略。在匹配阶段，我们提出了一种均匀先验引导的最优传输分配（“适应性”，“公平性”）来选择匹配的可见和红外原型。在训练阶段，我们利用这种匹配信息引入基于原型的对比学习，以最小化类内和跨模态的熵（“锐度”）。在基准上的广泛实验结果表明了我们方法的有效性，例如在SYSU-MM01和RegDB上无任何注释的情况下，Rank-1精度分别为60.6%和90.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unsupervised visible-infrared person re-identification by formulating an optimization objective based on mutual information. It introduces three learning principles: Sharpness, Fairness, and Fitness, and proposes a loop iterative training strategy. The method uses uniform prior guided optimal transport for matching visible and infrared prototypes and applies prototype-based contrastive learning to minimize entropy. Experimental results show significant improvements, achieving 60.6% and 90.3% Rank-1 accuracy on SYSU-MM01 and RegDB without any annotations.</div>
<div class="mono" style="margin-top:8px">本文通过基于互信息的方法解决了无监督可见红外行人再识别的挑战，并提出了三个学习原则：Sharpness（最小化熵）、Fairness（均匀标签分布）和Fitness（可靠的跨模态匹配）。该方法采用交替进行模型训练和跨模态匹配的循环迭代训练策略。具体而言，在匹配阶段使用均匀先验引导的最优运输分配来选择匹配的可见和红外原型，在训练阶段利用匹配信息引入基于原型的对比学习以最小化内模态和跨模态的熵。实验结果表明该方法的有效性，在SYSU-MM01和RegDB基准上分别达到了60.6%和90.3%的Rank-1准确率，且无需任何标注。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised   Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Xiangbo Yin, Jiangming Shi, Yachao Zhang, Yang Lu, Zhizhong Zhang, Yuan Xie, Yanyun Qu</div>
<div class="meta-line">First: 2024-05-09T08:17:06+00:00 · Latest: 2024-05-09T08:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2405.05613v1">Abs</a> · <a href="http://arxiv.org/pdf/2405.05613v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a
formidable challenge, which aims to match pedestrian images across visible and
infrared modalities without any annotations. Recently, clustered pseudo-label
methods have become predominant in USVI-ReID, although the inherent noise in
pseudo-labels presents a significant obstacle. Most existing works primarily
focus on shielding the model from the harmful effects of noise, neglecting to
calibrate noisy pseudo-labels usually associated with hard samples, which will
compromise the robustness of the model. To address this issue, we design a
Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for
USVI-ReID. To be specific, we first introduce a straightforward yet potent
Noisy Pseudo-label Calibration module to correct noisy pseudo-labels. Due to
the high intra-class variations, noisy pseudo-labels are difficult to calibrate
completely. Therefore, we introduce a Neighbor Relation Learning module to
reduce high intra-class variations by modeling potential interactions between
all samples. Subsequently, we devise an Optimal Transport Prototype Matching
module to establish reliable cross-modality correspondences. On that basis, we
design a Memory Hybrid Learning module to jointly learn modality-specific and
modality-invariant information. Comprehensive experiments conducted on two
widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR
outperforms the current state-of-the-art GUR with an average Rank-1 improvement
of 10.3%. The source codes will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒的邻居关系伪标签学习方法用于无监督可见-红外行人重识别</div>
<div class="mono" style="margin-top:8px">无监督可见-红外行人重识别（USVI-ReID）面临巨大挑战，目标是在没有任何标注的情况下，匹配可见光和红外模态的行人图像。最近，聚类伪标签方法在USVI-ReID中占据主导地位，尽管伪标签中的固有噪声构成了重大障碍。现有大多数工作主要关注于防止模型受到噪声的负面影响，而忽视了对通常与硬样本相关的噪声伪标签进行校准，这会损害模型的鲁棒性。为解决这一问题，我们为USVI-ReID设计了一种鲁棒的邻居关系伪标签学习框架（RPNR）。具体而言，我们首先引入了一个简单而有效的噪声伪标签校准模块，以纠正噪声伪标签。由于类内变异性高，噪声伪标签难以完全校准。因此，我们引入了一个邻居关系学习模块，通过建模所有样本之间的潜在交互来降低高类内变异性。随后，我们设计了一种最优传输原型匹配模块，以建立可靠的跨模态对应关系。在此基础上，我们设计了一种记忆混合学习模块，以联合学习模态特定和模态不变信息。在两个广泛认可的基准数据集SYSU-MM01和RegDB上进行的全面实验表明，RPNR在平均Rank-1上优于当前最先进的GUR，提高了10.3%。源代码将很快发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unsupervised visible-infrared person re-identification (USVI-ReID) by proposing a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework. It introduces a Noisy Pseudo-label Calibration module to correct noisy pseudo-labels and a Neighbor Relation Learning module to model interactions between samples, reducing intra-class variations. Additionally, it includes an Optimal Transport Prototype Matching module for reliable cross-modality correspondences and a Memory Hybrid Learning module to learn both modality-specific and invariant information. Experiments on SYSU-MM01 and RegDB benchmarks show that RPNR outperforms the current state-of-the-art method, GUR, with a 10.3% improvement in Rank-1 accuracy.</div>
<div class="mono" style="margin-top:8px">论文提出了一种鲁棒伪标签学习与邻域关系框架（RPNR），以解决无监督可见-红外行人重识别（USVI-ReID）的挑战。该框架引入了伪标签校准模块来修正噪声伪标签，邻域关系学习模块来建模样本间的潜在交互，以及最优传输原型匹配模块来建立跨模态对应关系。在SYSU-MM01和RegDB基准上的实验表明，RPNR在Rank-1准确率上比当前最先进的方法GUR提高了10.3%。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251010_0328.html">20251010_0328</a>
<a href="archive/20251009_0319.html">20251009_0319</a>
<a href="archive/20251008_0344.html">20251008_0344</a>
<a href="archive/20251007_0345.html">20251007_0345</a>
<a href="archive/20251006_0350.html">20251006_0350</a>
<a href="archive/20251005_0349.html">20251005_0349</a>
<a href="archive/20251004_0351.html">20251004_0351</a>
<a href="archive/20251003_0351.html">20251003_0351</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0340.html">20251001_0340</a>
<a href="archive/20250930_0357.html">20250930_0357</a>
<a href="archive/20250929_0354.html">20250929_0354</a>
<a href="archive/20250928_1405.html">20250928_1405</a>
<a href="archive/20250928_0338.html">20250928_0338</a>
<a href="archive/20250927_2233.html">20250927_2233</a>
<a href="archive/20250925_0328.html">20250925_0328</a>
<a href="archive/20250924_0337.html">20250924_0337</a>
<a href="archive/20250923_0336.html">20250923_0336</a>
<a href="archive/20250922_0334.html">20250922_0334</a>
<a href="archive/20250921_0333.html">20250921_0333</a>
<a href="archive/20250920_0334.html">20250920_0334</a>
<a href="archive/20250919_1904.html">20250919_1904</a>
<a href="archive/20250919_1023.html">20250919_1023</a>
<a href="archive/20250919_0332.html">20250919_0332</a>
<a href="archive/20250918_2047.html">20250918_2047</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250918_0326.html">20250918_0326</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250917_0326.html">20250917_0326</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250916_0328.html">20250916_0328</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250915_0324.html">20250915_0324</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250914_0320.html">20250914_0320</a>
<a href="archive/20250913_0323.html">20250913_0323</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_1121.html">20250912_1121</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250912_0317.html">20250912_0317</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250911_0319.html">20250911_0319</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250910_0318.html">20250910_0318</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250909_0340.html">20250909_0340</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250908_0340.html">20250908_0340</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250907_0331.html">20250907_0331</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_1222.html">20250905_1222</a>
<a href="archive/20250905_0335.html">20250905_0335</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_1123.html">20250904_1123</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
